{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83c\udfe1 Home","text":"Vinay Kumar /  \u0935\u093f\u0928\u092f \u0915\u0941\u092e\u093e\u0930  <p>         I'm a Machine Learning Engineer with expertise in deep learning, computer vision, and natural language processing.         Currently working on large language models and multimodal AI systems. Previously, I completed my         Masters in Electrical Engineering         from NC State University, specializing in AI/ML, and my         BTech in Electrical Engineering         from IIT Hyderabad. I'm passionate about developing AI solutions that can make a real-world impact.       </p>"},{"location":"awesome/","title":"\u2728 Awesome List:","text":"<ul> <li> PositionalEncoding:<ul> <li> https://e2eml.school/transformers.html</li> <li> https://github.com/tensorflow/tensor2tensor/issues/1591</li> <li> https://kazemnejad.com/blog/transformer_architecture_positional_encoding/</li> </ul> </li> <li> BatchNorm:<ul> <li> https://colab.research.google.com/drive/1Sw2GXJmylz9DvtPaBfJIeutoXNISelZy?usp=sharing</li> <li> https://e2eml.school/batch_normalization.html</li> <li> How BatchNorm Helps?: https://arxiv.org/pdf/1805.11604.pdf</li> <li> Understanding BatchNorm: https://proceedings.neurips.cc/paper/2018/file/36072923bfc3cf47745d704feb489480-Paper.pdf</li> <li> Batch-ReNorm: https://arxiv.org/pdf/1702.03275.pdf</li> </ul> </li> <li> StreamingNorm: https://arxiv.org/pdf/1610.06160.pdf</li> <li> Online Norm: https://proceedings.neurips.cc/paper/2019/file/cb3ce9b06932da6faaa7fc70d5b5d2f4-Paper.pdf</li> <li> How optimization works:<ul> <li> https://e2eml.school/how_optimization_works_1.html</li> <li> https://e2eml.school/how_optimization_works_2.html</li> </ul> </li> <li> Optimizing a linear model: https://e2eml.school/how_optimization_works_3.html</li> <li> Optimizing complex models:vhttps://e2eml.school/how_optimization_works_4.html</li> <li> EINSUM:<ul> <li> https://obilaniu6266h16.wordpress.com/2016/02/04/einstein-summation-in-numpy/</li> <li> https://rockt.github.io/2018/04/30/einsum</li> <li> https://ajcr.net/Basic-guide-to-einsum/</li> <li> einops: https://einops.rocks/</li> <li> einops EXAMPLES: http://einops.rocks/pytorch-examples.html</li> <li> EINSUM / einops / PyTorch: https://theaisummer.com/einsum-attention/</li> </ul> </li> <li> Linear Regression: https://machinelearningcompass.com/machine_learning_models/linear_regression/</li> <li> Types of Convolutions: https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d</li> <li> 3D Graph Scene: https://3dscenegraph.stanford.edu</li> </ul>"},{"location":"awesome/#external-code-refernce","title":"External code refernce","text":"fibonacci: test.py<pre><code>    elif n == 1:\n        return [0]\n\n    fib = [0, 1]\n    for i in range(2, n):\n        fib.append(fib[i-1] + fib[i-2])\n</code></pre>"},{"location":"algorithms/023_add_lists/","title":"023 add lists","text":""},{"location":"algorithms/023_add_lists/#problem","title":"Problem","text":""},{"location":"algorithms/024_depth_first_values/","title":"024 depth first values","text":""},{"location":"algorithms/024_depth_first_values/#problem","title":"Problem","text":""},{"location":"algorithms/025_breadth_first_values/","title":"025 breadth first values","text":""},{"location":"algorithms/025_breadth_first_values/#problem","title":"Problem","text":""},{"location":"algorithms/026_tree_includes/","title":"026 tree includes","text":""},{"location":"algorithms/026_tree_includes/#problem","title":"Problem","text":""},{"location":"algorithms/027_tree_sum/","title":"027 tree sum","text":""},{"location":"algorithms/027_tree_sum/#problem","title":"Problem","text":""},{"location":"algorithms/028_tree_min_value/","title":"028 tree min value","text":""},{"location":"algorithms/028_tree_min_value/#problem","title":"Problem","text":""},{"location":"algorithms/QA/","title":"\u2753Q/A","text":""},{"location":"algorithms/QA/#linked-list","title":"Linked List","text":"Question #1 <p>What two properties are typically stored in the nodes of a singly linked list?</p> <p> <code>value</code> and <code>next</code></p> Question #2 <p>What terms are commonly used to describe the first node &amp; last node of a linked list?</p> <p> HEAD for first-node </p> <p> TAIL for last-node</p> Question #3 <p>What is the dummy head pattern for a linked-list?</p> <p> The dummy head pattern is where we use a fake node to  act as the HEAD of the linked-list. The dummy-head is used to simplify edge cases such as inserting teh first node into an empty linked-list.</p> Question #4 <p>Why might the expression <code>current_node.next.val</code> be UNSAFE?</p> <p> If the current node is a TAIL-node then its <code>.next</code> will be <code>None</code> and <code>None</code> object does not have <code>.val</code> attribute.</p> Question #5 <p>What is the OPTIMAL COMPLEXITY for searching a target value in a tandard singly-linked-list?</p> <p> \\(O(n)\\) time-complexity and \\(O(1)\\) space-complexity for ITERATIVE Solution.</p> <p> \\(O(n)\\) time-complexity and \\(O(n)\\) space-complexity for RECURSIVE solution.</p>"},{"location":"algorithms/arrays/","title":"Arrays","text":""},{"location":"algorithms/arrays/#arrays","title":"Arrays","text":"<p><code>Author: Vinay Kumar (@imflash217) | Date: 15/March/2021</code></p>"},{"location":"algorithms/arrays/#definition","title":"Definition","text":"Definition <p>An Array is an ordered list of data that we access with a numerical index. Generally speaking an array is allocated upfront as a single block of memory based on the number of elements and type of the data we want the array to hold. This allows us to read and write elements into the array efficiently, since our program knows exactly where each element is stored in the memory.</p> <p>On the other hand removing, adding and finding arbitrary values in an array can be a linear-time operation.</p> <p>Removing or splicing requires shifting all elements by one to fill the gap.</p> <p>Inserting a new element would requires shifting or allocating a new larger array to hold the elements.</p> <p>Finding an element in the array would require iterating over the entire array in the worst-case.</p>"},{"location":"algorithms/arrays/#dynamic-array","title":"Dynamic Array","text":"Dynamic Arrays <p>It is worth noting that in many statically-typed programming languages (e.g. Java, C++); an array is limited to its initially declared size.</p> <p>ALL modern languages support DYNAMICALLY-SIZED arrays; which automatically increase or decrease their size by allocating a new copy of the array when it begins to run out-of-memory.</p> <p>Dynamic arrays guarantee better amortized performance by only performing these costly operations when necessary.</p>"},{"location":"algorithms/arrays/#calculating-memory-usage","title":"Calculating Memory Usage","text":"Calculating Memory Usage <p>To calculate the memory usage of an array simply multiply the size of the array with the size of the data-type.</p> What is the memory usage of an array that contains one-thousand 32-bit integers? <pre><code>1000 * 32 bits  = 1000 * 4 bytes = 4000 bytes = 4Kb\n</code></pre> What is the memory usage of an array that contains one-hundred 10-char strings? <pre><code>100 * 10 chars = 100 * 10 * 1 byte = 1000 bytes = 1Kb\n</code></pre>"},{"location":"algorithms/arrays/#common-array-operations","title":"Common Array Operations","text":"Common Array Operations <ul> <li>Insert an item</li> <li>Remove an item</li> <li>Update an item</li> <li>Find an item</li> <li>Loop over array</li> <li>Copy an array</li> <li>Copy-part-of-the-array</li> <li>Sort an array</li> <li>Reverse an array</li> <li>Swap an array</li> <li>Filter an array</li> </ul>"},{"location":"algorithms/arrays/#when-to-use-an-array-in-an-interview","title":"When to use an array in an interview","text":"When to use an array in an interview <p>Use an array when you need dta in an ordered list with fast-indexing or compact-memory-footprint.</p> <p>Don't use an array if you need to search for unsorted items efficiently or insert and remove items frequently.</p>"},{"location":"algorithms/binary_tree/","title":"Binary Tree","text":""},{"location":"algorithms/binary_tree/#introduction","title":"Introduction","text":"<p>A <code>tree</code> is a frequently used data structure to simulate a hierarchical tree-like structure. Each node of the tree will a <code>value</code> and list of references to otehr nodes which are called <code>child nodes</code>.</p> <p>From a <code>graph</code> view, a <code>tree</code> can also be described as a DAG (directed acyclic graph)  which has <code>N</code> nodes and <code>(N-1)</code> edges.</p> <p>A <code>binary tree</code> is a tree data structure where each node can have maximum 2 children only. A binary tree where all internal nodes (i.e. except the leaf nodes) have exactly 2 children are called <code>Complete Binary Tree</code>.</p>"},{"location":"algorithms/binary_tree/#traversals","title":"Traversals","text":"<p>Pre-Order:      - Traverse the root first then traverse the left subtree. Finally traverse the right subtree </p> <p>In-Order:     - Traverse the left subtree first. Then visit the root node. Finally traverse the right subtree.     - In binary search tree; in-order traversal gives the whole tree nodes in a sorted order.</p> <p>Post-Order:     - Traverse the left subtree first, then traverse the right subtree*. Finally visit the root node**</p>"},{"location":"algorithms/binary_tree/#examples","title":"Examples","text":""},{"location":"algorithms/binary_tree/#25-depth-first-values","title":"25: Depth First values","text":"Depth First Values <p>Write a function, <code>depth_first_values</code>, that takes in the root of a binary tree.  The function should return a list containing all values of the tree in depth-first order.</p> <pre><code>a = Node('a')\nb = Node('b')\nc = Node('c')\nd = Node('d')\ne = Node('e')\nf = Node('f')\ng = Node('g')\na.left = b\na.right = c\nb.left = d\nb.right = e\nc.right = f\ne.left = g\n\n#      a\n#    /   \\\n#   b     c\n#  / \\     \\\n# d   e     f\n#    /\n#   g\n\ndepth_first_values(a)\n#   -&gt; ['a', 'b', 'd', 'e', 'g', 'c', 'f']\n</code></pre> Solution <pre><code># class Node:\n#   def __init__(self, val):\n#     self.val = val\n#     self.left = None\n#     self.right = None\n\ndef depth_first_values(root):\n  \"\"\"recursive solution Depth First Traversal\"\"\"\n\n  ## base case\n  if root is None:\n    return []\n\n  left_values = depth_first_values(root.left)\n  right_values = depth_first_values(root.right)\n\n  ## in DFS, the visiting order is\n  ## root, left-child, right-child\n  return [root.val, *left_values, *right_values]\n</code></pre>"},{"location":"algorithms/binary_tree/#26-breadth-first-values","title":"26: Breadth First Values","text":"Breadth First Values <p>Write a function, <code>breadth_first_values</code>, that takes in the root of a binary tree.  The function should return a list containing all values of the tree in breadth-first order.</p> <pre><code>    a = Node('a')\nb = Node('b')\nc = Node('c')\nd = Node('d')\ne = Node('e')\nx = Node('x')\n\na.right = b\nb.left = c\nc.left = x\nc.right = d\nd.right = e\n\n#      a\n#       \\\n#        b\n#       /\n#      c\n#    /  \\\n#   x    d\n#         \\\n#          e\n\nbreadth_first_values(a) \n#    -&gt; ['a', 'b', 'c', 'x', 'd', 'e']\n</code></pre> Solution <pre><code># class Node:\n#   def __init__(self, val):\n#     self.val = val\n#     self.left = None\n#     self.right = None\n\ndef breadth_first_values(root):\n  ## base case for empty tree\n  if root is None:\n    return []\n\n  ## using double-ended queue struct\n  from collections import deque\n\n  ## step-1: add the root into the queue\n  queue = deque([root])\n  visited = []\n\n  while queue:\n    ## grabbing current visited node\n    current_node = queue.popleft()\n\n    ## put that node in the visited list\n    visited.append(current_node.val)\n\n    ## and add their children (if present) into the queue\n    if current_node.left:\n      queue.append(current_node.left)\n    if current_node.right:\n      queue.append(current_node.right)\n\n  return visited\n</code></pre>"},{"location":"algorithms/binary_tree/#27-tree-includes","title":"27: Tree Includes","text":"Problem <p>Write a function, tree_includes, that takes in the root of a binary tree and a target value.  The function should return a boolean indicating whether or not the value is contained in the tree.</p> <pre><code>a = Node(\"a\")\nb = Node(\"b\")\nc = Node(\"c\")\nd = Node(\"d\")\ne = Node(\"e\")\nf = Node(\"f\")\n\na.left = b\na.right = c\nb.left = d\nb.right = e\nc.right = f\n\n#      a\n#    /   \\\n#   b     c\n#  / \\     \\\n# d   e     f\n\ntree_includes(a, \"e\") # -&gt; True\n</code></pre> Solution <pre><code># class Node:\n#   def __init__(self, val):\n#     self.val = val\n#     self.left = None\n#     self.right = None\n\ndef tree_includes(root, target):\n\n  ## base case of leaf-nodes\n  if not root: return False\n\n  ## success\n  if root.val == target: return True\n\n  ## else search in the children tree\n  return tree_includes(root.left, target) or tree_includes(root.right, target)\n</code></pre>"},{"location":"algorithms/binary_tree/#28-tree-sum","title":"28: Tree Sum","text":"Problem <p>Write a function, <code>tree_sum</code>, that takes in the root of a binary tree that contains number values.  The function should return the total sum of all values in the tree. <pre><code>a = Node(3)\nb = Node(11)\nc = Node(4)\nd = Node(4)\ne = Node(-2)\nf = Node(1)\n\na.left = b\na.right = c\nb.left = d\nb.right = e\nc.right = f\n\n#       3\n#    /    \\\n#   11     4\n#  / \\      \\\n# 4   -2     1\n\ntree_sum(a) # -&gt; 21\n</code></pre></p> Solution <pre><code># class Node:\n#   def __init__(self, val):\n#     self.val = val\n#     self.left = None\n#     self.right = None\n\ndef tree_sum(root):\n  if not root: return 0                     ## base-case of leaf nodes\n  left_sum = tree_sum(root.left)            ## sum of left subtree\n  right_sum = tree_sum(root.right)          ## sum of right subtree\n  return root.val + left_sum + right_sum\n</code></pre>"},{"location":"algorithms/binary_tree/#29-binary-tree-min-value","title":"29: Binary Tree Min Value","text":"Minimum value in a binary tree <p>Write a function, tree_min_value, that takes in the root of a binary tree that contains number values.  The function should return the minimum value within the tree.</p> <p>You may assume that the input tree is non-empty.</p> <pre><code>a = Node(3)\nb = Node(11)\nc = Node(4)\nd = Node(4)\ne = Node(-2)\nf = Node(1)\n\na.left = b\na.right = c\nb.left = d\nb.right = e\nc.right = f\n\n#       3\n#    /    \\\n#   11     4\n#  / \\      \\\n#  4   -2     1\ntree_min_value(a) # -&gt; -2\n</code></pre> Solution <pre><code># class Node:\n#   def __init__(self, val):\n#     self.val = val\n#     self.left = None\n#     self.right = None\n\ndef tree_min_value(root):\n  import math\n\n  if not root: return math.inf                ## base case for leaf nodes's children\n  left_min = tree_min_value(root.left)        ## minimum value in left subtree\n  right_min = tree_min_value(root.right)      ## minimum value in right subtree\n  return min(root.val, left_min, right_min)   ## return minimum of root, lef, &amp; right subtrees\n</code></pre>"},{"location":"algorithms/binary_tree/#30-root-to-leaf-path-w-max-sum","title":"30: Root-to-Leaf path w/ MAX sum","text":"Problem <p>Write a function <code>max_path_sum()</code> that takes in the root of a Binary Tree that conatins number values. The functions should return the maximum sum of any  root-to-leaf path in the tree.</p> <p>Assumption: The tree is non-empty</p> <p>An example of the scenario is shown below:</p> <pre><code>a = Node(-1)\nb = Node(-6)\nc = Node(-5)\nd = Node(-3)\ne = Node(0)\nf = Node(-13)\ng = Node(-1)\nh = Node(-2)\n\na.left = b\na.right = c\nb.left = d\nb.right = e\nc.right = f\ne.left = g\nf.right = h\n\n#        -1\n#      /   \\\n#    -6    -5\n#   /  \\     \\\n# -3   0    -13\n#     /       \\\n#    -1       -2\n\nmax_path_sum(a) # -&gt; -8\n</code></pre> Solution <pre><code>import math\n\nclass Node:\n    def __init__(self, value):\n        self.val = value\n        self.left = None\n        self.right = None\n\n## RECURSIVE Solution\ndef max_path_sum(root):\n    ## base case (None node)\n    if root is None:\n        return -math.inf\n\n    ## base case (leaf node)\n    if root.left == None and root.right == None:\n        return root.val\n\n    left_sum = max_path_sum(root.left)\n    right_sum = max_path_sum(root.right)\n    return root.val + max(left_sum, right_sum)\n</code></pre>"},{"location":"algorithms/binary_tree/#31-tree-path-finder","title":"31: Tree Path Finder","text":"Problem <p>Write a function <code>path_finder</code> that takes in the root of a BT and a target value The function should return an array representing the path to teh target value. If the target value is not found, then return <code>None</code>.</p> <p>Assumption: Every node in teh tree contains unique value.</p> <p>Sample example:</p> <pre><code>a = Node(\"a\")\nb = Node(\"b\")\nc = Node(\"c\")\nd = Node(\"d\")\ne = Node(\"e\")\nf = Node(\"f\")\ng = Node(\"g\")\nh = Node(\"h\")\n\na.left = b\na.right = c\nb.left = d\nb.right = e\nc.right = f\ne.left = g\nf.right = h\n\n#      a\n#    /   \\\n#   b     c\n#  / \\     \\\n# d   e     f\n#    /       \\\n#   g         h\n\npath_finder(a, \"h\") # -&gt; ['a', 'c', 'f', 'h']\n</code></pre> Solution <pre><code>class Node:\n    def __init__(self, val):\n        self.val = val\n        self.left = None\n        self.right = None\n\ndef _path_finder(root, target):\n    ## base case (None node)\n    if root is None:\n        return None\n\n    ## base case (node found)\n    if root.val == target:\n        return [root.val]\n\n    ## recuse over the left child\n    left_path = _path_finder(root.left, target)\n    if left_path is not None:\n        ## target found in the left child subtree\n        left_path.append(root.val)\n        return left_path\n\n    ## recurse over the right child\n    right_path = _path_finder(root.right, target)\n    if right_path is not None:\n        ## target found in the left child subtree\n        right_path.append(root.val)\n        return right_path\n\n    return None     ## edge case\n\ndef path_finder(root, target):\n    path = _path_finder(root, target)\n    if path is None:\n        return None\n    return path[::-1]       ## return the path in reverse order (from root to target)\n</code></pre>"},{"location":"algorithms/binary_tree/#32-tree-value-count","title":"32: Tree Value Count","text":"Problem <p>Write a function, tree_value_count, that takes in the root of a binary tree and a target value.  The function should return the number of times that the target occurs in the tree.</p> <pre><code>a = Node(12)\nb = Node(6)\nc = Node(6)\nd = Node(4)\ne = Node(6)\nf = Node(12)\n\na.left = b\na.right = c\nb.left = d\nb.right = e\nc.right = f\n\n#      12\n#    /   \\\n#   6     6\n#  / \\     \\\n# 4   6     12\n\ntree_value_count(a,  6) # -&gt; 3\n</code></pre> Solution <pre><code>class Node:\n    def __init__(self, val):\n        self.val = val\n        self.left = None\n        self.right = None\n\n## RECURSIVE DFS\n## time = O(n)\n## space = O(n)\n\ndef tree_value_count(root, target):\n    ## base case (None node)\n    if root is None:\n        return 0\n\n    left_count = tree_value_count(root.left, target)\n    right_count = tree_value_count(root.right, target)\n    if root.val == target:\n        return 1 + left_count + right_count\n    return left_count + right_count\n</code></pre>"},{"location":"algorithms/binary_tree/#33-height-of-a-bt","title":"33: Height of a BT","text":"Problem <p>Write a function, <code>how_high()</code>, that takes in the root of a binary tree.  The function should return a number representing the height of the tree.</p> <p>The height of a binary tree is defined as the maximal number of edges from the root node to any leaf node.</p> <p>If the tree is empty, return -1.</p> <pre><code>a = Node('a')\nb = Node('b')\nc = Node('c')\nd = Node('d')\ne = Node('e')\nf = Node('f')\ng = Node('g')\n\na.left = b\na.right = c\nb.left = d\nb.right = e\nc.right = f\ne.left = g\n\n#      a\n#    /   \\\n#   b     c\n#  / \\     \\\n# d   e     f\n#    /\n#   g\n\nhow_high(a) # -&gt; 3\n</code></pre> Solution <pre><code>class Node:\n    def __init__(self, val):\n        self.val = val\n        self.left = None\n        self.right = None\n\n## RECURSIVE approach\ndef how_high(root):\n    ## base case (None node)\n    if root is None:\n        return -1       ## see definition of height of a BT above\n\n    left_height = how_high(root.left)\n    right_height = how_high(root.right)\n    return 1 + max(left_height, right_height)\n</code></pre>"},{"location":"algorithms/binary_tree/#34-botton-right-value","title":"34: Botton Right Value","text":"Problem <p>Write a function, <code>bottom_right_value()</code>, that takes in the root of a binary tree.  The function should return the right-most value in the bottom-most level of the tree.</p> <p>You may assume that the input tree is non-empty.</p> <pre><code>a = Node(-1)\nb = Node(-6)\nc = Node(-5)\nd = Node(-3)\ne = Node(-4)\nf = Node(-13)\ng = Node(-2)\nh = Node(6)\n\na.left = b\na.right = c\nb.left = d\nb.right = e\nc.right = f\ne.left = g\ne.right = h\n\n#        -1\n#      /   \\\n#    -6    -5\n#   /  \\     \\\n# -3   -4   -13\n#     / \\       \n#    -2  6\n\nbottom_right_value(a) # -&gt; 6\n</code></pre> Solution <pre><code>from collections import deque\n\ndef bottom_right_value(root):\n    queue = deque([root])\n    current = None\n\n    while queue:\n        current = queue.popleft()\n        if current.left is not None:\n            queue.append(current.left)\n        if current.right is not None:\n            queue.append(current.right)\n\n    return current.val\n</code></pre>"},{"location":"algorithms/binary_tree/#35-all-tree-paths","title":"35: All tree paths","text":"<p>Write a function, <code>all_tree_paths()</code>, that takes in the root of a binary tree.  The function should return a 2-Dimensional list where each subarray represents  a root-to-leaf path in the tree.</p> <p>The order within an individual path must start at the root and end at the leaf,  but the relative order among paths in the outer list does not matter.</p> <p>You may assume that the input tree is non-empty.</p> <p>An example use case is shown below:</p> <pre><code>a = Node('a')\nb = Node('b')\nc = Node('c')\nd = Node('d')\ne = Node('e')\nf = Node('f')\ng = Node('g')\nh = Node('h')\ni = Node('i')\n\na.left = b\na.right = c\nb.left = d\nb.right = e\nc.right = f\ne.left = g\ne.right = h\nf.left = i\n\n#         a\n#      /    \\\n#     b      c\n#   /  \\      \\\n#  d    e      f\n#      / \\    /   \n#     g  h   i \n\nall_tree_paths(a) # -&gt;\n# [ \n#   [ 'a', 'b', 'd' ], \n#   [ 'a', 'b', 'e', 'g' ], \n#   [ 'a', 'b', 'e', 'h' ], \n#   [ 'a', 'c', 'f', 'i' ] \n# ] \n</code></pre> Solution <pre><code># class Node:\n#   def __init__(self, val):\n#     self.val = val\n#     self.left = None\n#     self.right = None\n\n## Recursive DFS\n## during DFS let the nodes return two lists which contains\n## the two possible paths (left &amp; right)\n## branch it according to values it receives from each branch\n\ndef _all_tree_paths(root):\n    ## base case (None node)\n    if root is None:\n        return []         ## return empty list as there its a null node\n\n    left_paths = _all_tree_paths(root.left)\n    right_paths = _all_tree_paths(root.right)\n    paths = left_paths + right_paths\n\n    ## base-case (leaf-node)\n    if len(paths) == 0:\n        paths.append([root.val])\n        return paths\n\n    for path in paths:\n        path.append(root.val)\n    return paths\n\ndef all_tree_paths(root):\n    all_paths = _all_tree_paths(root)\n    return [path[::-1] for path in all_paths]\n</code></pre>"},{"location":"algorithms/binary_tree/#36-tree-levels","title":"36: Tree levels","text":"<p>Write a function, <code>tree_levels()</code>, that takes in the root of a binary tree.  The function should return a 2D list where each sublist represents a level of the tree.</p> <p>An example use-case is shown below:</p> <pre><code>a = Node('a')\nb = Node('b')\nc = Node('c')\nd = Node('d')\ne = Node('e')\nf = Node('f')\ng = Node('g')\nh = Node('h')\ni = Node('i')\n\na.left = b\na.right = c\nb.left = d\nb.right = e\nc.right = f\ne.left = g\ne.right = h\nf.left = i\n\n#         a\n#      /    \\\n#     b      c\n#   /  \\      \\\n#  d    e      f\n#      / \\    /\n#     g  h   i\n\ntree_levels(a) # -&gt;\n# [\n#   ['a'],\n#   ['b', 'c'],\n#   ['d', 'e', 'f'],\n#   ['g', 'h', 'i']\n# ]\n</code></pre> Solution <p>...</p>"},{"location":"algorithms/design_patterns/","title":"Intro","text":""},{"location":"algorithms/design_patterns/#design-patterns-for-humans","title":"Design Patterns for Humans \ud83e\udd17","text":"<p>Design Patterns are guidelines to recurring problems; guidelines on how to solve certain problems They are not classes, packages, or libraries that you can plug into an application and wait for magic to happen. These are rather guideline son how to solve certain problems in certain situations.</p> <p>Design Patterns are guidelines to solving certain recurring problems.</p> <p>Wikipedia describes them as:</p> <p>In software engineering, a software design pattern is a general reusable solution  to a commonly recurring problem within a given context in software design. It is not a finished design that can be directly ported into source or machine code. Rather, it is a description or template for how to solve a problem that can be used in many different situations.</p>"},{"location":"algorithms/design_patterns/#be-careful","title":"\ud83d\udea8 Be Careful!!","text":"<p>Developers, mostly beginners, makes the mistake of over-thinking and forcing the design patterns which results in un-maintainable mess. The rule should always be to make the codebase as simple as possible. Once you start developing you will start to see recurring patterns in your codebase,  at which point you can start factoring your code using relevant design patterns.</p> <ul> <li>Design Patterns are not silver bullets to your problems. Use them consciously.</li> <li>Don't try to force them. Bad things might happen if done so!!</li> <li>Remember! Design Patterns are gudelines towards finding solutions to problem; not solutions to problems themselves.</li> </ul>"},{"location":"algorithms/design_patterns/#types-of-design-patterns","title":"Types of Design Patterns","text":"<p>Adapted from the Gang-of-four (GoF) book on <code>Design Patterns</code>;  there are broadly three types of useful &amp; popular design patterns:</p> <ol> <li>Creational</li> <li>Structural </li> <li>Behavioral</li> </ol>"},{"location":"algorithms/design_patterns/#creational-design-patterns","title":"Creational Design Patterns","text":"<p>In simple words, Creational design patterns are focused towards  how to instantiate an object or group of objects</p> <p>Wikipedia says:</p> <p>In software engineering, Creational design patterns are design patterns that deal with object creation mechanism; trying to create objects in a manners suitable for the sitation. The basic form of object creation could result in design problems or added complexity to the design. Creational design patetrns solve this problem by controlling this object creation.</p> <p>There are 6 types of Creational patterns:</p> <ol> <li>Simple Factory</li> <li>Factory Method</li> <li>Abstract Factory</li> <li>Builder</li> <li>Prototpe</li> <li>Singleton</li> </ol>"},{"location":"algorithms/dict/","title":"Dictionary Abstract Data Types","text":""},{"location":"algorithms/dict/#another-solution","title":"Another Solution","text":"<ul> <li> Can do better with Hash Tables in \\(O(1)\\) expected time, \\(O(n+m)\\) space;  where m is the table size</li> </ul>"},{"location":"algorithms/dict/#an-example","title":"An example","text":"<ul> <li> Let <code>keys</code> be the student ids of students registered in class CLS201; eg. <code>2022CS10110</code>.</li> <li> There are \\(100\\) students in the class, so we create a hash-table of size say 100.</li> <li> Hash function <code>hash()</code> is say, the last two digits of the student-id.</li> <li> So, now, <code>2022CS10110</code> goes to location <code>10</code> in the hash-table.</li> <li> </li> </ul>"},{"location":"algorithms/dict/#multiply-add-divide-mad","title":"Multiply-Add-Divide (MAD)","text":"<p>This technique is used to create pseudo-unique hash values.</p> <p> It is also used in pseudo random number generators</p> <p>The method is as follows:</p> \\[ \\text{hash}(k) = |a\\cdot k + b|\\ \\text{mod}\\ N\\]"},{"location":"algorithms/dict/#collisions","title":"Collisions","text":"<p>Methods to resolve collisions</p> <ol> <li>Chaining</li> <li>Open Addressing:<ol> <li>Linear Probing</li> <li>Double Hashing</li> </ol> </li> </ol>"},{"location":"algorithms/dict/#linear-probing","title":"Linear Probing","text":"<pre><code>def linear_probing_insert(hash_table, key):\n    if hash_table.isfull():\n        raise TableFullError\n\n    slots = len(hash_table)                 ## the total number of slots\n    probe = hash_fn(key)                    ## the hash of the 'key'\n\n    while hash_table[probe] == None:\n        probe += 1                          ## go to the next index if the current index is occupied\n        probe = probe % slots               ## wrap-arround to support in-range indexing\n    hash_table[probe] = key                 ## INSERT the key in the hash-table\n    return hash_table\n</code></pre>"},{"location":"algorithms/document_distance/","title":"Document Distance","text":"Call Graph for the code <pre><code>- main()\n  - word_frequencies_for_file()\n    - get_words_from_line_list()\n    - count_frequency()\n  - vector_angle()\n    - inner_product()\n</code></pre> Author Disclaimer <p><code>Author: Vinay Kumar (@imflash217)</code></p> <p><code>Date: 01/February/2021</code></p> <p>The contents of this article were originally published at the references below. I have assembled it for my own understanding. Feel free to reuse and tag along the references. </p>"},{"location":"algorithms/document_distance/#references","title":"References","text":""},{"location":"algorithms/dp_simple_factory/","title":"\ud83c\udfe0 Simple Factory","text":""},{"location":"algorithms/dp_simple_factory/#a-real-world-example","title":"A real world example:","text":"<p>Consider, you are building house and you need doors. You can either put on some carpenter clothes, bring some glue, wood, tools and make the door yourself. Or, you can call the factory and get the door delivered to you; so that you don't need to learn anything about door making or deal with the mess it brings.</p> <p>In simple words,</p> <p>Simple Factory generates an instance for client without exposing any instantiation logic to the clients.</p> <p>Wikipedia says,</p> <p>In Object oriented Programming language (OOP), Factory is an object for creating other objects. Formally, a factory is a function or method that returns objects of a varying protoype  or class from some method call, which is assumed to be <code>new</code>.</p>"},{"location":"algorithms/dp_simple_factory/#a-programatic-example","title":"A Programatic example:","text":"<p>First of all, we have a door interface and implementation of a wooden door.</p> <pre><code>&lt;?php\n// The Simple Factory INTERFACE\n// \ninterface Door{\n    public function get_width(): float;\n    public function get_height(): float;\n}\n\n// A CONCRETE implementation of Door interface\n//\nclass WoodenDoor implements Door{\n    protected $width;\n    protected $height;\n\n    public function __construct(float $width, float $height){\n        $this-&gt;$width = $width;\n        $this-&gt;$height = $height;\n    }\n\n    public function get_width(): float{\n        return $this-&gt;$width;\n    }\n\n    public function get_height(): float{\n        return $this-&gt;$height;\n    }\n}\n</code></pre> <p>Then we have our door factory that makes the door and returns it.</p> <pre><code>&lt;?php\n\nclass DoorFactory{\n    public static function make_door($width, $height): Door {\n        return new WoodenDoor($height, $width);\n    }\n}\n</code></pre> <p>And then it can be used as:</p> <pre><code>&lt;?php\n\n// Make a door of 100x200\n$door = DoorFactory::make_door(100, 200);\n\necho \"Width = \" . $door-&gt;get_width();\necho \"Height = \" . $door-&gt;get_width();\n\n// Make a door of 50x100\n$door2 = DoorFactory::make_door(50, 100);\n</code></pre>"},{"location":"algorithms/dp_simple_factory/#when-to-use","title":"\u2753 When to use?","text":"<p>When creating an object is not just a few assignments, but involves some logic;  it makes sense to put it in a dedicated factory instead of repeating same code everywhere.</p>"},{"location":"algorithms/hashtables/","title":"\ud83e\uddee Hash Tables","text":""},{"location":"algorithms/hashtables/#hash-tables","title":"Hash Tables","text":"<p><code>Author: Vinay Kumar (@imflash217) | Date: 29/January/2021</code></p>"},{"location":"algorithms/hashtables/#definition","title":"Definition","text":"Definition <p>Hash Table is a data structure which stores data in an associative manner (i.e. in a (key, value) pair).</p> <ul> <li>In a hash table, the data is stored in an array format where each data-value has its own unique index-value. Due to this feature, the access to data becomes very fast if we know the desired index-value; irrespective of the size of the data.</li> <li>Hash Table uses an array as a storage medium and uses hashing to generate the index where an element is to be inserted or to be located from.</li> </ul>"},{"location":"algorithms/hashtables/#hashing","title":"Hashing","text":"Hashing <p>Hashing is a technique to map a range of keys into a range of indexes (usually of an array).</p> <ul> <li>A very generic hashing function is modulo operator (<code>x % y</code>).</li> </ul>"},{"location":"algorithms/hashtables/#example","title":"Example","text":"Example of Hashing <ul> <li>Consider a hash-table of <code>size=20</code></li> <li>Following (<code>key</code>, <code>value</code>) pairs to be stored using the hash-table</li> </ul> <pre><code>dict = {9: 20,\n        12: 70,\n        42: 80,\n        7: 25,\n        2: 21}\n</code></pre> Key Hash Array index 9 <code>9 % 20 = 9</code> <code>9</code> 12 <code>12 % 20 = 12</code> <code>12</code> 42 <code>42 % 20 = 2</code> <code>2</code> 7 <code>7 % 20 = 7</code> <code>7</code> 2 <code>2 % 20 = 2</code> <code>2</code> <p>As we can see that a given hashing function can create the same hash-value from two different keys. (in above table keys <code>42</code> and <code>2</code>). So we use <code>Linear Probing</code> to resolve conflicts.</p>"},{"location":"algorithms/hashtables/#linear-probing","title":"Linear Probing","text":"Linear Probing <p>Linear Probing is a method used to resolve conflicts in the hash-value. It may happen that the hash-function creates an already used index of the array. In such case we search the next empty location of the array by looking into the next cell until we find an empty cell</p> <p>So in our above example, the updated hash-table would map <code>key = 2</code> to <code>index = 3</code>:</p> Key Hash Array index 9 <code>9 % 20 = 9</code> <code>9</code> 12 <code>12 % 20 = 12</code> <code>12</code> 42 <code>42 % 20 = 2</code> <code>2</code> 7 <code>7 % 20 = 7</code> <code>7</code> 2 <code>2 % 20 = 2</code> <code>3</code>"},{"location":"algorithms/hashtables/#search","title":"Search","text":"search() method for hash-table <p><code>Search</code></p>"},{"location":"algorithms/hashtables/#delete","title":"Delete","text":"delete() method for hash-table <p><code>Delete</code></p>"},{"location":"algorithms/hashtables/#python-implementation","title":"Python Implementation","text":""},{"location":"algorithms/hashtables/#references","title":"References","text":"<ol> <li> <p>https://www.hackerearth.com/practice/data-structures/hash-tables/basics-of-hash-tables/tutorial/ \u21a9</p> </li> <li> <p>https://www.tutorialspoint.com/python_data_structure/python_hash_table.htm \u21a9</p> </li> <li> <p>https://www.tutorialspoint.com/data_structures_algorithms/hash_data_structure.htm \u21a9</p> </li> <li> <p>http://blog.chapagain.com.np/hash-table-implementation-in-python-data-structures-algorithms/ \u21a9</p> </li> <li> <p>https://runestone.academy/runestone/books/published/pythonds/SortSearch/Hashing.html \u21a9</p> </li> <li> <p>http://paulmouzas.github.io/2014/12/31/implementing-a-hash-table.html \u21a9</p> </li> </ol>"},{"location":"algorithms/linked_list/","title":"Linked List Data Structure","text":"<p>In this tutorial we will talk about a widely useful Linked List data structure. Previously, we had tried to implement dynamic lists using <code>arrays</code>,  but we encountered several issues with its inefficiency in terms of memory usage. To understand LInked List properly, we first need to understand theses limitations encountered  while trying to implement dynamic lists using <code>arrays</code>.</p> <p>So, let's start with a story, imagine a computer's memory (RAM) and imagine that to be represented  as separate partitions where each partition represents a <code>byte</code> of the memory  and each <code>byte</code> has its memory-address as well. Let's focus on one region of this RAM's memory address starting with address <code>200</code>.</p>"},{"location":"algorithms/linked_list/#23-adding-lists","title":"23: Adding lists","text":"Add lists <p>Write in a function that takes head of two linked lists,  each representing a number. The nodes of the linked-lists contain digits as value. The nodes in the input lists are reversed  (i.e. the least significant digit of the number is head).</p> <p>The function should return the head of the new linked list  representing the sum of the input lists. The output should have its digits reversed as well.</p> <pre><code>Say we wanted to compute 621 + 354 normally. The sum is 975:\n\n   621\n + 354\n -----\n   975\n\nThen, the reversed linked list format of this problem would appear as:\n\n    1 -&gt; 2 -&gt; 6\n +  4 -&gt; 5 -&gt; 3\n --------------\n    5 -&gt; 7 -&gt; 9\n</code></pre> Solution <pre><code>class Node:\n    def __init__(self, val):\n        super().__init__()\n        self.val = val\n        self.next = None\n\ndef add_lists(head_1, head_2, carry=0):\n    \"\"\"Recursive solution\"\"\"\n    ## base case:\n    if head_1 is None and head_2 is None and carry == 0:\n        return None\n\n    ## grab the values of each node \n    ## or use a dummy value 0 if the node is None\n\n    val_1 = head_1.val if head_1 else 0\n    val_2 = head_2.val if head_2 else 0\n\n    _sum = val_1 + val_2 + carry                    ## add the two values\n    digit = _sum % 10                               ## accounting for carry (next line)\n    carry = 1 if _sum &gt;= 10 else 0\n\n    result = Node(digit)                            ## create a new \"Node\" with new digit\n\n    next_1 = head_1.next if head_1 else None\n    next_2 = head_2.next if head_2 else None\n\n    result.next = add_lists(next_1, next_2, carry)  ## recursive call\n    return result\n</code></pre> Discussion <p>...</p>"},{"location":"algorithms/priority_queue/","title":"Priority Queue","text":"Definition <p>Implements a set S of elements; each of the elements is associated with a key</p> <p>Operations: <code>search()</code>, <code>insert()</code>, <code>delete()</code>, <code>change_priorities()</code>, <code>max_priority()</code>, <code>min_priority()</code></p> Author Disclaimer <p><code>Author: Vinay Kumar (@imflash217)</code></p> <p><code>Date: 01/February/2021</code></p> <p>The contents of this article were originally published at the references below. I have assembled it for my own understanding. Feel free to reuse and tag along the references. </p>"},{"location":"algorithms/priority_queue/#references","title":"References","text":""},{"location":"algorithms/sorting/","title":"Sorting Algorithms","text":""},{"location":"algorithms/sorting/#insertion-sort","title":"Insertion Sort","text":""},{"location":"algorithms/sorting/#vanilla-insertion-sort","title":"Vanilla Insertion Sort","text":"Vanilla Insertion Sort <p><pre><code>for i = 1, 2, 3, ..., n{\n    insert A[i] into sorted array A[0:i-1]\n    by \"pairwise swaps\" down to the correct position\n}\n</code></pre> </p> <p>This above version has \\(\\theta(n)\\) steps and each step has \\(\\theta(n)\\) comparisons. SO this version of the algorithm is \\(\\theta(n^2)\\) runtime complexity.</p>"},{"location":"algorithms/sorting/#binary-insertion-sort","title":"Binary Insertion Sort","text":"Binary Insertion Sort <p>This improved version is slightly improved by using Binary Search while searching for the position to place the key <code>A[i]</code> in the sorted part of the array (i.e. <code>A[0:i-1]</code>) <pre><code>for i = 1, 2, 3, ..., n{\n    insert A[i] into sorted array A[0:i-1]\n    by \"Binary Search\" down to the correct position\n}\n</code></pre></p> <p>This above version has \\(\\theta(n)\\) steps and each step has \\(\\theta(\\log(n))\\) comparisons due to Binary Search. SO this version of the algorithm is \\(\\theta(n\\times \\log(n))\\) runtime complexity (in sorting but not in swappings). If we consider swapping operations too then even Binary Search will take \\(\\theta(n)\\) time to swap positions as it might have to move a lot of positions.</p>"},{"location":"algorithms/sorting/#references","title":"References","text":"<ol> <li> <p>https://twitter.com/pottolama/status/1354745837524553728 \u21a9</p> </li> <li> <p>https://github.com/mportesi/sorting_algo_visualizer \u21a9</p> </li> </ol>"},{"location":"blogs/notes/","title":"Notes","text":""},{"location":"blogs/notes/#the-preface-of-the-key-technological-stuffs-here","title":"The Preface of the key technological stuffs here","text":""},{"location":"blogs/notes/#tips-tricks","title":"Tips &amp; Tricks","text":""},{"location":"blogs/notes/#lr-scheduler","title":"LR Scheduler","text":"<ul> <li> Similar to the <code>learning rate</code>, the <code>lr-scheduler</code> to apply depends on the      classifier &amp; the model.</li> <li> For image classifiers and <code>SGD</code> optimizer, the <code>Multi-Step LR Scheduler</code>     is shown to be a good choice.</li> <li> Models trained with <code>Adam</code> commonly use a smooth exponential-decay      in the <code>lr</code> or a cosine-like scheduler.</li> <li> For TRANSFORMERS:<ul> <li> Remember to use a <code>learning rate WARMUP</code></li> <li> The <code>cosine-scheduler</code> is often used for decaying the <code>lr</code>      afterwards (but can also be replaced by <code>exponential decay</code>)</li> </ul> </li> </ul>"},{"location":"blogs/notes/#regularizaation","title":"Regularizaation","text":"<ul> <li> Regularization is important in networks when we see a significantly higher      training performance than test performance.</li> <li> The regularization parameters all interact with each other and hence      must be tuned together. The most commonly used regularization techniques are:<ul> <li><code>Weight Decaay</code></li> <li><code>Dropout</code></li> <li><code>Augmentation</code></li> </ul> </li> <li> Dropout is a good regularization technique as it has shown to be     applicable on most architectures and has shown to reduce overfitting.</li> <li> If you want to use weight-decay in Adam, use <code>torch.optim.AdamW</code> instead of <code>torch.optim.Adam</code>.</li> <li> Domain specific regularization: There are a couple of regularization techniques that      depend on the input-data / domain as shown below.<ul> <li> Computer Vision: Image augmenatation like <ul> <li><code>horizontal_flip</code>, </li> <li><code>rotation</code>, </li> <li><code>scale_and_crop</code>, </li> <li><code>color_distortion</code>, </li> <li><code>gaussian_noise</code> etc.</li> </ul> </li> <li> NLP: input dropout of whole words</li> <li> Graphs: <ul> <li>Dropping edges</li> <li>Dropping nodes</li> <li>Dropping part of the features of all nodes</li> </ul> </li> </ul> </li> </ul>"},{"location":"blogs/notes/#debugging-in-pytorch","title":"Debugging in PyTorch","text":""},{"location":"blogs/notes/#under-performing-model","title":"Under-performing model","text":"Situation/Problem <p>Your model is not reaching the performance it should,  but PyTorch is not telling you why that happens!! These are very annoying bugs.</p>"},{"location":"blogs/notes/#softmax-crossentropy-nllloss","title":"Softmax, CrossEntropy &amp; NLLLoss","text":"<p> The most common mistake is the mismatch between the loss function and the output activations. A very usual common source of confusion is the relationship  between <code>nn.Softmax</code>, <code>nn.LogSoftmax</code>, <code>nn.NLLLoss</code>, &amp; <code>nn.CrossEntropyLoss</code></p> <ol> <li> <p><code>nn.CrossEntropyLoss</code> does two operations on its inputs: <code>nn.LogSoftmax</code> &amp; <code>nn.NLLLoss</code>. Hence, the input to the <code>nn.CrossEntropyLoss</code> should be the output of the last layer of the network.</p> <p> Don't apply <code>nn.Softmax</code> before the <code>nn.CrossEntropyLoss</code>.  Otherwise, PyTorch will apply the Softmax TWICE which will signifacntly worsen the performance.</p> </li> <li> <p>If you use <code>nn.NLLLoss</code>, you need to apply log-softmax before yourselves. <code>nn.NLLLoss</code> requires log-probabilities as its input not just plain probabilities.  So, make sure to use <code>F.log_softmax()</code> instead of <code>nn.Softmax</code></p> </li> </ol>"},{"location":"blogs/notes/#softmax-over-correct-dimensionaxis","title":"Softmax over correct dimension/axis","text":"<p>Be careful to apply softmax over correct dimensio/axis in your output. For eg. you apply softamx over last dimension like this: <code>nn.Softmax(dim=-1)</code></p>"},{"location":"blogs/notes/#categorical-data-embeddings","title":"Categorical Data &amp; Embeddings","text":""},{"location":"blogs/notes/#hidden-size-mismatch","title":"Hidden size mismatch","text":"<p>If you perform matrix multiplications and have a shape mismatch between two matrices, PyTorch will contain and throw error. </p> <p>However, there are situations where PyTorch does not throw any error because the misaligned dimensions have (unluckily) the same dimension. For example, imagine you have a weight matrix <code>W</code> of shape <code>[d_in, d_out]</code>. If you take an inout <code>x</code> of shape <code>[batch_size, d_in]</code>. And you want to do the matrix multiplication as <code>out = W.matmul(x)</code> then the shape of the output <code>out</code>  will be correct as <code>[batch_size, d_out]</code>. But, suppose if by chance <code>batch_size == d_in</code> then both <code>W.matmul(x)</code> and <code>x.matmul(W)</code> will produce the same sized output <code>[d_in, d_out]</code>. This is definitely not the behaviour we want as it hides the error in the order of  matrix maultiplication over different dimension.</p> <p> So, always test your code with multiple different batch sizes to prevent shape misalignments with the batch dimension.</p>"},{"location":"blogs/notes/#use-nnsequential-nnmodulelist","title":"Use nn.Sequential &amp; nn.ModuleList","text":"<p>If you have a model with lots of layers, you might waant to summarize them into  <code>nn.Sequential</code> or <code>nn.ModuleList</code> object. In the forward pass, you only need to call the  <code>Sequential</code> or iterate through the <code>ModuleList</code>.</p> <p>A multi-layer-perceptron (MLP) can be implemented as follows:</p> <pre><code>import torch\nimport torch.nn as nn\n\nclass MLP(nn.Module):\n    def __init__(\n        self,\n        input_dims=64,\n        hidden_dims=[128, 256],\n        output_dims=10,\n    ):\n        super().__init__()\n        hidden_dims = [input_dims] + hidden_dims\n        layers = []\n        for i in range(len(hidden_dims)-1):\n            layers += [\n                nn.Linear(hidden_dims[i], hidden_dims[i+1],\n                nn.ReLU(inplace=True)\n            ]\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.layers(x)\n</code></pre>"},{"location":"blogs/notes/#in-place-activation-functions","title":"In-place Activation functions","text":"<p>Some activation functions such as <code>nn.ReLU</code> and <code>nn.LeakyReLU</code> have an argument <code>inplace</code>. By default, it is set to <code>False</code>, but it is highly recommended to set it to <code>True</code> in neural networks.</p> <p>Setting it to <code>True</code>, makes the original value of the input overridden by the new output during the forward pass.</p> <p> This option of <code>inplace</code> is ONLY available to activations functions where we don't need to know the original input for backpropagation.</p> <p>For example, in <code>nn.ReLU</code>, the value sthat are set to zero have a gradient of ZERO independent of the specific input values.</p> <p> In-place operations can save a lot of memory, especially if you have a very large feature map.</p>"},{"location":"blogs/notes/#references","title":"References","text":"<ol> <li> <p>Learning rate finder:      https://pytorch-lightning.readthedocs.io/en/latest/advanced/lr_finder.html#learning-rate-finder \u21a9</p> </li> <li> <p>Auto Scaling batch sizes:      https://pytorch-lightning.readthedocs.io/en/latest/advanced/training_tricks.html#auto-scaling-of-batch-size \u21a9</p> </li> <li> <p>Compare hyperparam search performance in TensorBoard:      https://towardsdatascience.com/a-complete-guide-to-using-tensorboard-with-pytorch-53cb2301e8c3 \u21a9</p> </li> <li> <p>\\(3^{rd}\\) party libraries:      https://medium.com/pytorch/accelerate-your-hyperparameter-optimization-with-pytorchs-ecosystem-tools-bc17001b9a49 \u21a9</p> </li> <li> <p>Saving <code>git</code> hash metadata:      https://github.com/Nithin-Holla/meme_challenge/blob/f4dc2079acb78ae30caaa31e112c4c210f93bf27/utils/save.py#L26 \u21a9</p> </li> <li> <p>PyTorch Tutorials: https://effectivemachinelearning.com/ \u21a9</p> </li> <li> <p>Attention &amp; Transformers: https://e2eml.school/transformers.html \u21a9</p> </li> </ol>"},{"location":"blogs/deep_learning/blog_dataloaders/","title":"Blog dataloaders","text":""},{"location":"blogs/deep_learning/blog_dataloaders/#pytorch-dataloaders","title":"PyTorch Dataloaders","text":""},{"location":"blogs/deep_learning/blog_tf_v1/","title":"Tensorflow Tutorial","text":""},{"location":"blogs/deep_learning/blog_tf_v1/#tensorflow-tutorial","title":"Tensorflow Tutorial","text":"<p>In this session you will learn to do the following in <code>TensorFlow v1.0</code></p> <ol> <li>Initialize Variables</li> <li>Start your own session</li> <li>Train Algorithms</li> <li>Implement a Neural Network</li> </ol>"},{"location":"blogs/deep_learning/blog_tf_v1/#exploring-the-tensorflow-library","title":"Exploring the Tensorflow Library","text":""},{"location":"blogs/deep_learning/blog_tf_v1/#example-1-general-overview","title":"Example-1: General Overview","text":"<pre><code>import tensorflow as tf\n\ny_hat = tf.constant(36, name=\"y_hat\")           ## Defins a \"y_hat\" constant. Sets its value to 36\ny = tf.constant(39, name=\"y\")                   ## Defins a \"y\" constant. Sets its value to 39\nloss = tf.Variable((y-y_hat)**2, name=\"loss\")\n\ninit = tf.global_variables_initializer()        ## Used to initialize the variables with the\n                                                ## respective values when \"sess.run(init)\" is called\n\nwith tf.Session() as sess:                      ## Creates a session to execute our program\n    sess.run(init)                              ## initializes the global variables\n    sess.run(loss)                              ## executes the program stored in \"loss\" variable\n    print(loss)                                 ## prints the value stored in \"loss\" variable\n</code></pre> <p>Writing and running programs in <code>Tensorflow</code> has the following steps:</p> <ol> <li>Create tensors (variables) that are not yet evaluated/executed.</li> <li>Write operations between those tensors.</li> <li>Initialize the tensors.</li> <li>Create a Session.</li> <li>Run the session. This will run the operations written in step-2.</li> </ol> <p>So, when we created a variable for <code>loss</code>, we simply defined the loss as a function of other quantities but did not evaluate its value. To evaluate it, we had to run  <code>tf.global_variables_initializer()</code> to intialize the values and then inside <code>sess.run(init)</code> we calculated the updated value and prited it in the last line above.</p>"},{"location":"blogs/deep_learning/blog_tf_v1/#example-2-tfsession","title":"Example-2: <code>tf.Session()</code>","text":"<p>Now, let's take a look at</p> <pre><code>a = tf.constant(2)\nb = tf.constant(10)\nc = tf.multiply(a, b)\nprint(c)\n</code></pre> <pre><code>Tensor(\"Mul:0\", shape=(), dtype=int32)\n</code></pre> <p>As expected we will not see <code>20</code>. We got a tensor saying that the result of the tensor does not have the <code>shape</code> attribute and is of the type <code>int32</code>. All we did was to put in the computation graph; but we haven't run this computation yet! In order to actually  multiply the two numbers we have to create a sessiona nd run it.</p> <p><pre><code>sess = tf.Session()\nprint(sess.run(c))\n</code></pre> <pre><code>20\n</code></pre></p> <p>Awesome!!. To summarize, remember the following:</p> <ol> <li>Initialize your variables.</li> <li>Create a session.</li> <li>Run the operations inside the session.</li> </ol>"},{"location":"blogs/deep_learning/blog_tf_v1/#example-3-tfplaceholder","title":"Example-3: <code>tf.placeholder()</code>","text":"<p>Next, we will see how to use a placeholder.</p> <p>A placeholder is an object whose value we can specify ONLY later.</p> <p>To specify values for a placeholder, we can pass in values by using a  \"feed dictionary\" (<code>feed_dict</code> variable).</p> <p><pre><code>## Below we create a placeholder for x.\n## This allows us to pass in a number later when we run the SESSION\n\nsess = tf.Session()\n\nx = tf.placeholder(tf.int64, name=\"x\")      ## the placeholder variable\nprint(sess.run(2*x, feed_dict={x:9}))\n\nsess.close()\n</code></pre> <pre><code>18\n</code></pre></p>"},{"location":"blogs/deep_learning/blog_tf_v1/#using-one-hot-encodings","title":"Using one-hot encodings:","text":"<p><pre><code>def one_hot_matrix(labels, num_classes):\n    \"\"\"\n    Creates a matrix where the i-th row corresponds to the ith class number.\n    j-th column corresponds to the j-th example.\n    So, if the label for j-th example is i; then only the ith value is 1 in j-th column\n\n    Args:\n        labels: the labels for each example\n        num_classes: the number of classes in this task\n    Returns:\n        a one-hot matrix\n    \"\"\"\n    ## create a tf.constant &amp; name it \"num_classes\"\n    num_classes = tf.constant(num_classes, name=\"num_classes\")\n\n    ## Use tf.one_hot (be careful with \"axis\")\n    one_hot_matrix = tf.one_hot(indices=labels, depth=num_classes, axis=0)\n\n    ## Create a session\n    sess = tf.Session()\n\n    ## Execute the one_hot_matrix graph inside the session\n    one_hot = sess.run(one_hot_matrix)\n\n    ## Close the session\n    sess.close()\n\n    ## return the one_hot matrix\n    return one_hot\n</code></pre> <pre><code>import numpy as np\n\nlabels = np.array([1,2,0,1,2,2,3])\nnum_classes = 4\none_hot = one_hot_matrix(labels, num_classes)\nprint(one_hot)\n</code></pre> <pre><code>[[0,0,1,0,0,0,0],\n [1,0,0,1,0,0,0],\n [0,1,0,0,1,1,0],\n [0,0,0,0,0,0,1]]\n</code></pre></p>"},{"location":"blogs/deep_learning/blog_tf_v1/#initialize-with-zeros-ones","title":"Initialize with zeros &amp; ones","text":"<p>We will use <code>tf.ones()</code> and <code>tf.zeros()</code> to initialize a tensor of shape <code>shape</code>, where all elements are either zeros or ones</p> <p><pre><code>def ones(shape, dtype=tf.int64):\n    \"\"\"Creates a tensor of ones with shape=shape\n    Args:\n        shape: the shape of the resulting tensor\n        dtype: the datatype of every element in the resulting tensor\n    Returns:\n        A tensor where all elements are 1\n    \"\"\"\n    ## Create ones tensor using `tf.ones()`\n    ones = tf.ones(shape, dtype=dtype)\n\n    ## Create a session\n    sess = tf.Session()\n\n    ## Execute the op in the session to calculate its value\n    ones = sess.run(ones)\n\n    ## Close the session\n    sess.close()\n\n    ## Return the ones tensor\n    return ones\n</code></pre> <pre><code>ones_tensor = ones([2,3])\nprint(ones_tensor)\n</code></pre> <pre><code>[[1,1,1],\n [1,1,1]]\n</code></pre></p>"},{"location":"blogs/deep_learning/blog_tf_v1/#building-a-neural-network","title":"Building a Neural Network","text":""},{"location":"blogs/deep_learning/blog_tf_v1/#building-the-model","title":"Building the model","text":"<pre><code>from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict\nfrom tensorflow.python.framework import ops\n\ndef model(X_train, Y_train, X_test, Y_test, \n        lr=1e-3, num_epochs=1500, bs=32, verbose=True):\n        \"\"\"\n        Implements a 3-layer Tensorflow Neural Network:\n        [Linear]-&gt;[Relu]-&gt;[Linear]-&gt;[Relu]-&gt;[Linear]-[Softmax]\n\n        Args:\n            X_train: the train dataset inputs\n            Y_train: the train dataset labels\n            X_test: the test dataset inputs\n            Y_test: the test dataset labels\n            lr: the learnign rate\n            num_epochs: number of epochs\n            bs: batch-size\n            verbose: True if you want to print the process else False\n\n        Returns:\n            the trained model parameters.\n        \"\"\"\n\n        ops.reset_default_graph()       ## to be able to rerun the model, w/o overwriting the tf.variables\n        tf.set_random_seed(217)         ## to keep consistent results\n        seed = 3                        ## to keep consistent results\n        (n_x, m) = X_train.shape        ## n_x = input size; m = number of training examples\n        n_y = Y_train.shape[0]          ## n_y = output size\n        costs = []                      ## to keep track of the costs\n\n        ## Step-1: Create placeholders of shape = (n_x, n_y)\n        X, Y = create_placeholders(n_x, n_y)\n\n        ## Step-2: Initialize parameters\n        parameters = initialize_parameters()\n\n        ## Step-3: Forward propagation\n        ##         Build the forward propagation the tf graph\n        Z3 = forward_proagation(X, parameters)\n\n        ## Step-4: Cost function\n        ##         Add cost function to tf graph\n        cost = compute_cost(Z3, Y)\n\n        ## Step-5: Backward propagation\n        ##         Define the tf optimizer. Use `AdamOptimizer`\n        optimizer = tf.train.AdamOptimizer(lr).minimize(cost)\n\n        ## Step-6: Initialize all variables\n        init = tf.global_variables_initializer()\n\n        ## Step-7: Start the session to compute the tf graph\n        with tf.Session() as sess:\n            ## Step-7.1: Run the initializer `init`\n            sess.run(init)\n\n            ## Step-7.2: Do the training loop\n            for epoch in range(num_epchs):\n                epoch_cost = 0.0        ## Define the cost for each epoch\n                num_batches = m // bs\n                seed += 1\n                minibatches = random_mini_batches(X_train, Y_train, bs, seed)\n                for (Xb, Yb) in minibatches:\n                    _, minibatch_cost = sess.run([optimizer, cost], feed_dict={X:Xb, Y:Yb})\n                    epoch_cost += minibatch_cost\n                epoch_cost /= num_batches\n\n            ## Step-8: Save the trained model parameters\n            parameters = sess.run(parameters)\n            print(\"parameters have been trained\")\n\n            ## Step-9: How to calculate the correct predictions &amp; accuracy\n            correct_preds = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n            accuracy = tf.reduce_mean(tf.cast(correct_preds, \"float\"))\n\n            ## Step-10: Calculate the train &amp; test accuracies\n            accuracy_train = accuracy.eval({X:X_train, Y:Y_train})\n            accuracy_test = accuracy.eval({X:X_test, Y:Y_test})\n\n            return parameters\n</code></pre>"},{"location":"blogs/deep_learning/dl_04_09/","title":"Environment & Distribution Shift","text":""},{"location":"blogs/deep_learning/dl_04_09/#correction-of-distribution-shifts","title":"Correction of Distribution Shifts","text":""},{"location":"blogs/deep_learning/dl_04_09/#covariate-shift-correction","title":"Covariate Shift Correction","text":""},{"location":"blogs/deep_learning/dl_04_09/#label-shift-correction","title":"Label Shift Correction","text":""},{"location":"blogs/deep_learning/dl_04_09/#concept-shift-correction","title":"Concept Shift Correction","text":"<p>Concept Shift is much harder to fix in a principled manner. For instance, in a situation where the problem suddenly changes from classifying <code>cats</code> v/s <code>dogs</code> to classying <code>white</code> v/s <code>black</code> animals; it will be unreasonable to assume that we can do much better by collecting new labels and training from scratch.</p> <p>Fortunately, in practice such concept-shifts are very rare, but instead it is usually observed that the  task keeps on changing gradually. For example:</p> <ol> <li>Traffic Camera lenses degrade over time due to wear-&amp;-tear and hence the quality of images degrade too.</li> <li>News content changes gradually; i.e. new stories appear while the news still remain almost same.</li> <li>In computational advertising, new products gets launched and hence popularity of products changes  and the CTR-Predictor needs to change as well.</li> </ol> <p>In such above cases, we use the same strategy that we used while trainign the network to make them  adapt to change in data i.e.; we use Transfer Learning by finetuning the pre-trained weights with the new aquired data.</p>"},{"location":"blogs/deep_learning/dl_04_09/#a-taxonomy-of-learning-problems","title":"A taxonomy of Learning Problems","text":"<p>Armed with the knowledge of dealing with changes in the distribution of data/labels,  we now focus on otehr aspects of machine learning formulations:</p>"},{"location":"blogs/deep_learning/dl_04_09/#batch-learning","title":"Batch Learning","text":""},{"location":"blogs/deep_learning/dl_interviews/","title":"\ud83e\uddd1\ud83c\udffb\u200d\ud83d\udcbb DL Interviews","text":""},{"location":"blogs/deep_learning/dl_interviews/#deep-learning-interviews","title":"Deep Learning Interviews","text":""},{"location":"blogs/deep_learning/dl_interviews/#introduction","title":"Introduction","text":""},{"location":"blogs/deep_learning/dl_interviews/#q1-distribution-of-maximum-entropy","title":"Q1: Distribution of maximum entropy","text":"<p>What is the distribution of maximum entropy; i.e. the distribution that has the maximum entropy among all distributions in a bounded interval <code>[a, b]</code>, <code>(-\\inf, +\\inf)</code>?</p> Solution <p>In a bounded interval <code>[a, b]</code>, the UNIFORM DISTRIBUTION has the maximum entropy. The variance of the Uniform Distribution \\(\\mathcal{U}(a, b)\\) is \\(\\sigma^2 = \\frac{(b-a)^2}{12}\\). Therefore, the maximum entropy in a bounded interval <code>[a, b]</code> is \\(\\left(\\frac{\\log{12}}{2} + \\log(\\sigma)\\right)\\)</p>"},{"location":"blogs/deep_learning/dl_interviews/#q2-whats-the-purpose-of-this-code-snippet","title":"Q2: What's the purpose of this code-snippet?","text":"<p>Describe in your own words. what is the purpose of this code snippet? <pre><code>self.transforms = []\nif rotate:\n    self.transforms.append(RandomRotate())\nif flip:\n    self.transforms.append(RandomFLip())\n</code></pre></p> Solution <p>Overfitting is a common problem that occurs during training of machine learning systems. Among various strategies to overcome the problem of overfitting; data-augmentation is a very handy method. Data Augmentation is a regularization technique that synthetically expands the data-set by utilizing label-preserving transformations to add more invariant examples of the same data samples. Data Augmentation is very important in balancing the data distribution across various classes in the datatset. Some of the data-augmentation techniques are: <code>random rotation</code>, <code>cropping</code>, <code>random flip</code>, <code>zooming</code>etc.</p> <p>Usually, the data-augmentation process is done in the CPU before uploading the batched-data for training the model on the GPU.</p>"},{"location":"blogs/deep_learning/dl_interviews/#logistic-regression","title":"Logistic Regression","text":""},{"location":"blogs/deep_learning/dl_interviews/#q3-drawbacks-of-model-fitting","title":"Q3: Drawbacks of model fitting","text":"<p>For a fixed number of observations in a dataset, introducing more number of variables normally generate a model that has a better fit to the data. What may be drawbacks of such a model fitting strategy?</p> Solution <p>Introducing more number of variables increasing the capacity of the model. If the number of data points in teh dataset is kept fixed, and then increasing the number of model parameters (variables) leads to OVERFITTING. Overfitting is a scenario where the trained model performs very well on the training data but performs poorly in the test daatset due to lack of generalization capabilites as the overly sized model just remembered the data points instead of understanding teh features &amp; data distribution in the traning set.</p> \u0939\u093f\u0928\u094d\u0926\u0940 \u092e\u0947\u0902 <p>\u092a\u094d\u0930\u0936\u094d\u0928:  Training data \u092e\u0947\u0902 data-points \u0915\u0940 \u0938\u0902\u0916\u094d\u092f\u093e \u0928\u093f\u0936\u094d\u091a\u093f\u0924 \u0930\u0916\u0924\u0947 \u0939\u0941\u090f, model-variables \u0915\u0940 \u0938\u0902\u0916\u094d\u092f\u093e \u092c\u0922\u093c\u093e\u0928\u0947 \u0938\u0947 \u0939\u092e \u090f\u0915 \u0910\u0938\u0947 trained model \u0915\u094b \u092a\u094d\u0930\u093e\u092a\u094d\u0924 \u0915\u0930 \u0938\u0915\u0924\u0947 \u0939\u0948\u0902 \u091c\u094b training data \u0915\u094b \u0905\u0924\u094d\u092f\u0902\u0924 \u0905\u091a\u094d\u091b\u0940 \u092a\u094d\u0930\u0915\u093e\u0930 \u0938\u0947 fit \u0915\u0930 \u0938\u0915\u0924\u093e \u0939\u0948\u0964 \u0907\u0938 \u092a\u094d\u0930\u0915\u094d\u0930\u093f\u092f\u093e \u0915\u0947 \u0915\u0930\u0928\u0947 \u0938\u0947 \u0915\u094d\u092f\u093e-\u0915\u094d\u092f\u093e \u0939\u093e\u0928\u093f\u092f\u093e\u0901 \u0939\u094b\u0924\u0940 \u0939\u0948\u0902?</p> <p>\u0909\u0924\u094d\u0924\u0930: Model \u0915\u0947 variables \u092c\u0922\u093c\u093e\u0928\u0947 \u0938\u0947 model \u0915\u0940 capacity \u092c\u0922\u093c\u0924\u0940 \u0939\u0948 \u0914\u0930 \u0907\u0938\u0938\u0947 \u0935\u094b training data \u0915\u094b \u0906\u091a\u0947 \u0938\u0947 \u0938\u092e\u091d \u0938\u0915\u0924\u093e \u0939\u0948 \u0914\u0930 training data-distribution \u0915\u0947 \u092c\u093e\u0930\u0947 \u092e\u0947\u0902 \u0938\u092e\u091d \u0938\u0915\u0924\u093e \u0939\u0948\u0964 \u092a\u0930\u0902\u0924\u0941 \u0905\u0917\u0930 variables \u0915\u0940 \u0938\u0902\u0916\u094d\u092f\u093e \u092c\u0922\u093c\u093e\u0928\u0947 \u0915\u0947 \u0938\u093e\u0925-\u0938\u093e\u0925 \u0905\u0917\u0930 \u0939\u092e training-data \u0915\u0940 \u0938\u0902\u0916\u094d\u092f\u093e \u0928\u0939\u0940\u0902 \u092c\u0922\u093c\u093e\u0924\u0947 \u0939\u0948\u0902 \u0924\u094b trained model \u092e\u0947\u0902 \u090f\u0915 \u0935\u093f\u0915\u0943\u0924\u093f \u0939\u094b\u0928\u0947 \u0932\u0917\u0924\u0940 \u0939\u0948 \u091c\u093f\u0938\u0947 \u0939\u092e OVERFITTING \u0915\u0939\u0924\u0947 \u0939\u0948\u0902\u0964 Overfitting \u0939\u094b\u0928\u0947 \u0938\u0947 \u0939\u092e\u093e\u0930\u093e trained model, training-data \u0915\u0947 \u0935\u093f\u0937\u092f \u092e\u0947\u0902 \u0924\u094b \u092c\u0939\u0941\u0924 \u0905\u091a\u094d\u091b\u0947 \u0938\u0947 \u091c\u093e\u0928\u0924\u093e \u0939\u0948 \u092a\u0930\u0902\u0924\u0941 \u0935\u094b test-data \u092a\u0930 \u0935\u094b \u0905\u091a\u094d\u091b\u0947 \u0938\u0947 \u0915\u093e\u0930\u094d\u092f \u0928\u0939\u0940\u0902 \u0915\u0930\u0924\u093e \u0939\u0948 \u0915\u094d\u092f\u094b\u0902\u0915\u093f training \u0915\u0947 \u0938\u092e\u092f \u0935\u094b \u0905\u092a\u0928\u0940 generalization \u0915\u094d\u0937\u092e\u0924\u093e \u0915\u094b \u0935\u093f\u0915\u094d\u0937\u093f\u0924 \u0928\u0939\u0940\u0902 \u0915\u0930 \u092a\u093e\u092f\u093e \u0914\u0930 \u0938\u0902\u092d\u0935\u0924\u0903 \u090f\u0915 \u0930\u091f\u0902\u0924 (memorized training data) model \u0939\u0940 \u092c\u0928 \u092a\u093e\u092f\u093e\u0964 \u0905\u0924\u0903 \u0939\u092e\u0947 overfitting \u0915\u094b \u092f\u0925\u093e \u0938\u0902\u092d\u0935 \u0930\u094b\u0915\u0928\u0947 \u0915\u093e \u092a\u094d\u0930\u092f\u093e\u0938 \u0915\u0930\u0928\u093e \u091a\u093e\u0939\u093f\u090f \u091c\u093f\u0938\u0915\u0947 \u0932\u093f\u092f\u0947 \u0905\u0928\u0947\u0915 \u092a\u0925 \u0905\u092a\u0928\u093e\u090f \u091c\u093e\u0924\u0947 \u0939\u0948\u0902 \u091c\u0948\u0938\u0947: (\u0967) dropout (\u0968) data augmentation (\u0969) pruning</p>"},{"location":"blogs/deep_learning/dl_interviews/#q4-odds-of-success","title":"Q4: Odds of Success","text":"<p>Define the term <code>odds of success</code>, both qualitively and formally. Give a numerical example that stresses the relationship the relationship between probability and odds of an event occuring.</p> Solution <p>Odds of Success of an event in an experiment is the ratio of probability of the event occuring and the probability of the event not occuring i.e. \\(\\left(\\frac{\\text{probability of occurance of an event E}}{1 - \\text{(probability of the occurance of the event E)}}\\right)\\)</p>"},{"location":"blogs/deep_learning/einops/","title":"einops tutorial","text":""},{"location":"blogs/deep_learning/einops/#part-1","title":"Part-1","text":""},{"location":"blogs/deep_learning/einops/#welcome-to-einops","title":"Welcome to einops","text":"<ol> <li>We don't write      <pre><code>y = x.transpose(0,2,3,1)\n</code></pre></li> <li>We write comprehensible code     <pre><code>y = einops.rearrange(x, \"b c h w -&gt; b h w c\")\n</code></pre></li> <li><code>einops</code> supports widely used tensor packages viz.      <code>numpy</code>, <code>pytorch</code>, <code>tensorflow</code>, <code>chainer</code>, <code>gluon</code>     and extends them.</li> </ol>"},{"location":"blogs/deep_learning/einops/#whats-in-this-tutorial","title":"What's in this tutorial?","text":"<ol> <li>Fundamentals: reordering, composition, and decomposition of tensors.</li> <li>Operations: <code>rearrange</code>, <code>reduce</code>, <code>repeat</code></li> <li>How much can you do with a single operation?</li> </ol>"},{"location":"blogs/deep_learning/einops/#preparations","title":"Preparations","text":"<pre><code>import numpy\nfrom utils import display_np_arrays_as_images\ndisplay_np_arrays_as_images()\n</code></pre>"},{"location":"blogs/deep_learning/einops/#load-a-batch-of-images","title":"Load a batch of images","text":"<pre><code>## there are 6 images of shape 96x96\n## with 3 color channels packed as tensors\nimages = np.load(\"./resources/tes_images.npy\", allow_pickle=False)\n\nprint(images.shape, images.dtype)   ## (6, 96, 96, 3), float64\n</code></pre> <pre><code>## display the 1st image (whole 4d tensor can't be rendered)\nimages[0]\n</code></pre> <pre><code>images[1]\n</code></pre> <p>We will use three opeartions: <code>rearrange</code>, <code>reduce</code>, <code>repeat</code> <pre><code>from einops import rearrange, reduce, repeat\n</code></pre></p>"},{"location":"blogs/deep_learning/einops/#meet-rearrange","title":"Meet \"rearrange\"","text":"rearrange <p>As its name suggests; it rearranges elements. Below, we swap <code>height</code> and <code>width</code>.</p> <p>In other words, below we transpose first two axes/dimensions. <pre><code>rearrange(images[0], \"h w c -&gt; w h c\")\n</code></pre> </p>"},{"location":"blogs/deep_learning/einops/#composition-of-axes","title":"Composition of axes","text":"<p>Transposition is very common and useful; but let's move to other  operations provided by <code>einops</code></p> composition using <code>rearrange()</code> : height <p><code>einops</code> allows seamlessly composing <code>batch</code> and <code>height</code> to a <code>new height</code> dimension.</p> <p>Below we just rendered all images in the 4D tensor by collapsing it to a 3D tensor. <pre><code>rearrange(images, \"b h w c -&gt; (b h) w c\")\n</code></pre> </p> composition using <code>rearrange()</code>: width <p><code>einops</code> allows seamlessly composing <code>batch</code> and <code>width</code> to a <code>new width</code> dimension.</p> <p>Below we just rendered all images in the 4D tensor by collapsing it to a 3D tensor. <pre><code>rearrange(images, \"b h w c -&gt; h (b w) c\")\n</code></pre> </p> <p>Resulting dimensions are computed very simply.  Length of any newly computed axes/dimension is a product of its components <pre><code>## [6, 96, 96, 3] -&gt; [96, (6*96), 3]\na = rearrange(images, \"b h w c -&gt; h (b w) c\")\na.shape\n</code></pre> <pre><code>(96, 576, 3)\n</code></pre></p> <p>We can compose more than 2 axes/dimensions. Let's flatten the whole 4D array into a 1D array. The resulting 1D array contains as many elements as the original 4D array.</p> <p><pre><code>## [6, 96, 96, 3] -&gt; [(6*96*96*3)]\na = rearrange(images, \"b h w c -&gt; (b h w c)\")\na.shape\n</code></pre> <pre><code>(165888, )\n</code></pre></p>"},{"location":"blogs/deep_learning/einops/#decomposition-of-axes","title":"Decomposition of axes","text":"<p>Decomposition is the inverse process of composition. </p> <p>It represents an existing axis as a combination of new axes.</p> <p>Several decompositions are possible. Some examples are shown below:</p> Combining composition and decomposition <p>Combining composition &amp; decomposition <pre><code>## here b1=2, decomposes b=6 into \"b1=2\" and \"b2=3\"\n## keeping b = b1*b2\na = rearrange(images, \"(b1 b2) w h c -&gt; b1 b2 w h c\", b1=2)\na.shape     ## (2, 3, 96, 96, 3)\n</code></pre> <pre><code>(2, 3, 96, 96, 3)\n</code></pre></p> An example <p>Combining composition &amp; decomposition <pre><code>## here b1=2, decomposes b=6 into \"b1=2\" and \"b2=3\"\n## keeping b = b1*b2\na = rearrange(images, \"(b1 b2) w h c -&gt; (b1 h) (b2 w) c\", b1=2)\n\na.shape     ## (2*96, 3*96, 3)\na\n</code></pre> </p> Another combination <p>Combining composition &amp; decomposition <pre><code>## here b1=2, decomposes b=6 into \"b1=2\" and \"b2=3\"\n## keeping b = b1*b2\na = rearrange(images, \"(b1 b2) w h c -&gt; (b2 h) (b1 w) c\", b1=2)\n\na.shape     ## (3*96, 2*96, 3)\na\n</code></pre> </p> Another example: <code>width_to_height</code> <p>Move part of the <code>width</code> dimension to <code>height</code></p> <p>We should call this <code>width_to_height</code> as the image <code>width</code> shrunk by 2 and <code>height</code> incresed by 2.</p> <p>But all pixels are same!!!</p> <p><pre><code>a = rearrange(images, \"b h (w w2) c -&gt; (h w2) (b w) c\", w2=2)\n\na.shape     ## (96*2, 6*48, 3)\na\n</code></pre> </p> Another example: <code>heigh_to_width</code> <p>Move part of the <code>height</code> dimension to <code>width</code></p> <p>We should call this <code>height_to_width</code> as the image <code>height</code> shrunk by 2 and <code>width</code> incresed by 2.</p> <p>But all pixels are same!!!</p> <pre><code>a = rearrange(images, \"b (h h2) w c -&gt; (b h) (w h2) c\", h2=2)\n\na.shape     ## (6*48, 96*2, 3)\n</code></pre>"},{"location":"blogs/deep_learning/einops/#order-of-axes-matter","title":"Order of axes matter","text":"<p>The order of axes in composition and decomposition is of prime importance.  It affects the way data is being transposed. Below examples show the impacts.</p> An example <p><pre><code>a = rearrange(images, \"b h w c -&gt; h (b w) c\")       ## notice the ordering of (b w)\na.shape                                             ## (96, 6*96, 3)\na\n</code></pre> </p> <p>v/s</p> <p><pre><code>b = rearrange(images, \"b h w c -&gt; h (w b) c\")       ## notice the ordeing of (w b)\nb.shape                                             ## (96, 96*6, 3)\nb\n</code></pre> </p> <p>Though the shapes of both <code>a</code> and <code>b</code> are same but the ordering of pixels are different.</p> <p><code>RULE</code>: The rule of importance is just as for digits.  The leftmost digit is most significant. Neighboring number differ in rightmost axis.</p> <p>What will happen if <code>b1</code> and <code>b2</code> are reordered before composing to <code>width</code>  (as shown in examples below): <pre><code>rearrange(images, \"(b1 b2) h w c -&gt; h (b1 b2 w) c\", b1=2)     ## produces \"einops\"\nrearrange(images, \"(b1 b2) h w c -&gt; h (b2 b1 w) c\", b1=2)     ## prodices \"eoipns\"\n</code></pre> </p>"},{"location":"blogs/deep_learning/einops/#meet-reduce","title":"Meet \"reduce\"","text":"<p>In <code>einops</code> we don't need to guess what happened (like below) <pre><code>x.mean(-1)\n</code></pre> Because we write clearly what happened (as shown below) <pre><code>import einops.reduce\n\nreduce(x, \"b h w c -&gt; b h w\", \"mean\")\n</code></pre> If an axis was not present in the output definition --you guessed it -- it was reduced</p> Average over batch <p>Average over batch <pre><code>u = reduce(images, \"b h w c -&gt; h w c\", \"mean\")      ## reduce using \"mean\" across the \"batch\" axis\nu.shape                                             ## (96, 96, 3)\nu\n</code></pre> </p> <p>The above code is similar to the standard code (without <code>einops</code>) as shown below <pre><code>u = images.mean(axis=0)     ## find mean across the \"batch\" dimension \nu.shape                     ## (96, 96, 3)\nu\n</code></pre> </p> <p>But, the code with <code>einops</code> is much more readable and states the operations clearly.</p> Reducing over multiple axes <p>Example of reducing over several dimensions.</p> <p>Besides <code>\"mean\"</code>, there are also <code>\"min\"</code>, <code>\"max\"</code>, <code>\"sum\"</code>, <code>\"prod\"</code> <pre><code>u = reduce(images, \"b h w c -&gt; h w\", \"min\")     ## redce across \"batch\" &amp; \"channel\" axes\nu.shape                                         ## (96, 96)\nu\n</code></pre> </p>"},{"location":"blogs/deep_learning/einops/#mean-pooling","title":"Mean-pooling","text":"Mean pooling with 2x2 kernel <p>Image is split into 2x2 patch and each path is avergaed <pre><code>u = reduce(images, \"b (h h2) (w w2) c -&gt; h (b w) c\", \"mean\", h2=2, w2=2)\nu.shape         ## (48, 6*48, 3)\nu\n</code></pre> </p>"},{"location":"blogs/deep_learning/einops/#max-pooling","title":"Max-pooling","text":"max-pooling with 2x2 kernel <p>Image is split into 2x2 patch and each patch is max-pooled <pre><code>u = reduce(images, \"b (h h2) (w w2) c -&gt; h (b w) c\", \"max\", h2=2, w2=2)\nu.shape         ## (49, 6*48, 3)\nu\n</code></pre> </p> yet another example <p><pre><code>u = reshape(images, \"(b1 b2) h w c -&gt; (b2 h) (b1 w)\", \"mean\", b1=2)\nu.shape         ## (3*96, 2*96)\nu\n</code></pre> </p>"},{"location":"blogs/deep_learning/einops/#stack-concatenate","title":"Stack &amp; Concatenate","text":"<pre><code>## rearrange can also take care of lists of arrays with the same shapes\n\nx = list(images)\n\n## Case-0: We can use the \"list-axis\" as 1st axis (\"b\") and rest of the axes stays as usual\nx0 = rearrange(x, \"b h w c -&gt; b h w c\")\nx0.shape                                    ## (6, 96, 96, 3)\n</code></pre> <pre><code>##----------------------------------------------------------------------------##\n## case-1: But the new axis can appear in any place\nx1 = rearrange(x, \"b h w c -&gt; h w c b\")\nx1.shape                                    ## (96, 96, 3, 6)\n\n## This is equivalent to using `numpy.stack`\nx11 = numpy.stack(x, axis=3)\nx11.shape                                   ## (96, 96, 3, 6)\n</code></pre> <pre><code>##----------------------------------------------------------------------------##\n## Case-2: ....Or we can also concatenate along axes\nx2 = rearrange(x, \"b h w c -&gt; h (b w) c\")\nx2.shape                                    ## (96, 6*96, 3)\n\n## This is equivalent to using `numpy.concatenate`\nx22 = numpy.concatenate(x, axis=1)\nx22.shape                                   ## (96. 6*96, 3)\n</code></pre>"},{"location":"blogs/deep_learning/einops/#addition-and-removal-of-axes","title":"Addition and removal of axes","text":"<p>You can write <code>1</code> to create new axis of length 1.  There is also a synonym <code>()</code> that does exactly the same</p> <p>It is exactly what <code>numpy.exapand_axis()</code> and <code>torch.unsqueeze()</code> does.</p> <pre><code>## both operations does the same as \"numpy.expand_dims()\" or \"torch.unsqueeze()\"\nu = rearrange(images, \"b h w c -&gt; b 1 h w 1 c\")\nv = rearrange(images, \"b h w c -&gt; b () h w () c\")\n\nu.shape         ## (6, 1, 96, 96, 1, 3)\nv.shape         ## (6, 1, 96, 96, 1, 3)\n</code></pre> <p>The <code>numpy.squeeze()</code> operation is also facilitated by <code>rearrange()</code> as usual. <pre><code>u = rearrange(images, \"b h w c -&gt; b 1 h w 1 c\")         ## torch.unsqueeze()\nv = rearrange(u, \"b 1 h w 1 c -&gt; b h w c\")              ## torch.unsqueeze()\n\nv.shape                                                 ## (6, 96, 96, 3)\n</code></pre></p> An example usage <p>Compute max in each image individually and then show a difference <pre><code>x = reduce(images, \"b h w c -&gt; b () () c\", max)\nx -= images\ny = rearrange(x, \"b h w c -&gt; h (b w) c\")\ny.shape                                             ## (96, 6*96, 3)\n</code></pre> </p>"},{"location":"blogs/deep_learning/einops/#meet-repeat-repeating-elements","title":"Meet \"repeat\": Repeating elements","text":"<p>This is the third operation in <code>einops</code> library</p> <p> Repeat along a new axis. The new axis can be placed anywhere. <pre><code>u = repeat(images[0], \"h w c -&gt; h new_axis w c\", new_axis=5)\nu.shape         ## (96, 5, 96, 3)\n\n## -- OR -- a shortcut\n\nv = repeat(images[0], \"h w c -&gt; h 5 w c\")   ## repeats 5 times in the new axis.\nv.shape         ## (96, 5, 96, 3)\n</code></pre></p> <p> Repat along an existing axis <pre><code>## repeats the width 3 times\nu = repeat(images[0], \"h w c -&gt; h (repeat w) c\", repeat=3)\nu.shape         ## (96, 3*96, 3)\n</code></pre></p> <p></p> <p> Repeat along multiple existing axes <pre><code>u = repeat(images[0], \"h w c -&gt; (2 h) (2 w) c\")\nu.shape         ## (2*96, 2*96, 3)\n</code></pre></p> <p></p> <p> Order of axes matter as usual. You can repeat each pixel 3 times by changing the order of axes in repeat <pre><code>## repeat the pixels along the width dim. 3 times\nu = repeat(images[0], \"h w c -&gt; h (w repeat) c\", repeat=3)\nu.shape         ## (96, 96*3, 3)\n</code></pre></p> <p></p> <p> NOTE: The <code>repeat</code> operation covers <code>numpy.tile</code>, <code>numpy.repeat</code> and much more.</p>"},{"location":"blogs/deep_learning/einops/#reduce-vs-repeat","title":"reduce v/s repeat","text":"<p><code>reduce</code> and <code>repeat</code> are opposite of each other. </p> <ol> <li><code>reduce</code>: reduces amount of elements</li> <li><code>repeat</code>: increases the number of elements.</li> </ol> An example of <code>reduce</code> v/s <code>repeat</code> <p>In this example each image is repeated first then reduced over the <code>new_axis</code> to get back the original tensor. <pre><code>repeated = repeat(images, \"b h w c -&gt; b h new_axis w c\", new_axis=2)\nreduced = reduce(repeated, \"b h new_axis w c -&gt; b h w c\", \"min\")\n\nrepeated.shape                                  ## (6, 96, 2, 96, 3)\nreduced.shape                                   ## (6, 96, 96, 3)\n\nassert numpy.array_equal(images, reduced)       ## True\n</code></pre> Notice that the operation pattern in <code>reduce</code> and <code>repeat</code> are reverse of each other. i.e. </p> <p>in <code>repeat</code> its <code>\"b h w c -&gt; b h new_axis w c\"</code> but</p> <p>in <code>reduce</code> its <code>\"b h new_axis w c -&gt; b h w c\"</code></p>"},{"location":"blogs/deep_learning/einops/#some-more-examples","title":"Some more examples","text":"Interwaving pixels of different pictures <p>All letters can be observed in the final image <pre><code>u = rearrange(images, \"(b1 b2) h w c -&gt; (h b1) (w b2) c\", b1=2)\nu.shape             ## (2*96, 3*96, 3)\n</code></pre> </p> Interweaving along vertical for couple of images <p><pre><code>u = rearrange(images, \"(b1 b2) h w c -&gt; (h b1) (b2 w) c\", b1=2)\nu.shape             ## (96*2, 3*96, 3)\n</code></pre> </p> Interweaving lines for couple of images <p><pre><code>u = reduce(images, \"(b1 b2) h w c -&gt; h (b2 w) c\", \"max\", b1=2)\nu.shape             ## (96, 3*96, 3)\n</code></pre> </p> Decomposing color into different axes <p>Here we decompose color dimension into different axes. We also downsample the image. <pre><code>u = reduce(images, \"b (h 2) (w 2) c -&gt; (c h) (b w)\", \"mean\")\nu.shape             ## (3*48, 6*48)\n</code></pre> </p> Disproportionate resize <p><pre><code>u = reduce(images, \"b (h 3) (w 4) c -&gt; (h) (b w)\", \"mean\")\nu.shape             ## (96/3, 6*96/4)\n</code></pre> </p> Split &amp; Reduce <p>Split each image into two halves and compute the mean of the two halves. <pre><code>u = reduce(images, \"b (h1 h2) w c -&gt; h2 (b w)\", \"mean\", h1=2)\nu.shape             ## (96/2, 6*96)\n</code></pre> </p> Split and Transpose <p>Split into small patches and transpose each patch. <pre><code>## splitting each image into 96/8 * 96/8 = 12*12 = 144 patches\n## each patch is of shape (8, 8)\nu = rearrange(images, \"b (h1 h2) (w1 w2) c -&gt; (h1 w2) (b w1 h2) c\", h2=8, w2=8)\nu.shape             ## (12*8, 6*12*8, 3)\n</code></pre> </p> Another Split &amp; Transpose <p>This is crazy <pre><code>u = rearrange(images,\n              \"b (h1 h2 h3) (w1 w2 w3) c -&gt; (h1 w2 h3) (b w1 h2 w3) c\",\n              h2=2, h3=2, w2=2, w3=2)\nu.shape             ## (96/(2*2)*2*2, 6*96/(2*2)*2*2, c) = (96, 6*96, 3)\n</code></pre> </p> Yet another Split &amp; Transpose <p>This is crazy crazy.... <pre><code>u = rearrange(images,\n              \"(b1 b2) (h1 h2) (w1 w2) c -&gt; (h1 b1 h2) (w1 b2 w2) c\",\n              h1=3, w1=3, b2=3)\nu.shape             ## (3*(6/3)*(96/3), 3*3*(96/3), 3) = (192, 288, 3)\n</code></pre> </p> Arbitrarily Complicated Pattern <p><pre><code>u = reduce(images,\n           \"(b1 b2) (h1 h2 h3) (w1 w2 w3) c -&gt; (h1 w1 h3) (b1 w2 h2 w3 b2) c\", \n           \"mean\",\n           w1=2, w3=2, h2=2, h3=2, b2=2)\n</code></pre> </p> Subtract background &amp; Normalize <p>Subtract background in each image individually and normalize.</p> <p>**  NOTE: Pay attention to <code>()</code> -- this is a composition of <code>0</code> axis (a dummy axis with 1 element)**</p> <p><pre><code>u = reduce(images, \"b h w c -&gt; b () () c\", \"max\")   ## finding per-image per-channel max\nu -= images                                         ## subtracting\nu /= reduce(u, \"b h w c -&gt; b () () c\", \"max\")       ## NORMALIZATION\nu = rearrange(u, \"b h w c -&gt; h (b w) c\")\n\nu.shape                                             ## (96, 6*96, 3)\n</code></pre> </p> PIXELATE <p>First downscale by averaging then upscale by using the same pattern. <pre><code>## downscale using \"mean\" kernel of size (6, 8)\ndownscaled = reduce(images, \"b (h h2) (w w2) c -&gt; b h w c\", \"mean\", h2=6, w2=8)\nupscaled = repeat(downscaled, \"b h w c -&gt; b (h h2) (w w2) c\", h2=6, w2=8)\nv = rearrange(upscaled, \"b h w c -&gt; h (b w) c\")\n\ndownscaled.shape            ## (6, 96/6, 96/8, 3)\nupscaled.shape              ## (6, (96/6)*6, (96/8)*8, 3) = (6, 96, 96, 3)\nv.shape                     ## (96, 6*96, 3)\n</code></pre> </p> ROTATE <p><pre><code>u = rearrange(images, \"b h w c -&gt; w (b h) c\")       ## rotation of (width &lt;-&gt; height) \nu.shape             ## (96, 6*96, 3)\n</code></pre> </p> Another Example <p>Let's bring the <code>channel</code> dimension as part of the <code>width</code> axis.</p> <p>Also, at the same time downsample the <code>width</code> axis by 2x <pre><code>u = reduce(images, \n           \"b (h h2) (w w2) c -&gt; (h w2) (b w c)\", \n           \"mean\", \n           h2=3, w2=3)\n</code></pre> </p>"},{"location":"blogs/deep_learning/einops2/","title":"Popular Deep Learning Architectures using EINOPS","text":"<p>In this section we will be rewriting the building blocks of deep learning in both the traditional <code>PyTorch</code> way as well as using <code>einops</code> library.</p>"},{"location":"blogs/deep_learning/einops2/#imports","title":"Imports","text":"<p>Firstly, we will import the necessary libraries to be used.</p> <pre><code>## importing necessary libraries\n\nimport math\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom einops import rearrange, reduce, repeat, asnumpy, parse_shape\nfrom einops.layers.torch import Rearrange, Reduce\n</code></pre>"},{"location":"blogs/deep_learning/einops2/#simple-convnet","title":"Simple ConvNet","text":"Using only PyTorch <p>Here is an implementation of a simple ConvNet using only <code>PyTorch</code> without <code>einops</code>.</p> <pre><code>class ConvNet(nn.Module):\n    def __init__(self):\n        super(ConvNet, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(20, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.max_pool2d(x, 2)\n        x = F.relu(x)\n\n        x = self.conv2(x)\n        x = self.conv2_drop(x)\n        x = F.max_pool2d(x, 2)\n        x = F.relu(x)\n\n        x = x.view(-1, 320)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n## Instantiating the ConvNet class\n\nconv_net_old = ConvNet()\n</code></pre> Using EINOPS + PyTorch <p>Implementing the same above ConvNet using <code>einops</code> &amp; <code>PyTorch</code></p> <pre><code>conv_net_new = nn.Sequential(\n    nn.Conv2d(1, 10, kernel_size=5),\n    nn.MaxPool2d(kernel_size=2),\n    nn.ReLU(),\n    nn.Conv2d(10, 20, kernel_size=5),\n    nn.MaxPool2d(kernel_size=2),\n    nn.ReLU(),\n    nn.Dropout2d(),\n    Rearrange(\"b c h w -&gt; b (c h w)\"),\n    nn.Linear(320, 50),\n    nn.ReLU(),\n    nn.Dropout(),\n    nn.Linear(50, 10),\n    nn.LogSoftmax(dim=1)\n)\n</code></pre> Why prefer EINOPS implementation? <p>Following are the reasons to prefer the new implementation:</p> <ul> <li> In the original code, if the input is changed and the <code>batch_size</code>          is divisible by 16 (which usually is), we will get something senseless after reshaping.<ul> <li>  The new code using <code>einops</code> explicitly raise ERROR in the above scenario. Hence better!!</li> </ul> </li> <li> We won't forget to use the flag <code>self.training</code> with the new implementation.</li> <li> Code is straightforward to read and analyze.</li> <li> <code>nn.Sequential</code> makes printing/saving/passing trivial.          And there is no need in your code to load the model (which also has lots of benefits).</li> <li> Don't need <code>logsoftmax</code>? Now, you can use <code>conv_net_new[-1]</code>.          Another reason to prefer <code>nn.Sequential</code></li> <li> ... And we culd also add inplace <code>ReLU</code></li> </ul>"},{"location":"blogs/deep_learning/einops2/#super-resolution","title":"Super-resolution","text":"Only PyTorch <pre><code>class SuperResolutionNetOLD(nn.Module):\n    def __init__(self, upscale_factor):\n        super(SuperResolutionNetOLD, self).__init__()\n\n        self.relu = nn.ReLU()\n        self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2))\n        self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1))\n        self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1))\n        self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1))\n        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n\n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        x = self.relu(self.conv3(x))\n        x = self.pixel_shuffle(self.conv4(x))\n        return x\n</code></pre> Using EINOPS <pre><code>def SuperResolutionNetNEW(upscale_factor):\n    return nn.Sequential(\n        nn.Conv2d(1, 64, kernel_size=5, padding=2),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(64, 64, kernel_size=3, padding=1),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(64, 32, kernel_size=3, padding=1),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(32, upscale_factor ** 2, kernel_size=3, padding=1),\n        Rearrange(\"b (h2 w2) h w -&gt; b (h h2) (w w2)\", h2=upscale_factor, w2=upscale_factor)\n    )\n</code></pre> Improvements over the old implementation <ul> <li> No need in special instruction <code>pixel_shuffle</code> (&amp; the result is transferrable b/w the frameworks)</li> <li> Output does not contain a fake axis (&amp; we could do the same for the input)</li> <li> inplace <code>ReLU</code> used now. For high resolution images this becomes critical         and saves a lot of memory.</li> <li> and all the benefits of <code>nn.Sequential</code></li> </ul>"},{"location":"blogs/deep_learning/einops2/#gram-matrix-style-transfer","title":"Gram Matrix / Style Transfer","text":"<p>Restyling Graam Matrix for style transfer.</p> Original Code using ONLY PyTorch <p>The original code is already very good. First line shows what kind of input is expected.</p> <pre><code>def gram_matrix_old(y):\n    (b, c, h, w) = y.size()\n    features = y.view(b, c, h * w)\n    features_t = features.transpose(1, 2)\n    gram = features.bmm(features_t) / (c * h * w)\n    return gram\n</code></pre> Using EINSUM <pre><code>def gram_matrix_new(y):\n    b, c, h, w = y.shape\n    return torch.einsum(\"bchw, bdhw -&gt; bcd\", [y, y]) / (h * w)\n</code></pre> Improvements <p><code>einsum</code> operations should be read like:</p> <ul> <li> For each batch &amp; each pair of channels we sum over <code>h</code> and <code>w</code>.</li> <li> The normalization is also changed, because that's how Gram Matrix is defined.         Else we should call it Normalized Gram Matrix or alike.</li> </ul>"},{"location":"blogs/deep_learning/einops2/#recurrent-models-rnns","title":"Recurrent Models (RNNs)","text":"ONLY PyTorch <pre><code>class RNNModelOLD(nn.Module):\n    \"\"\"Container module with an ENCODER, a RECURRENT module &amp; a DECODER module\"\"\"\n    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n        super(RNNModelOLD, self).__init__()\n        self.drop = nn.Dropout(dropout)\n        self.encoder = nn.Embedding(ntoken, ninp)\n        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n        self.decoder = nn.Linear(nhid, ntoken)\n\n    def forward(self, input, hidden):\n        emb = self.drop(self.encoder(input))\n        output, hidden = self.rnn(emb, hidden)\n        output = self.drop(output)\n        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n</code></pre> Using EINOPS <pre><code>def RNNModelNEW(nn.Module):\n    \"\"\"Container module with an ENCODER, RNN &amp; a DECODER modules.\"\"\"\n    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n        super(RNNModelNEW, self).__init__()\n        self.drop = nn.Dropout(dropout)\n        self.encoder = nn.Embedding(ntoken, ninp)\n        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n        self.decoder = nn.Linear(nhid, ntoken)\n\n    def forward(self, input, hidden):\n        t, b = input.shape[:2]\n        emb = self.drop(self.encoder(input))\n        output, hidden = self.rnn(emb, hidden)\n        output = rearrange(self.drop(output), \"t b nhid -&gt; (t b) nhid\")\n        decoded = rearrange(self.decoder(output), \"(t b) token -&gt; t b token\", t=t, b=b)\n        return decoded, hidden\n</code></pre>"},{"location":"blogs/deep_learning/einops2/#improving-rnn","title":"Improving RNN","text":"Only PyTorch <pre><code>class RNNold(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        embedding_dim,\n        hidden_dim,\n        output_dim,\n        n_layers,\n        bidirectional,\n        dropout\n    ):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.rnn = nn.LSTM(embedding_dim,\n                            hidden_dim,\n                            num_layers=n_layers,\n                            bidirectional=bidirectional,\n                            dropout=dropout)\n        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        ## x = [sent_len, batch_size]\n        embedded = self.dropout(self.embedding(x))      ## size = [sent_len, batch_size, emb_dim]\n        output, (hidden, cell) = self.rnn(embedded)\n\n        ## output.shape = [sent_len, batch_size, hid_dim * num_directions]\n        ## hidden.shape = [num_layers * num_directions, batch_size, hid_dim]\n        ## cell.shape = [num_layers * num_directions, batch_size, hid_dim]\n\n        ## concat the final dropout (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n        ## and apply dropout\n        ## hidden.size = [batch_size, hid_dim * num_directions]\n        hidden = self.dropout(torch.cat([hidden[-2,:,:], hidden[-1,:,:]], dim=1))\n\n        return self.fc(hidden.squeeze(0))\n</code></pre> Using EINOPS <pre><code>class RNNnew(nn.Module):\n    def __init__(\n        self,\n        vocab_size,\n        embedding_dim,\n        hidden_dim,\n        output_dim,\n        n_layers,\n        bidirectional,\n        dropout\n    ):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.rnn = nn.LSTM(embedding_dim,\n                            hidden_dim,\n                            num_layers=n_layers,\n                            bidirectional=bidirectional,\n                            dropout=dropout)\n        self.dropout = nn.Dropout(dropout)\n        self.directions = 2 if bidirectional else 1\n        self.fc = nn.Linear(hidden_dim * self.directions, output_dim)\n\n    def forward(self, x):\n        embedded = self.dropout(self.embedding(x))\n        output, (hidden, cell) = self.rnn(embedded)\n        hidden = rearrange(hidden, \"(layer dir) b c -&gt; layer b (dir c)\", dir=self.directions)\n\n        ## take the fina layer's hidden\n        return self.fc(self.dropout(hidden[-1]))\n</code></pre>"},{"location":"blogs/deep_learning/einops2/#channel-shuffle-from-shufflenet","title":"Channel Shuffle (from ShuffleNet)","text":"ONLY PyTorch <pre><code>def channel_shuffle_old(x, groups):\n    b, c, h, w = x.data.size()\n    channels_per_group = c // groups\n\n    ## reshape\n    x = x.view(b, groups, channels_per_group, h, w)\n\n    ## transpose\n    ## - contiguous() is required if transpose() is used before view()\n    ##   See https://github.com/pytorch/pytorch/issues/764\n    x = x.transpose(1, 2).contiguous()\n\n    ## flatten\n    x = x.view(b, -1, h, w)\n    return x\n</code></pre> Using EINOPS <pre><code>def channel_shuffle_new(x, groups):\n    return rearrange(x, \"b (c1 c2) h w -&gt; b (c2 c1) h w\", c1=groups)\n</code></pre>"},{"location":"blogs/deep_learning/einops2/#shufflenet","title":"ShuffleNet","text":"ONLY PyTorch <pre><code>from collections import OrderedDict\n\ndef channel_shuffle(x, groups):\n    b, c, h, w = x.data.size()\n    channels_per_group = c // groups\n\n    ## reshape\n    x = x.view(b, groups, channels_per_group, h, w)\n\n    ## transpose\n    ## - contiguous() is required if transpose() is used before view()\n    x = x.transpose(1, 2).contiguous()\n\n    x = x.view(b, -1, h, w)\n    return x\n\nclass ShuffleUnitOLD(nn.Module):\n    def __init__(self, \n                 in_channels, \n                 out_channels,\n                 groups=3,\n                 grouped_conv=True,\n                 combine=\"add\",\n    ):\n        super(ShuffleUnitOLD, self).__init__()\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.grouped_conv = grouped_conv\n        self.combine = combine\n        self.groups = groups\n        self.bottleneck_channels = self.out_channels // 4\n\n        ## define the type of ShuffleUnit\n        if self.combine == \"add\":\n            ## shuffleUnit fig-2b\n            self.depthwise_stride = 1\n            self._combine_func = self._add\n        elif self.combine == \"concat\":\n            ## ShuffleUnit fig-2c\n            self.depthwise_stride = 2\n            self._combine_func = self._concat\n\n            ## ensure output of the concat has the same channels\n            ## as the original input channels\n            self.out_channels -= self.in_channels\n        else:\n            raise ValueError(f\"Cannot combine tensors with {self.combine}.\\n\"\n                             f\"Only 'add' &amp; 'concat' supported.\")\n\n        ## Use a 1x1 grouped or non-grouped convolution to reduce input channels\n        ## to bottleneck channels, as in ResNet bottleneck module.\n        ## NOTE: do not use group convolution for the first conv1x1 in stage-2\n        self.first_1x1_groups = self.groups if grouped_conv else 1\n\n        self.g_conv_1x1_compress = self._make_grouped_conv1x1(\n            self.in_channels,\n            self.bottleneck_channels,\n            self.first_1x1_groups,\n            batch_norm=True,\n            relu=True,\n        )\n\n        ## 3x3 depthwise convolution followed by batch normalization\n        self.depthwise_conv3x3 = conv3x3(\n            self.bottleneck_channels,\n            self.bottleneck_channels,\n            stride=self.depthwise_stride,\n            groups=self.bottleneck_channels\n        )\n        self.bn_after_depthwise = nn.BatchNordm2d(self.bottleneck_channels)\n\n        ## use 1x1 grouped convolution to expand from bottleneck_channels to out_channels\n        self.g_conv_conv_1x1_expand = self._make_grouped_conv1x1(\n            self.bottleneck_channels,\n            self.out_channels,\n            self.groups,\n            batch_norm=True,\n            relu=False\n        )\n\n\n    @staticmethod\n    def _add(x, out):\n        ## residual connection\n        return x + out\n\n    @staticmethod\n    def _concat(x, out):\n        ## concat along channel dim\n        return torch.cat((x, out), 1)\n\n    def _make_grouped_conv1x1(\n        self,\n        in_channels,\n        out_channels,\n        groups,\n        batch_norm=True,\n        relu=False\n    ):\n        modules = OrderedDict()\n        conv = conv1x1(in_channels, out_channels, groups=groups)\n        modules['conv1x1'] = conv\n\n        if batch_norm:\n            modules['batch_norm'] = nn.BatchNorm2d(out_channels)\n        if relu:\n            modules['relu'] = nn.ReLU()\n        if len(modules) &gt; 1:\n            return nn.Sequential(modules)\n        else:\n            return conv\n\n    def forward(self, x):\n        ## save for combining later with output\n        residual = x\n        if self.combine == \"concat\":\n            residual = F.avg_pool2d(residual, kernel_size=3, stride=2, padding=1)\n\n        out = self.g_con_1x1_compress(x)\n        out = channel_shuffle(out, self.groups)\n        out = self.depthwise_conv3x3(out)\n        out = self.bn_after_depthwise(out)\n        out = self.g_conv_1x1_expand(out)\n\n        out = self._combine_func(residual, out)\n        return F.relu(out)\n</code></pre> Using EINOPS <pre><code>class ShuffleUnitNEW(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        groups=3,\n        grouped_conv=True,\n        combine=\"add\"\n    ):\n        super().__init__()\n        first_1x1_groups = groups if grouped_conv else 1\n        bottleneck_channels = out_channels // 4\n        self.combine = combine\n        if combine == \"add\":\n            ## ShuffleUnit fig-2b\n            self.left = Rearrange(\"...-&gt;...\")   ## identity\n            depthwise_stride = 1\n        else:\n            ## ShuffleUnit fig-2c\n            self.left = nn.AvgPool2d(kernel_size=3, stride=2, padding=1)\n            depthwise_stride = 2\n            ## ensure output of concat has the same channels as the original output channels\n            out_channels -= in_channels\n            assert out_channels &gt; 0\n\n        self.right = nn.Sequential(\n            ## use a 1x1grouped or non-grouped convolution to reduce\n            ## input channels to bottleneck channels as in ResNet bottleneck module.\n            conv1x1(in_channels, bottleneck_channels, groups=first_1x1_groups),\n            nn.BatchNorm2d(bottleneck_channels),\n            nn.ReLU(inplace=True),\n            ## channel shuffle\n            Rearrange(\"b (c1 c2) h w -&gt; b (c2 c1) h w\", c1=groups),\n            ## 3x3 depthwise convolution followed by BatchNorm\n            conv3x3(bottleneck_channels, \n                    bottleneck_channels, \n                    stride=depthwise_stride,\n                    groups=bottleneck_channels),\n            nn.BatchNorm2d(bottleneck_channels),\n            ## Use 1x1 grouped convolution to expand from bottleneck_channels to output_channels\n            conv1x1(bottleneck_channels, out_channels, groups=groups),\n            nn.BatchNorm2d(out_channels),\n        )\n\n    def forward(self, x):\n        if self.combine == \"add\":\n            combined = self.left(x) + self.right(x)\n        else:\n            combined = torch.cat([self.left(x), self.right(x)], dim=1)\n        return F.relu(combined, inplace=True)\n</code></pre> Improvements <p>Rewriting the code helped to identify the following:</p> <ul> <li> There is no sense in doing reshuffling and not using groups in the first convolution         (indeed in the paper it is not so). However , the result is an equivalent model.</li> <li> It is strage that the first convolution may not be grouped,         while the last convolution is always grouped. (and th's different from the paper)</li> </ul> <p>Also,</p> <ul> <li> There is an identity layer for pyTorch introduced here.</li> <li> The last thing to do is to get rid of <code>conv1x1</code> and <code>conv3x3</code>          (those are not better than the standard implementation)</li> </ul>"},{"location":"blogs/deep_learning/einops2/#resnet","title":"ResNet","text":"ONLY PyTorch <pre><code>class ResNetOLD(nn.Module):\n    def __init__(self, block, layers, num_classes=1000):\n        self.inplanes = 64\n        super(ResNetOLD, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaaxPool2d(kernel_size=3, stride=2, paadding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(7, stride=1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules:\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2./n))\n            elif isinstance(m, nn.BatchNord2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes*block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n</code></pre> Using EINOPS <pre><code>def make_layer(inplanes, planes, block, n_blocks, stride=1):\n    downsample = None\n    if stride != 1 or inplanes != planes * block.expansion:\n        ## output-size won't match input-size; so adjust the residual\n        downsample = nn.Sequential(\n            nn.Conv2d(inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n            nn.BatchNorm2d(planes * block.expansion),\n        )\n\n    return nn.Sequential(\n        block(inplanes, planes, stride, downsample),\n        *[block(planes * block.expansion, planes) for _ in range(1, n_blocks)]\n    )\n\ndef ResNetNEW(block, layers, num_classes=1000):\n    e = block.expansion\n\n    resnet = nn.Sequential(\n        Rearrange(\"b c h w -&gt; b c h w\", c=3, h=224, w=224),\n        nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n        nn.BatchNorm2d(64),\n        nn.ReLU(inplace=True),\n        nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n        make_layer(64,      64,  block, layers[0], stride=1),\n        make_layer(64 * e,  128, block, layers[1], stride=2),\n        make_layer(128 * e, 256, block, layers[2], stride=2),\n        make_layer(256 * e, 512, block, layers[3], stride=2),\n        ## Combined AvgPool &amp; view in one single operation\n        Reduce(\"b c h w -&gt; b c\", \"mean\"),\n        n.Linear(512 * e, num_classes),\n    )\n\n    ## initialization\n    for m in resnet.modules():\n        if isinstance(m, nn.Conv2d):\n            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            m.weight.data.normal_(0, math.sqrt(2./n))\n        elif isinstance(m, nn.BatchNorm2d):\n            m.weight.data.fill_(1.)\n            m.bias.data.zero_()\n\n    return resnet\n</code></pre>"},{"location":"blogs/deep_learning/einops2/#fasttext","title":"FastText","text":"ONLY PyTorch <pre><code>class FastTextOLD(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, output_dim):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.fc = nn.Linear(embedding_dim, output_dim)\n\n    def forward(self, x):\n        embedded = self.embedding(x)\n        embedded = embedded.permute(1, 0, 2)\n        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1)\n        return self.fc(pooled)\n</code></pre> Using EINOPS <pre><code>def FastTextNEW(vocab_size, embedding_dim, output_dim):\n    return nn.Sequential(\n        Rearrange(\"t b -&gt; t b\"),\n        nn.Embedding(vocab_size, embedding_dim),\n        Reduce(\"t b c -&gt; b c\", \"mean\"),\n        nn.Linear(embedding_dim, output_dim),\n        Rearrange(\"b c -&gt; b c\"),\n    )\n</code></pre> <ul> <li> Here, the first and last operations (highlighted) do nothing and can be removed.         But, were added to explicitly added to show expected input and output shape</li> <li> This also gives us the flexibility of changing interface by editing a single line.         Should you need to accept inputs of shape <code>(b, t)</code> we just need to change          the line to <code>Rearrange(\"b t -&gt; t b\")</code></li> </ul>"},{"location":"blogs/deep_learning/einops2/#cnns-for-text-classification","title":"CNNs for text classification","text":"ONLY PyTorch <pre><code>class CNNold(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.conv_0 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[0], embedding_dim))\n        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[1], embedding_dim))\n        self.conv_2 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[2], embedding_dim))\n        self.fc = nn.Linear(len(filter_sizes)*n_filters, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = x.permute(1, 0)\n        embedded = self.embedding(x)\n        embedded = embedded.unsqueeze(dim=1)\n\n        conved_0 = F.relu(self.conv_0(embedded).squeeze(dim=3))\n        conved_1 = F.relu(self.conv_1(embedded).squeeze(dim=3))\n        conved_2 = F.relu(self.conv_2(embedded).squeeze(dim=3))\n\n        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(dim=2)\n        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(dim=2)\n        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(dim=2)\n\n        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n        return self.fc(cat)\n</code></pre> Using EINOPS <pre><code>class CNNnew(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.convs = nn.ModuleList([nn.Conv1d(embedding_dim, n_filters, kernel_size=size) \n                                    for size in filter_sizes])\n        self.fc = nn.Linear(len(filter_sizes)*n_filters, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = rearrange(x, \"t b -&gt; t b\")\n        emb = rearrange(self.embedding(x), \"t b c -&gt; b c t\")\n        pooled = [reduce(conv(emb), \"b c t -&gt; b c\") for conv in self.convs]\n        concatenated = rearrange(pooled, \"filter b c -&gt; b (filter c)\")\n        return self.fc(self.dropout(F.relu(concatenated)))\n</code></pre> Discussion <ul> <li> Original code misuses <code>nn.Conv2d</code> while <code>nn.Conv1d</code> is the right choice.</li> <li> New code can work with any number of <code>filter_sizes</code> and won't fail.</li> <li> First line in the new code does nothing, but was just added for simplicity &amp; clarity of shapes.</li> </ul>"},{"location":"blogs/deep_learning/einops2/#highway-convolutions","title":"Highway Convolutions","text":"ONLY PyTorch <pre><code>class HighwayConv1dOLD(nn.Conv1d):\n    def forward(self, inputs):\n        L = super(HIghwayCon1dOLD, self).forward(inputs)\n        H1, H2 = torch.chunk(L, 2, dim=1)   ## chunk at the feature dimension\n        torch.sigmoid_(H1)\n        return H1 * H2 + (1.0 - H1) * inputs\n</code></pre> Using EINOPS <pre><code>class HighwayConv1dNEW(nn.Conv1d):\n    def forward(self, inputs):\n        L = super().forward(inputs)\n        H1, H2 = rearrange(L, \"b (split c) t -&gt; split b c t\", split=2)\n        torch.sigmoid_(H1)\n        return H1 * H2 + (1.0 - H1) * inputs\n</code></pre>"},{"location":"blogs/deep_learning/einops2/#simple-attention","title":"Simple Attention","text":"ONLY PyTorch <pre><code>class Attention(nn.Module):\n    def __init__(self):\n        super(Attention, self).__init__()\n\n    def forward(self, K, Q, V):\n        A = torch.bmm(K.transpose((1, 2), Q) / np.sqrt(Q.shape[1])\n        A = F.softmax(A, dim=1)\n        R = torch.bmm(V, A)\n        return torch.cat((R, Q), dim=1)\n</code></pre> Using EINOPS <pre><code>def attention(K, Q, V):\n    _, n_channels, _ = K.shape\n    A = torch.einsum(\"bct,bcl-&gt;btl\", [K, Q])\n    A = F.softmax(A * n_channels ** (-0.5), dim=1)\n    R = torch.einsum(\"bct,btl-&gt;bcl\", [V, A])\n    return torch.cat((R, Q), dim=1)\n</code></pre>"},{"location":"blogs/deep_learning/einops2/#multi-head-attention","title":"Multi-head Attention","text":"ONLY PyTorch <pre><code>class ScaledDotProductAttention(nn.Module):\n    \"\"\"Scaled Dot Product Attention\"\"\"\n    def __init__(self, temperature, attn_dropout=0.1):\n        super().__init__()\n        self.temperature = temperature\n        self.dropout = nn.Dropout(attn_dropout)\n        self.softmax = nn.Softmax(dim=2)\n\n    def forward(self, q, k, v, mask=None):\n        attn = torch.bmm(q, k.transpose(1, 2))\n        attn /= self.temperature\n        if mask is not None:\n            attn = attn.masked_fill(mask, -np.inf)\n        attn = self.softmax(attn)\n        attn = self.dropout(attn)\n        output = torch.bmm(attn, v)\n        return output, attn\n\nclass MultiHeadAttentionOLD(nn.Module):\n    \"\"\"Multi Head Attention Module\"\"\"\n    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n        super().__init__()\n        self.d_k = d_k\n        self.d_v = d_v\n        self.n_head = n_head\n\n        self.w_qs = nn.Linear(d_model, n_head * d_k)\n        self.w_ks = nn.Linear(d_model, n_head * d_k)\n        self.w_vs = nn.Linear(d_model, n_head * d_v)\n        nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n        nn.init.normal_(self.w_ks.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n        nn.init.normal_(self.w_vs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_v)))\n\n        self.attention = ScaledDotProductAttention(temperature=d_k**0.5)\n        self.layer_norm = nn.LayerNorm(d_model)\n        self.fc = nn.Linear(n_head * d_v, d_model)\n        nn.init.xavier_normal_(self.fc.weight)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, q, k, v, mask=None):\n        d_k = self.d_k\n        d_v = self.d_v\n        n_head = self.n_head\n\n        sz_b, len_q, _ = q.size()\n        sz_b, len_k, _ = k.size()\n        sz_b, len_v, _ = v.size()\n\n        residual = q\n        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n\n        q = q.permute(2, 0, 1, 3).contiguous().view(-1, len_q, d_k)     ## (n*b, len_q, d_k)\n        k = k.permute(2, 0, 1, 3).contiguous().view(-1, len_k, d_k)     ## (n*b, len_k, d_k)\n        v = v.permute(2, 0, 1, 3).contiguous().view(-1, len_v, d_v)     ## (n*b, len_v, d_v)\n\n        mask = mask.repeat(n_head, 1, 1)    ## (n*b, ...)\n        output, attn = self.attention(q, k, v, mask=mask)\n        output = output.view(n_head, sz_b, len_q, d_v)\n        output = output.permute(1, 2, 0, 3).contiguous().view(sz_b, len_q, -1)      ## (b, len_q, n*d_v)\n        output = self.dropout(self.fc(output))\n        output = self.layer_norm(output + residual)\n\n        return output, attn\n</code></pre> Using EINOPS <pre><code>class MultiHeadAttentionNEW(nn.Module):\n    def __init__(self, n_heads, d_model, d_k, d_v, dropout=0.1):\n        super().__init__()\n        self.n_heads = n_heads\n\n        self.w_qs = nn.Linear(d_model, n_heads * d_k)\n        self.w_ks = nn.Linear(d_model, n_heads * d_k)\n        self.w_vs = nn.Linear(d_model, n_heads * d_v)\n\n        nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt(2.0 / (d-model + d_k)))\n        nn.init.normal_(self.w_ks.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n        nn.init.normal_(self.w_vs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_v)))\n\n        self.fc = nn.Linear(n_heads * d_v, d_model)\n        nn.init.xavier_normal_(self.fc.weight)\n        self.dropout = nn.Dropout(dropout)\n        self.layer_norm = nn.LayerNorm(d_model)\n\n    def forward(self, q, k, v, mask=None):\n        residual = q\n</code></pre>"},{"location":"blogs/deep_learning/einops2/#references","title":"References","text":"<ol> <li> <p>http://einops.rocks/pytorch-examples.html \u21a9</p> </li> <li> <p>https://github.com/arogozhnikov/einops \u21a9</p> </li> </ol>"},{"location":"blogs/deep_learning/loss/","title":"Loss Functions","text":""},{"location":"blogs/deep_learning/loss/#loss-functions","title":"Loss Functions","text":""},{"location":"blogs/deep_learning/loss/#cross-entropy","title":"Cross Entropy","text":"<p>Cross Entropy is usually used in multiclass classification tasks.</p> <p><code>Pareto Optimization</code>: An area of multiple criteria decision making that is concerned with mathematical optimization problems involving more than one objective functions to be optimized simultaneously. </p> Cross Entropy using Numpy <pre><code>import numpy as np\n\ndef cross_entropy(preds, labels):\n    xentropy = 0\n    for i in range(len(preds)):\n        xentropy -= preds[i] * np.log(labels[i])    # NOTE the `-=` instead of `+=`\n    return xentropy\n</code></pre>"},{"location":"blogs/lightning/api/","title":"Api","text":""},{"location":"blogs/lightning/api/#configure_optimizers","title":"configure_optimizers()","text":""},{"location":"blogs/lightning/api/#forward","title":"forward()","text":""},{"location":"blogs/lightning/api/#freeze","title":"freeze()","text":""},{"location":"blogs/lightning/api/#log","title":"log()","text":""},{"location":"blogs/lightning/api/#training_step","title":"training_step()","text":""},{"location":"blogs/lightning/api/#test_step","title":"test_step()","text":""},{"location":"blogs/physics/blog_01282021/","title":"Electromagnetic Spectrum, properties of material medium & its effects","text":""},{"location":"blogs/physics/blog_01282021/#electromagnetic-spectrum-properties-of-material-medium-its-effects","title":"Electromagnetic Spectrum, properties of material medium &amp; its effects.","text":""},{"location":"blogs/physics/blog_01282021/#electromagnetic-spectrum","title":"Electromagnetic Spectrum","text":"Electromagnetic Spectrum <p> The different parts of the electromagnetic spectrum have very different effects upon interaction with matter. Starting with low frequency <code>radio waves</code>, the human body is quite transparent (you can listen to your portable radio inside your home since the waves pass freely through the walls of your house and even through the person beside you!) As you move upward through <code>microwaves</code> and <code>infrared</code> to <code>visible light</code>, you absorb more and more strongly. In the lower ultraviolet range, all the <code>UV</code> from the sun is absorbed in a thin outer layer of your skin. As you move further up into the <code>x-ray</code> region of the spectrum, you become transparent again, because most of the mechanisms for absorption are gone. You then absorb only a small fraction of the radiation, but that absorption involves the <code>more violent ionization events</code>.</p> <ul> <li>Each portion of the electromagnetic spectrum has quantum energies appropriate for the excitation of certain types of physical processes.</li> <li>The energy levels for all physical processes at the atomic and molecular levels are quantized, and if there are no available quantized energy levels with spacings which match the quantum energy of the incident radiation, then the material will be transparent to that radiation, and it will pass through.</li> <li>If electromagnetic energy is absorbed, but cannot eject electrons from the atoms of the material, then it is classified as non-ionizing radiation, and will typically just heat the material.</li> </ul>"},{"location":"blogs/physics/blog_01282021/#microwaves","title":"Microwaves","text":"Microwaves <p> The quantum energy of microwave photons is in the range <code>0.00001 to 0.001 eV</code> which is in the range of energies separating the quantum states of molecular rotation and torsion.</p> <ul> <li>The interaction of microwaves with matter other than metallic conductors** will be to rotate molecules and produce heat as result of that molecular motion.</li> <li>Conductors will strongly absorb microwaves and any lower frequencies because they will cause electric currents which will heat the material. Most matter, including the human body, is largely transparent to microwaves.</li> <li>High intensity microwaves, as in a microwave oven where they pass back and forth through the food millions of times, will heat the material by producing molecular rotations and torsions. Since the quantum energies are a million times lower than those of x-rays, they cannot produce ionization and the characteristic types of radiation damage associated with ionizing radiation.</li> </ul>"},{"location":"blogs/physics/blog_01282021/#infrared","title":"Infrared","text":"Infrared  <p> The quantum energy of infrared photons is in the range <code>0.001 to 1.7 eV</code> which is in the range of energies separating the quantum states of molecular vibrations.</p> <ul> <li>Infrared is absorbed more strongly than microwaves, but less strongly than visible light.</li> <li>The result of infrared absorption is heating of the tissue since it increases molecular vibrational activity.</li> <li>Infrared radiation does penetrate the skin further than visible light and can thus be used for photographic imaging of subcutaneous blood vessels.</li> </ul>"},{"location":"blogs/physics/blog_01282021/#visible-light","title":"Visible Light","text":"Visible Light  <p> The primary mechanism for the absorption of visible light photons is the elevation of electrons to higher energy levels. There are many available states, so visible light is absorbed strongly. With a strong light source, red light can be transmitted through the hand or a fold of skin, showing that the red end of the spectrum is not absorbed as strongly as the violet end.</p> <p>While exposure to visible light causes heating, it does not cause ionization with its risks. You may be heated by the sun through a car windshield, but you will not be sunburned - that is an effect of the higher frequency uv part of sunlight which is blocked by the glass of the windshield.</p> <p></p> <p></p>"},{"location":"blogs/physics/blog_01282021/#ultraviolet","title":"Ultraviolet","text":"Ultraviolet  <p> The near ultraviolet is absorbed very strongly in the surface layer of the skin by electron transitions. As you go to higher energies, the ionization energies for many molecules are reached and the more dangerous photoionization processes take place.</p> <ul> <li>Sunburn is primarily an effect of uv, and ionization produces the risk of skin cancer.</li> </ul> <p>The <code>ozone layer</code> in the upper atmosphere is important for human health because it absorbs most of the harmful ultraviolet radiation from the sun before it reaches the surface. The higher frequencies in the ultraviolet are ionizing radiation and can produce harmful physiological effects ranging from sunburn to skin cancer.</p> <p>Health concerns for UV exposure are mostly for the range <code>290-330 nm</code> in wavelength, the range called UVB. According to <code>Scotto, et al</code>, the most effective biological wavelength for producing skin burns is <code>297 nm</code>. Their research indicates that the biological effects increase logarithmically within the UVB range, with 330 nm being only 0.1% as effective as 297 nm for biological effects. So it is clearly important to control exposure to UVB.</p>"},{"location":"blogs/physics/blog_01282021/#x-ray","title":"X-ray","text":"X-men  First Header Second Header <p>Since the quantum energies of x-ray photons are much too high to be absorbed in electron transitions between states for most atoms, they can interact with an electron only by knocking it completely out of the atom. That is, all x-rays are classified as ionizing radiation. This can occur by giving all of the energy to an electron (photoionization) or by giving part of the energy to the electron and the remainder to a lower energy photon (Compton Scattering). At sufficiently high energies, the x-ray photon can create an electron positron pair.</p> Author Disclaimer <p><code>Author: Vinay Kumar (@imflash217)</code> <code>Date: 28/January/2021</code></p> <p>The contents of this article were originally published at the references below. I have assembled it for my own understanding. Feel free to reuse and tag along the references. </p>"},{"location":"blogs/physics/blog_01282021/#references","title":"References:","text":"<ol> <li> <p>http://hyperphysics.phy-astr.gsu.edu/hbase/mod3.html \u21a9</p> </li> <li> <p>http://hyperphysics.phy-astr.gsu.edu/hbase/mod2.html \u21a9</p> </li> <li> <p>https://physics.stackexchange.com/questions/300551/how-can-wifi-penetrate-through-walls-when-visible-light-cant \u21a9</p> </li> <li> <p>https://physics.stackexchange.com/questions/1836/why-is-air-invisible \u21a9</p> </li> <li> <p>https://physics.stackexchange.com/questions/7437/why-is-glass-transparent \u21a9</p> </li> </ol>"},{"location":"blogs/prob/intro/","title":"Probablity Theory","text":"<p>A key concept in the field of pattern recognition is that of uncertainity. It arrises through:</p> <ol> <li>noise on measurements</li> <li>finite size of datasets.</li> </ol> <p>Probability theory provides a consistent framework for the quantification  and manipulation of uncertainity and forms one of the central foundations of Pattern Recognition.</p> <p>When combined with Decision Theory, it allows us to make optimal predictions given all the informtion available to us, even though that information may be incomplete or ambiguous.</p>"},{"location":"blogs/prob/intro/#example-redblue-boxes","title":"Example: Red/Blue boxes","text":""},{"location":"blogs/prob/intro/#the-setup","title":"The setup","text":"<p>Imagine we have two boxes <code>red_box</code> and <code>blue_box</code>.  Each box contains different number of fruits <code>oranges</code> and <code>apples</code>. <pre><code>red_box:\n    - apples: 2\n    - oranges: 6\nblue_box:\n    - apples: 3\n    - oranges: 1\n</code></pre></p>"},{"location":"blogs/prob/intro/#the-experiment","title":"The Experiment","text":"<p>Now, we do the following steps:</p> <ol> <li>Select a box: Randomly pick one of the boxes (either <code>red_box</code> or <code>blue_box</code>)</li> <li>Pick a fruit: Then, randomly select an item of fruit for the box.</li> <li>Replace the fruit: Having observed the type of the picked fruit (<code>apple</code> or <code>orange</code>), now, replace it in the box from which it came.</li> <li>Repeat steps 1-to-3 multiple times.</li> </ol>"},{"location":"blogs/prob/intro/#the-pre-conditions-assumptions","title":"The Pre-conditions / Assumptions","text":"<p>Now, let's suppose in doing the above experiment, </p> <ol> <li>we pick the <code>red_box</code> 40% of the times and the <code>blue_box</code> 60% of the times. </li> <li>Also, assume that that when we remove an item of fruit from a box, we are equally likely to select any of the pices of fruits in the box.</li> </ol>"},{"location":"blogs/prob/intro/#the-theory","title":"The Theory","text":"<p>In this experiemnt:</p> <ol> <li>the identity of the box to be chosen is a random variable \\(B\\) which can take  two possible values <code>r</code> and <code>b</code> (corresponding to red or blue boxes).</li> <li>Similarly, the identity of fruit is also a random variable \\(F\\) and it can take either of the values <code>a</code> and <code>o</code> (corresponding to apple an dorange respectively).</li> </ol> <p>Definition:</p> <p>The Probability of an event is defined as the fraction of times, that event occurs out of the total number of trials (in the limit that the total number of trials goes to infinity). All probabilities must lie in the interval [0, 1]</p> <p>Thus , the probability of selecting the <code>red_box</code> is \\(4/10\\) and that of the <code>blue_box</code> is \\(6/10\\).</p> \\[ p(B=r) = 4/10 \\] \\[ p(B=b) = 6/10 \\]"},{"location":"blogs/system_design/cdn/","title":"\ud83c\udf29\ufe0f CDN","text":""},{"location":"blogs/system_design/cdn/#cdn-content-delivery-network","title":"CDN \u0905\u0930\u094d\u0925\u093e\u0924\u094d Content Delivery Network","text":"<p>CDN \u0905\u0930\u094d\u0925\u093e\u0924\u094d servers \u0915\u093e \u090f\u0915 \u0910\u0938\u093e network \u091c\u094b \u092a\u0943\u0925\u094d\u0935\u0940 \u092a\u0930 \u0905\u0932\u0917-\u0905\u0932\u0917 \u092d\u0942\u0917\u094c\u0932\u093f\u0915 \u0915\u094d\u0937\u0947\u0924\u094d\u0930\u094b\u0902 \u092e\u0947\u0902 \u0939\u094b\u0924\u093e \u0939\u0948; \u0914\u0930 \u0907\u0928\u0915\u093e \u090f\u0915 \u0939\u0940 \u0915\u093e\u0930\u094d\u092f \u0939\u094b\u0924\u093e \u0939\u0948: \"static content \u091c\u0948\u0938\u0947 images, videos, CSS, JavaScript, etc. \u0915\u094b \u092e\u093e\u0901\u0917\u0928\u0947 \u0935\u093e\u0932\u0947 client (\u091c\u0948\u0938\u0947 apps, end-users, etc) \u0924\u0915 \u092a\u0939\u0941\u0901\u091a\u093e\u0928\u093e\"\u0964 \u0907\u0938 \u0915\u093e\u0930\u094d\u092f \u0915\u094b \u0915\u0930\u0928\u0947 \u0915\u0947 \u0932\u093f\u090f CDN \u0915\u0947 \u0926\u094d\u0935\u093e\u0930\u093e \u092a\u094d\u0930\u092f\u0941\u0915\u094d\u0924 \u0939\u094b\u0928\u0947 \u0935\u093e\u0932\u0947 technology \u0915\u093e \u0928\u093e\u092e \u0939\u0948: Dynamic Content Caching \u091c\u094b <code>request path</code>, <code>query strings</code>, <code>cookies</code> \u0914\u0930 <code>request headers</code> \u0915\u0947 \u0906\u0927\u093e\u0930 \u092a\u0930 HTML pages \u0915\u094b cache \u0915\u0930\u0928\u0947 \u0915\u0940 \u092f\u094b\u0917\u094d\u092f\u0924\u093e \u092a\u094d\u0930\u0926\u093e\u0928 \u0915\u0930\u0924\u093e \u0939\u0948\u0964</p> <p>\u0938\u0930\u0932 \u092d\u093e\u0937\u093e \u092e\u0947\u0902 CDN \u092e\u0941\u0916\u094d\u092f\u0924\u0903 \u0926\u094b \u0915\u093e\u0930\u094d\u092f\u094b\u0902 \u0915\u094b \u0915\u0930\u0924\u093e \u0939\u0948:</p> <ul> <li>Client \u0915\u094b static content \u0926\u0947\u0928\u093e </li> <li>\u0928\u091c\u093c\u0926\u0940\u0915\u0940 client \u0915\u0940 \u0938\u0930\u094d\u0935\u092a\u094d\u0930\u0925\u092e \u0938\u0947\u0935\u093e \u0915\u0930\u0928\u093e (\u0905\u0930\u094d\u0925\u093e\u0924\u094d, \u0905\u0917\u0930 \u0939\u092e\u093e\u0930\u093e CDN <code>\u091a\u0947\u0928\u094d\u0928\u0908</code> \u092e\u0947\u0902 \u0939\u0948 \u0914\u0930 \u0926\u094b client, \u090f\u0915 <code>\u092c\u0948\u0902\u0917\u0932\u0941\u0930\u0941</code> \u0935 \u0926\u0942\u0938\u0930\u093e <code>\u0926\u093f\u0932\u094d\u0932\u0940</code> \u0938\u0947 CDN \u0938\u0947 \u090f\u0915 website \u092e\u093e\u0901\u0917 \u0930\u0939\u0947 \u0939\u0948\u0902 \u0924\u094b \u0938\u0930\u094d\u0935\u092a\u094d\u0930\u0925\u092e <code>\u092c\u0948\u0902\u0917\u0932\u0941\u0930\u0941</code> \u0915\u0947 <code>request</code> \u0915\u094b \u092a\u0942\u0930\u093e \u0915\u093f\u092f\u093e \u091c\u093e\u090f\u0917\u093e \u092b\u093f\u0930 <code>\u0926\u093f\u0932\u094d\u0932\u0940</code> \u0915\u094b \u0915\u094d\u092f\u094b\u0902\u0915\u093f <code>\u0926\u093f\u0932\u094d\u0932\u0940</code> \u0915\u0940 \u0905\u092a\u0947\u0915\u094d\u0937\u093e <code>\u092c\u0948\u0902\u0917\u0932\u0941\u0930\u0941</code> \u0939\u092e\u093e\u0930\u0947 CDN \u0915\u0947 \u0928\u091c\u093c\u0926\u0940\u0915 \u0939\u0948 \u091c\u094b <code>\u091a\u0947\u0928\u094d\u0928\u0908</code> \u092e\u0947\u0902 \u0939\u0948)\u0964</li> </ul>"},{"location":"gists/about/","title":"About","text":""},{"location":"gists/about/#stay-tuned","title":"<code>Stay tuned</code>.","text":""},{"location":"gists/python_snippets/","title":"\ud83d\udc0dPython_Snippets","text":""},{"location":"gists/python_snippets/#python-snippets","title":"Python Snippets","text":""},{"location":"gists/python_snippets/#1-mappingproxytype","title":"<code>1: MappingProxyType</code>","text":"Immutable Mappings <p>The mapping types provided by the standard library are all mutable;  but you may need to gurantee that a user cannot change a mapping by mistake.</p> <p>Since <code>Python 3.3</code> the <code>types</code> module provides a wrapper class <code>MappingProxyType</code> which, given a mapping returns a <code>mappingproxy</code> instance that is read-only but a dynamic-view  of the original mapping. </p> <p>This means that the original mapping can be seen through <code>mappingproxy</code> but changes cannot be made through it.</p> <pre><code>from types import MappingProxyType\n\nd = {1:\"A\"}\nd_proxy = MappingProxyType(d)   ## creating a proxy for the original dict d\n                                ## d_proxy = {1:\"A\"}\nprint(d_proxy[1])               ## \"A\"\nd_proxy[2] = \"X\"                ## TypeERROR. mappingproxy does not support item assignment\n\nd[2] = \"B\"                      ## OKAY. The original dictionary is still mutable\n\nprint(d_proxy)                  ## The proxy has a dynamic view of the original dict. \n                                ## So, it refelects the change\n                                ## {1:\"A\", 2:\"B\"}\n</code></pre>"},{"location":"gists/python_snippets/#2-set-operators","title":"<code>2: Set operators</code>","text":"Set Operators <pre><code>**operator:     method:                     desciption:**\n                `s.isdisjoint(z)`           `s` and `z` are disjoint (i.e. have no elements in common)\n`e in s`        `s.__contains__(e)`         element `e` is a subset of `s` set\n\n`s &lt;= z`        `s.__le__(z)`               `s` is a **subset** of `z` set\n                `s.issubset(it)`            `s` is a **subset** of the set built from the iterable `it`\n`s &lt; z`         `s.__lt__(z)`               `s` is a **PROPER-subset** of `z` set\n\n`s &gt;= z`        `s.__ge__(z)`               `s` is a **superset** of `z` set\n                `s.issuperset(it)`          `s` is a **superset** of the set built from iterable `it`\n`s &gt; z`         `s.__gt__(z)`               `s` is a **PROPER-superset** of the set `z`\n</code></pre>"},{"location":"gists/python_snippets/#3-set-vs-frozenset","title":"<code>3: set v/s frozenset</code>","text":"Set v/s Frozenset <pre><code>**operator:     set:    frozenset:      description:**\n`s.add(e)`      \u2705                      Add element `e` to set `s`\n`s.clear()`     \u2705                      Remove all elements from set `s`\n`s.copy()`      \u2705      \u2705              Shallow copy of set/frozenset `s`\n`s.discard(e)`  \u2705                      Remove element `e` from set `s` IF it is present\n`s.__iter__()`  \u2705      \u2705              Get iterator over set/frozenset `s`\n`s.__len__()`   \u2705      \u2705              `len(s)`\n`s.pop()`       \u2705                      Remove and return an element from `s`; raising `keyError` if `s` is empty\n`s.remove(e)`   \u2705                      Remive element `e` from set `s`; raise `KeyError` if `e not in s`\n</code></pre>"},{"location":"gists/lightning/api/configure_optimizers/","title":"pl.LightningModule.configure_optimizers()","text":""},{"location":"gists/lightning/api/configure_optimizers/#code-snippets","title":"Code Snippets","text":"Author Disclaimer <p><code>Author: Vinay Kumar (@imflash217)</code> <code>Date: 30/January/2021</code></p> <p>The contents of this article were originally published at the references below. I have assembled it for my own understanding. Feel free to reuse and tag along the references. </p>"},{"location":"gists/lightning/api/configure_optimizers/#references","title":"References","text":""},{"location":"gists/lightning/api/forward/","title":"<code>pl.LightningModule.forward()</code>","text":""},{"location":"gists/lightning/api/forward/#code-snippets","title":"<code>Code Snippets</code>","text":"Author Disclaimer <p><code>Author: Vinay Kumar (@imflash217) | Date: 30/January/2021</code></p> <p>The contents of this article were originally published at the references below. I have assembled it for my own understanding. Feel free to reuse and tag along the references. </p>"},{"location":"gists/lightning/api/forward/#references","title":"<code>References</code>","text":""},{"location":"gists/lightning/api/freeze/","title":"<code>pl.LightningModule.freeze()</code>","text":""},{"location":"gists/lightning/api/freeze/#code-snippets","title":"<code>Code Snippets</code>","text":"Author Disclaimer <p><code>Author: Vinay Kumar (@imflash217) | Date: 30/January/2021</code></p> <p>The contents of this article were originally published at the references below. I have assembled it for my own understanding. Feel free to reuse and tag along the references. </p>"},{"location":"gists/lightning/api/freeze/#references","title":"<code>References</code>","text":""},{"location":"gists/lightning/api/log/","title":"<code>pl.LightningModule.log()</code>","text":""},{"location":"gists/lightning/api/log/#code-snippets","title":"<code>Code Snippets</code>","text":"Author Disclaimer <p><code>Author: Vinay Kumar (@imflash217) | Date: 30/January/2021</code></p> <p>The contents of this article were originally published at the references below. I have assembled it for my own understanding. Feel free to reuse and tag along the references. </p>"},{"location":"gists/lightning/api/log/#references","title":"<code>References</code>","text":""},{"location":"gists/lightning/api/test_step/","title":"<code>pl.LightningModule.test_step()</code>","text":""},{"location":"gists/lightning/api/test_step/#code-snippets","title":"<code>Code Snippets</code>","text":"Author Disclaimer <p><code>Author: Vinay Kumar (@imflash217) | Date: 30/January/2021</code></p> <p>The contents of this article were originally published at the references below. I have assembled it for my own understanding. Feel free to reuse and tag along the references. </p>"},{"location":"gists/lightning/api/test_step/#references","title":"<code>References</code>","text":""},{"location":"gists/lightning/api/training_step/","title":"<code>pl.LightningModule.training_step()</code>","text":""},{"location":"gists/lightning/api/training_step/#code-snippets","title":"<code>Code Snippets</code>","text":"Author Disclaimer <p><code>Author: Vinay Kumar (@imflash217) | Date: 30/January/2021</code></p> <p>The contents of this article were originally published at the references below. I have assembled it for my own understanding. Feel free to reuse and tag along the references. </p>"},{"location":"gists/lightning/api/training_step/#references","title":"<code>References</code>","text":""},{"location":"hobbies/about/","title":"About","text":""},{"location":"hobbies/about/#stay-tuned","title":"<code>Stay tuned</code>.","text":""},{"location":"nlp/naive_bayes/","title":"Na\u00efve Bayes Classifiers","text":"<p>In this article we will talk about Multinomial Na\u00efve Bayes Classifier, so called because it is a Bayesian Classifier that makes a simplifying (na\u00efve) assumption about the interaaction b/w features.</p> <p>Let's understand the intuition of this calssifier in the context of text classification. Given a text document we first respresnt the text document as a bag-of-words  (i.e. an unordered set of words in the document with their position information removed) keeping only the word-frequency in the given document. In this bag-of-words representation, all we care about is how many times a given word appears in this document.</p> <p>Na\u00efve Bayes is a probabilistic classifier, meaning that for a given document <code>d</code>, out of all classes \\(c\\in C\\) the classifier returns the class \\(\\hat{c}\\) which has the maximum posterior probability given the document <code>d</code>.</p>"},{"location":"nlp/nlp_book/","title":"NLP notes:","text":""},{"location":"nlp/nlp_book/#text-normalization","title":"Text Normalization","text":"<p>Normalizing text means converting them into a more convenient, stndard form. For example, most of what we are going to do with language relies on first  separating out or tokenizing words from running text  (which is called as tokenization).</p>"},{"location":"nlp/nlp_book/#tokenization","title":"Tokenization","text":""},{"location":"nlp/nlp_book/#lemmatization","title":"Lemmatization","text":""},{"location":"nlp/nlp_book/#stemming","title":"Stemming","text":""},{"location":"nlp/nlp_book/#sentence-segmentation","title":"Sentence Segmentation","text":""},{"location":"nlp/nlp_book/#edit-distance","title":"Edit Distance","text":""},{"location":"nlp/regex/","title":"Regular Expressions","text":"<p>One of the unsung successes in standardization of computer science has been the Regular Expressions (RE), a language for specifying text search strings. This practical language is in every computer language, word processor, and  text processing tools like the Unix's <code>grep</code>. </p> <p> A regular expression is an algebraic notation for characterizing a set of strings.</p> <p>They are particularly useful for searching in texts, when we have a pattern to search for and a corpus of texts to search through.  A regex search function will search through the corpus returning all teexts that match the pattern. The corpus can be a single document or a collection.  For example, the Unix cmd tool <code>grep</code> takes a regex and  returns every line of the input document that matches the pattern in regex.</p> <p> RegEx comes in many variants. In his article we will be discussing extended regex.</p>"},{"location":"nlp/regex/#basic-patterns","title":"Basic Patterns","text":""},{"location":"nlp/regex/#references","title":"References:","text":"<ol> <li> <p>\"Speech &amp; Language Processing\" by Jurafsky et al.; 2021\u00a0\u21a9</p> </li> <li> <p>https://regex101.com/ \u21a9</p> </li> </ol>"},{"location":"nlp/CS224N/cs224n_1/","title":"Lecture #1","text":""},{"location":"nlp/CS224N/cs224n_1/#word-meaning","title":"Word Meaning","text":"<ul> <li> Denotational Semantics</li> <li> Distributional Semantics</li> <li> Localist Representation (like <code>one-hot vectors</code>)</li> <li> Distributed Representation (like <code>word-vectors</code>)</li> </ul> NLTK example: synonyms of word <code>good</code> <pre><code>\n</code></pre> <pre><code>\u276f python 01_word_meaning.py\nnoun: good\nnoun: goodgoodness\nnoun: goodgoodness\nnoun: commoditytrade_goodgood\nadj: good\nadj (s): fullgood\nadj: good\nadj (s): estimablegoodhonorablerespectable\nadj (s): beneficialgood\nadj (s): good\nadj (s): goodjustupright\nadj (s): adeptexpertgoodpracticedproficientskillfulskilful\nadj (s): good\nadj (s): deargoodnear\nadj (s): dependablegoodsafesecure\nadj (s): goodrightripe\nadj (s): goodwell\nadj (s): effectivegoodin_effectin_force\nadj (s): good\nadj (s): goodserious\nadj (s): goodsound\nadj (s): goodsalutary\nadj (s): goodhonest\nadj (s): goodundecomposedunspoiledunspoilt\nadj (s): good\nadv: wellgood\nadv: thoroughlysoundlygood\n</code></pre> Problems with toolkits like <code>WordNet</code> <ul> <li> Great as a resource but missing <code>nuance</code><ul> <li> Example: <code>proficent</code> is listed as a synonym for <code>good</code>; but it is correct only in some contexts.</li> </ul> </li> <li> Missing new meaning of words<ul> <li> Example: new slang words etc like <code>wicked</code>, <code>badass</code>, <code>nifty</code> etc.</li> <li> IMPOSSIBLE to keep up-to-date</li> </ul> </li> <li> Very Subjective</li> <li> Requires human labour to curate and maintain</li> <li> Can't compute accurate word-similarity.</li> </ul>"},{"location":"nlp/CS224N/cs224n_1/#references","title":"References","text":"<ol> <li> <p>Stanford's NLP+DL course.\u00a0\u21a9</p> </li> </ol>"},{"location":"notes/about/","title":"About","text":"Image caption <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre> <p>Nunc eu odio eleifend, blandit leo a, volutpat sapien. Phasellus posuere in sem ut cursus. Nullam sit amet tincidunt ipsum, sit amet elementum turpis. Etiam ipsum quam, mattis in purus vitae, lacinia fermentum enim.</p> <p>Pied Piper</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Quote</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Danger</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Success</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Submit </p> C <pre><code>#include &lt;stdio.h&gt;\n\nint main(void) {\n  printf(\"Hello world!\\n\");\n  return 0;\n}\n</code></pre> C++ <pre><code>#include &lt;iostream&gt;\n\nint main(void) {\n  std::cout &lt;&lt; \"Hello world!\" &lt;&lt; std::endl;\n  return 0;\n}\n</code></pre>"},{"location":"notes/about/#welcome-to-mkdocs","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p> <p>Lorem ipsum<sup>1</sup> dolor sit amet, consectetur adipiscing elit.<sup>2</sup></p> <pre><code>#####################################################################\n## \u092a\u094d\u0930\u094b\u091c\u0947\u0915\u094d\u091f-\u0936\u093f\u0915\u094d\u0937\u093e\n#####################################################################\n\ndef \u0935\u093f\u092d\u093e\u0917(\u092d\u093e\u091c\u094d\u092f, \u092d\u093e\u091c\u0915):\n    \u092d\u093e\u0917\u092b\u0932 = 0\n    \u092d\u093e\u0917 = 1\n    \u0936\u0947\u0937\u092b\u0932 = 0\n\n    print(f\"-----------------------------------\")\n    print(f\"\u092d\u093e\u091c\u094d\u092f - (\u092d\u093e\u091c\u0915 x \u092d\u093e\u0917) = \u0936\u0947\u0937 [?] \u092d\u093e\u091c\u0915\")\n    print(f\"-----------------------------------\")\n\n    if \u092d\u093e\u091c\u094d\u092f &lt; \u092d\u093e\u091c\u0915:\n        # print\n        raise ValueError(f\"\u092d\u093e\u091c\u094d\u092f &lt; \u092d\u093e\u091c\u0915 [\u0917\u093c\u0932\u0924 \u0938\u0902\u0916\u094d\u092f\u093e\u090f\u0901 \u0926\u0940 \u0917\u092f\u0940\u0902. \u0915\u0943\u092a\u092f\u093e \u0938\u0939\u0940 \u0938\u0902\u0916\u094d\u092f\u093e \u0905\u0902\u0915\u093f\u0924 \u0915\u0930\u0947\u0902.]\")\n\n    while True:\n        \u0936\u0947\u0937 = \u092d\u093e\u091c\u094d\u092f - (\u092d\u093e\u091c\u0915 * \u092d\u093e\u0917)\n        if \u0936\u0947\u0937 &gt;= \u092d\u093e\u091c\u0915:\n            print(f\"{\u092d\u093e\u091c\u094d\u092f} - ({\u092d\u093e\u091c\u0915} x {\u092d\u093e\u0917}) = {\u0936\u0947\u0937} &gt; {\u092d\u093e\u091c\u0915}\")\n            \u092d\u093e\u0917 = \u092d\u093e\u0917 + 1\n        else:\n            print(f\"{\u092d\u093e\u091c\u094d\u092f} - ({\u092d\u093e\u091c\u0915} x {\u092d\u093e\u0917}) = {\u0936\u0947\u0937} &lt; {\u092d\u093e\u091c\u0915} .\u0938\u092e\u093e\u092a\u094d\u0924\")\n            \u092d\u093e\u0917\u092b\u0932 = \u092d\u093e\u0917\n            \u0936\u0947\u0937\u092b\u0932 = \u0936\u0947\u0937\n            print(f\"-----------------------------------\")\n            return {\"\u092d\u093e\u0917\u092b\u0932\": \u092d\u093e\u0917\u092b\u0932, \"\u0936\u0947\u0937\u092b\u0932\": \u0936\u0947\u0937\u092b\u0932}\n\n#####################################################################\n</code></pre> <p> lorem ipsum</p> <p></p> Image caption <ul> <li> Lorem ipsum dolor sit amet, consectetur adipiscing elit</li> <li> Vestibulum convallis sit amet nisi a tincidunt<ul> <li> In hac habitasse platea dictumst</li> <li> In scelerisque nibh non dolor mollis congue sed et metus</li> <li> Praesent sed risus massa</li> </ul> </li> <li> Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque</li> </ul> \\[ \\operatorname{ker} f=\\{g\\in G:f(g)=e_{H}\\}{\\mbox{.}} \\] <p>The homomorphism \\(f\\) is injective if and only if its kernel is only the singleton set \\(e_G\\), because otherwise \\(\\exists a,b\\in G\\) with \\(a\\neq b\\) such that \\(f(a)=f(b)\\).</p> <p>The HTML specification is maintained by the W3C.</p> <pre><code>nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor\nmassa, nec semper lorem quam in massa.\n</code></pre>"},{"location":"notes/about/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"notes/about/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"notes/about/#deployment","title":"Deployment","text":"<pre><code>git add . &amp;&amp; git commit -m \"update\" &amp;&amp; git push -u origin main &amp;&amp; mkdocs gh-deploy --force\n</code></pre> <ol> <li> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit.\u00a0\u21a9</p> </li> <li> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod\u00a0\u21a9</p> </li> </ol>"},{"location":"notes/ECE542/ece542_hw1a/","title":"Homework <code>1a</code>","text":"<p><code>Author: Vinay Kumar (@imflash217) | Date: 31/January/2021</code></p>"},{"location":"notes/ECE542/ece542_hw1a/#q1","title":"Q1","text":"Q1 <p>Given a vector \\(v \\in R^n\\) and square matrices \\(A, B \\in R^{n\\times n}\\); show that:</p> <ul> <li> \\(v^T v\\) = trace(\\(vv^T\\))</li> <li> trace(AB) = trace(BA)</li> </ul> Q1 Solution <p>Let \\(v = [v_1, v_2, \\cdots, v_n]^T\\) be a column vector of size \\((n,1)\\) - [ ] Then, $v^T v = $</p>"},{"location":"notes/ECE542/ece542_lecture1/","title":"Lecture <code>#1</code>","text":"<p><code>Author: Vinay Kumar (@imflash217) | Date: 30/January/2021</code></p> <ul> <li> AI v/s ML v/s DL (Venn Diagram)</li> <li> One-hot-encoding</li> <li> Loss Function</li> <li> Training v/s Evaluation Error</li> <li> Model Selection</li> <li> Hyperparams</li> <li> Overfitting v/s Underfitting</li> <li> Generalization Gap</li> <li> Model Capacity</li> <li> K-fold Cross Validation</li> <li> Leave-one-out Cross Validation</li> <li> </li> </ul> What is Machine Learning? <p>It is a field that aims to extract relationships and structures in the data. <code>Example: How to map data to annotations?</code></p> Loss Function <p>We need a measure to see how well our system is doing at learning. This measure is called Loss Function</p> <ul> <li> Sum-of-Squared-Error (SSE): \\(2^2\\) \\(\\sum_{i}\\normalize{(y_i - f(x_i)}_2^2\\)</li> </ul> Training <p>The process of teaching our system to minimize errors is called as Training.</p> Evaluation <p>The process of determining the performance of our trained system over an unseen dataset is called as Evaluation.</p> Unsupervised Learning <ul> <li> Generative Models (GAN, AE, RBM)</li> <li> Latent Variable Modeling (PCA, AE)</li> <li> Clustering</li> </ul> [special case of] Cross Validation  <p>If there are many point on the graph of CV(\\(\\theta\\)) with similar values near the minimum; we choose the most parsimonious model that has a CV value within the standard deviation from the best model \\(\\theta^*\\).</p> <p>In other words; we pick the first \\(\\theta\\) for which the CV value satisfies \\(CV(\\theta) &lt; CV(\\theta^*) + std(CV(\\theta^*))\\)</p> <p>Benefits of this process: It decreases the possibility of choose an underfit or slightly ovefit model than what is required. Provides better Guarantees.</p>"},{"location":"notes/hands_on_llm/embeddings/","title":"Embeddings","text":""},{"location":"notes/hands_on_llm/tokenizers/","title":"Tokenizers","text":""},{"location":"notes/library/important_ai_libraries/","title":"Important AI libraries","text":"<ul> <li> <code>autogen</code>: https://github.com/microsoft/autogen</li> <li> <code>sentence-transformers</code>: https://github.com/UKPLab/sentence-transformers</li> <li> <code>gensim</code>:</li> <li> ...</li> </ul>"},{"location":"notes/library/papers_to_read/","title":"Papers to Read","text":"<ul> <li> DeBERTa-V3: https://openreview.net/pdf?id=sE7-XhLxHA</li> <li> LLAMA-2: https://arxiv.org/pdf/2307.09288</li> <li> Sentence-BERT: https://arxiv.org/pdf/1908.10084</li> <li> Noise Contrastive Estimation: https://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf</li> </ul>"},{"location":"notes/library/papers_to_read/#articles-to-read","title":"Articles to Read:","text":"<ul> <li> Word2Vec <code>@jay_alammar</code>: https://jalammar.github.io/illustrated-word2vec/</li> <li> .</li> </ul>"},{"location":"paper_reviews/about/","title":"About","text":""},{"location":"paper_reviews/about/#stay-tuned","title":"<code>Stay tuned</code>.","text":""},{"location":"paper_reviews/detr/","title":"Detr","text":""},{"location":"projects/about/","title":"About","text":""},{"location":"projects/about/#stay-tuned","title":"<code>Stay tuned</code>.","text":""},{"location":"publications/about/","title":"About","text":""},{"location":"publications/about/#stay-tuned","title":"<code>Stay tuned</code>.","text":""},{"location":"publications/interspeech_2014/","title":"Interspeech 2014","text":"<pre><code>#####################################################################\n## \u092a\u094d\u0930\u094b\u091c\u0947\u0915\u094d\u091f-\u0936\u093f\u0915\u094d\u0937\u093e\n#####################################################################\n\ndef \u0935\u093f\u092d\u093e\u0917(\u092d\u093e\u091c\u094d\u092f, \u092d\u093e\u091c\u0915):\n    \u092d\u093e\u0917\u092b\u0932 = 0\n    \u092d\u093e\u0917 = 1\n    \u0936\u0947\u0937\u092b\u0932 = 0\n\n    print(f\"-----------------------------------\")\n    print(f\"\u092d\u093e\u091c\u094d\u092f - (\u092d\u093e\u091c\u0915 x \u092d\u093e\u0917) = \u0936\u0947\u0937 [?] \u092d\u093e\u091c\u0915\")\n    print(f\"-----------------------------------\")\n\n    if \u092d\u093e\u091c\u094d\u092f &lt; \u092d\u093e\u091c\u0915:\n        # print\n        raise ValueError(f\"\u092d\u093e\u091c\u094d\u092f &lt; \u092d\u093e\u091c\u0915 [\u0917\u093c\u0932\u0924 \u0938\u0902\u0916\u094d\u092f\u093e\u090f\u0901 \u0926\u0940 \u0917\u092f\u0940\u0902. \u0915\u0943\u092a\u092f\u093e \u0938\u0939\u0940 \u0938\u0902\u0916\u094d\u092f\u093e \u0905\u0902\u0915\u093f\u0924 \u0915\u0930\u0947\u0902.]\")\n\n    while True:\n        \u0936\u0947\u0937 = \u092d\u093e\u091c\u094d\u092f - (\u092d\u093e\u091c\u0915 * \u092d\u093e\u0917)\n        if \u0936\u0947\u0937 &gt;= \u092d\u093e\u091c\u0915:\n            print(f\"{\u092d\u093e\u091c\u094d\u092f} - ({\u092d\u093e\u091c\u0915} x {\u092d\u093e\u0917}) = {\u0936\u0947\u0937} &gt; {\u092d\u093e\u091c\u0915}\")\n            \u092d\u093e\u0917 = \u092d\u093e\u0917 + 1\n        else:\n            print(f\"{\u092d\u093e\u091c\u094d\u092f} - ({\u092d\u093e\u091c\u0915} x {\u092d\u093e\u0917}) = {\u0936\u0947\u0937} &lt; {\u092d\u093e\u091c\u0915} .\u0938\u092e\u093e\u092a\u094d\u0924\")\n            \u092d\u093e\u0917\u092b\u0932 = \u092d\u093e\u0917\n            \u0936\u0947\u0937\u092b\u0932 = \u0936\u0947\u0937\n            print(f\"-----------------------------------\")\n            return {\"\u092d\u093e\u0917\u092b\u0932\": \u092d\u093e\u0917\u092b\u0932, \"\u0936\u0947\u0937\u092b\u0932\": \u0936\u0947\u0937\u092b\u0932}\n\n#####################################################################\n</code></pre> <p> lorem ipsum</p> <p></p> Image caption <ul> <li> Lorem ipsum dolor sit amet, consectetur adipiscing elit</li> <li> Vestibulum convallis sit amet nisi a tincidunt<ul> <li> In hac habitasse platea dictumst</li> <li> In scelerisque nibh non dolor mollis congue sed et metus</li> <li> Praesent sed risus massa</li> </ul> </li> <li> Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque</li> </ul> \\[ \\operatorname{ker} f=\\{g\\in G:f(g)=e_{H}\\}{\\mbox{.}} \\] <p>The homomorphism \\(f\\) is injective if and only if its kernel is only the  singleton set \\(e_G\\), because otherwise \\(\\exists a,b\\in G\\) with \\(a\\neq b\\) such  that \\(f(a)=f(b)\\).</p> <p>The HTML specification is maintained by the W3C.</p>"},{"location":"publications/odyssey_2014/","title":"Odyssey 2014","text":"<pre><code>#####################################################################\n## \u092a\u094d\u0930\u094b\u091c\u0947\u0915\u094d\u091f-\u0936\u093f\u0915\u094d\u0937\u093e\n#####################################################################\n\ndef \u0935\u093f\u092d\u093e\u0917(\u092d\u093e\u091c\u094d\u092f, \u092d\u093e\u091c\u0915):\n    \u092d\u093e\u0917\u092b\u0932 = 0\n    \u092d\u093e\u0917 = 1\n    \u0936\u0947\u0937\u092b\u0932 = 0\n\n    print(f\"-----------------------------------\")\n    print(f\"\u092d\u093e\u091c\u094d\u092f - (\u092d\u093e\u091c\u0915 x \u092d\u093e\u0917) = \u0936\u0947\u0937 [?] \u092d\u093e\u091c\u0915\")\n    print(f\"-----------------------------------\")\n\n    if \u092d\u093e\u091c\u094d\u092f &lt; \u092d\u093e\u091c\u0915:\n        # print\n        raise ValueError(f\"\u092d\u093e\u091c\u094d\u092f &lt; \u092d\u093e\u091c\u0915 [\u0917\u093c\u0932\u0924 \u0938\u0902\u0916\u094d\u092f\u093e\u090f\u0901 \u0926\u0940 \u0917\u092f\u0940\u0902. \u0915\u0943\u092a\u092f\u093e \u0938\u0939\u0940 \u0938\u0902\u0916\u094d\u092f\u093e \u0905\u0902\u0915\u093f\u0924 \u0915\u0930\u0947\u0902.]\")\n\n    while True:\n        \u0936\u0947\u0937 = \u092d\u093e\u091c\u094d\u092f - (\u092d\u093e\u091c\u0915 * \u092d\u093e\u0917)\n        if \u0936\u0947\u0937 &gt;= \u092d\u093e\u091c\u0915:\n            print(f\"{\u092d\u093e\u091c\u094d\u092f} - ({\u092d\u093e\u091c\u0915} x {\u092d\u093e\u0917}) = {\u0936\u0947\u0937} &gt; {\u092d\u093e\u091c\u0915}\")\n            \u092d\u093e\u0917 = \u092d\u093e\u0917 + 1\n        else:\n            print(f\"{\u092d\u093e\u091c\u094d\u092f} - ({\u092d\u093e\u091c\u0915} x {\u092d\u093e\u0917}) = {\u0936\u0947\u0937} &lt; {\u092d\u093e\u091c\u0915} .\u0938\u092e\u093e\u092a\u094d\u0924\")\n            \u092d\u093e\u0917\u092b\u0932 = \u092d\u093e\u0917\n            \u0936\u0947\u0937\u092b\u0932 = \u0936\u0947\u0937\n            print(f\"-----------------------------------\")\n            return {\"\u092d\u093e\u0917\u092b\u0932\": \u092d\u093e\u0917\u092b\u0932, \"\u0936\u0947\u0937\u092b\u0932\": \u0936\u0947\u0937\u092b\u0932}\n\n#####################################################################\n</code></pre> <p> lorem ipsum</p> <p></p> Image caption <ul> <li> Lorem ipsum dolor sit amet, consectetur adipiscing elit</li> <li> Vestibulum convallis sit amet nisi a tincidunt<ul> <li> In hac habitasse platea dictumst</li> <li> In scelerisque nibh non dolor mollis congue sed et metus</li> <li> Praesent sed risus massa</li> </ul> </li> <li> Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque</li> </ul> \\[ \\operatorname{ker} f=\\{g\\in G:f(g)=e_{H}\\}{\\mbox{.}} \\] <p>The homomorphism \\(f\\) is injective if and only if its kernel is only the  singleton set \\(e_G\\), because otherwise \\(\\exists a,b\\in G\\) with \\(a\\neq b\\) such  that \\(f(a)=f(b)\\).</p> <p>The HTML specification is maintained by the W3C.</p>"},{"location":"python/context_managers/","title":"Context Managers","text":"<p>A context manager is a Python object that provides extra contextual information to an action. This extra contextual information takes the form of running a callable upon initiating the  context using the <code>with</code> statement; as well as running a callable upon completing all the code  inside the <code>with</code> block. </p> <p>For eg:</p> <pre><code>with open(\"file.txt\") as f:\n    contents = f.read()\n</code></pre> <p>Anyone familiar with this pattern knows that invoking <code>open</code> in this fashion ensures that  <code>f</code>'s <code>close()</code> will be called at some point. </p> <p>There are two ways to implement this functionality ourselves:</p> <ol> <li>using <code>class</code></li> <li>using <code>@contextmanager</code> decorator</li> </ol>"},{"location":"python/context_managers/#ctx-manager-using-class","title":"Ctx Manager using CLASS","text":"<pre><code>class CustomOpen:\n    def __init__(self, filename):\n        self.file = open(filename)\n\n    def __enter__(self):\n        return self.file\n\n    def __exit__(self, ctx_type, ctx_vale, ctx_traceback):\n        self.file.close()\n\n###################################################################\n\nwith CustomOPen(\"file.txt\") as f:\n    contents = f.read()\n</code></pre> <p>This is just a regular class with two extra methods <code>__enter__()</code> and <code>__exit__()</code>. Implementation of <code>__enter__</code> and <code>__exit__</code> are essential for its usage in <code>with</code> statement. Following are the three steps of functionality of <code>with</code> statement:</p> <ol> <li>Firstly, <code>CustomOpen</code> is initantiated</li> <li>Then its <code>__enter__()</code> method is called and whatever <code>__enter__()</code> returns is  assigned to <code>f</code> in <code>as f</code> part of the statement.</li> <li>When the contents of the <code>with</code> block is finished executing,  then, <code>__exit__()</code> method is called.</li> </ol>"},{"location":"python/context_managers/#ctx-managers-using-generators","title":"Ctx Managers using GENERATORS","text":"<pre><code>## implementing a smilar context manager as above \n## uisng a decorator\n\nfrom contextlib import contextmanager\n\n@contextmanager\ndef custom_open(filename):\n    f = open(filename)\n    try:\n        yield f\n    finally:\n        f.close()\n\n#########################################################\n\nwith custom_open(\"file.txt\") as f:\n    contents = f.read()\n</code></pre> <p>This works in exactly the same manner as the CLASS version:</p> <ol> <li>The <code>custom_open</code> function executes until it reaches the <code>yield</code> statement.</li> <li>The control was given back to the <code>with</code> statement which assigns whatever was <code>yield</code>ed to <code>f</code> in the <code>as f</code> part  of the <code>with</code> statement.</li> <li>The <code>finally</code> block is executed at the end of the <code>with</code> statement.</li> </ol>"},{"location":"python/decorators/","title":"Decorators & Closures","text":""},{"location":"python/decorators/#decorators-101","title":"Decorators 101","text":"<p>** A decorator is a <code>callable</code> that takes another function as argument (the decorated func.)** The decorator may perform some processing with the decorated function,  and return it or replaces it with another function or <code>callable</code> object.</p> <p>Both these code snippet shas the same effect: <pre><code>@decorate\ndef target():\n    print(\"running target()\")\n</code></pre> v/s <pre><code>## this code snippet has the same effect as the above one (using @decorate decorator)\ndef target():\n    print(\"running target()\")\n\ntarget = decorate(target)\n</code></pre> Decorators are just syntactic sugar.  We can always call a decorator like any regular callable, passing another function (as shown in 2<sup>nd</sup> snippet above).</p> <ol> <li>Decorators have the power to replace the decorated-function with a different one.</li> <li>Decorators are executed immediately when a module is loaded.</li> </ol>"},{"location":"python/decorators/#when-does-python-executs-decorators","title":"When does Python executs decorators?","text":"<p>A key feature of decorators is that they run right after the decorated function is defined. This happens usually at the <code>import</code> time (i.e. when the module is loaded).</p> An example <pre><code>## registration.py module\nregistry = []\n</code></pre>"},{"location":"python/design_patterns/","title":"Design Patterns","text":""},{"location":"python/design_patterns/#strategy-pattern","title":"STRATEGY Pattern","text":"<pre><code>from abc import ABC, abstractmethod\nfrom collections import namedtuple\n\nCustomer = namedtuple(\"Customer\", \"name fidelity\")\n\nclass LineItem:\n    def __init__(self, product, quantity, price):\n        self.product = product\n        self.quantity = quantity\n        self.price = price\n\n    def total(self):\n        return self.price * self.quantity\n\nclass Order:\n    \"\"\"This is the CONTEXT part of the Strategy-Pattern\"\"\"\n    def __init__(self, customer, cart, promotion=None):\n        self.customer = customer\n        self.cart = list(cart)\n        self.promotion = promotion\n\n    def total(self):\n        if not hasattr(self, \"__total\"):\n            self.__total = sum(item.total() for item in self.cart)\n        return self.__total\n\n    def due(self):\n        if self.promotion is None:\n            discount = 0\n        else:\n            discount = self.promotion.discount(self)\n        return self.total() - discount\n\n    def __repr__(self):\n        fmt = \"&lt; Order total = {:.2f}; DUE = {:.2f} &gt;\"\n        return fmt.format(self.total(), self.due())\n\nclass Promotion(ABC):\n    \"\"\"\n    The STRATEGY part of the Strategy-pattern\n    An Abstract Base Class\n    \"\"\"\n    @abstractmethod\n    def discount(self, order):\n        \"\"\"Return discount as a positive dollar amount\"\"\"\n\nclass FidelityPromot(Promotion):\n    \"\"\"\n    First CONCRETE implementation of STRATEGY ABC\n\n    5% disount for customer with 1000 or more fidelity points\n    \"\"\"\n    def discount(self, order):\n        return order.total() * 0.05 if order.customer.fidelity &gt;= 1000 else 0\n\nclass BulkPromo(Promotion):\n    \"\"\"\n    Second CONCRETE implementation of the Strategy-pattern\n\n    10% discount for each line-item with 20 or more units\n    \"\"\"\n    def discount(self, order):\n        discount = 0\n        for item in order.cart:\n            if item.quantity &gt;= 20:\n                discount += item.total() * 0.1\n        return discount\n\nclass LargeOrderPromo(Promotion):\n    \"\"\"\n    Third CONCRETE implementation of the Strategy-pattern\n\n    7% discount for orders with 10 or more distinct items\n    \"\"\"\n    def discount(self, order):\n        distinct_items = {item.product for item in order.cart}\n        if len(distinct_items) &gt;= 10:\n            return order.total() * 0.07\n        return 0\n</code></pre> Example Usage <p>Sample usage of <code>Order</code> class with different promotions applied</p> <p><pre><code>joe = Customer(\"John Doe\", 0)\nann = Customer(\"Ann Smith\", 1100)\n\ncart = [LineItem(\"banana\", 4, 0.5),\n        LineItem(\"apple\", 10, 1.5),\n        LineItem(\"watermelon\", 5, 5.0)]\n</code></pre> <pre><code>Order(joe, cart, FidelityPromo())       ## &lt; Order total = 42.00; DUE = 42.00 &gt;\nOrder(ann, cart, FidelityPromo())       ## &lt; Order total = 42.00; DUE = 39.90 &gt;\n</code></pre> Few more example usage with differnt cart types <pre><code>banana_cart = [LineItem(\"banana\", 30, 0.5),\n               LineItem(\"apple\", 10, 1.5)]\n\nOrder(joe, banana_cart, BulkItemPromo())    ## &lt; Order total = 30.00; DUE = 28.50 &gt;\n</code></pre> <pre><code>long_order = [LineItem(str(item_code), 1, 1.0) for item_code in range(10)]\n\nOrder(joe, long_order, LargeOrderPromo())   ## &lt; Order total = 10.00; DUE = 9.30 &gt;\nOrder(joe, cart, LargeOrderPromo())         ## &lt; Order total = 42.00; DUE = 42.00 &gt;\n</code></pre></p>"},{"location":"python/design_patterns/#function-oriented-strategy-pattern","title":"Function-oriented STRATEGY Pattern","text":"<p>Each concrete implementation of the Strategy Pattern in above code is a <code>class</code> with a single method <code>discount()</code>. Furthermore, the strategy instances have no state (i.e. no instance attributes).</p> <p>They look a lot like plain functions.  So, below we re-write the concrete implementations of the Strategy Pattern as plain function.</p> <pre><code>from collections import namedtuple\n\nCustomer = namedtuple(\"Customer\", \"name fidelity\")\n\nclass LineItem:\n    def __init__(self, product, quantity, price):\n        self.product = product\n        self.quantity = quantity\n        self.price = price\n\n    def total(self):\n        return self.price * self.quantity\n\nclass Order:\n    \"\"\"The CONTEXT\"\"\"\n    def __init__(self, customer, cart, promotion=None):\n        self.customer = customer\n        self.cart = list(cart)\n        self.promotion = promotion\n\n    def total(self):\n        if not hasattr(self, \"__total\"):\n            self.__total = sum(item.total() for item in self.cart)\n        return self.__total\n\n    def due(self):\n        discount = 0\n        if self.promotion:\n            discount = self.promotion(self)\n        return self.total() - discount\n\n    def __repr__(self):\n        fmt = \"&lt;Order total = {:.2f}; DUE = {:.2f}&gt;\"\n        fmt.format(self.total(), self.due())\n\n########################################################################################\n## Redesign of the concrete-implementations of STRATEGY PATTERN as functions\n\ndef fidelity_promot(order):\n    \"\"\"5% discount for customers with &gt;= 1000 fidelity points\"\"\"\n    return order.total() * 0.05 if order.customer.fidelity &gt;= 1000 else 0\n\ndef bulk_item_promo(order):\n    \"\"\"10% discount for each LineItem with &gt;= 20 units in cart\"\"\"\n    discount = 0\n    for item in oder.cart:\n        if item.quantity &gt;= 20:\n            discount += item.total() * 0.1\n    return discount\n\ndef large_order_promo(order):\n    \"\"\"7% discount for orders with &gt;= 10 distinct items\"\"\"\n    distinct_items = set(item.product for item in order.cart)\n    if len(distinct_items) &gt;= 10:\n        return order.total() * 0.07\n    return 0\n</code></pre> Example Usage <p>Smaple usage examples of <code>Order</code> class with promotion Strategy as functions <pre><code>joe = Customer(\"John Doe\", 0)\nann = Customer(\"Ann Smith\", 1100)\n\ncart = [LineItem(\"banana\", 4, 0.5),\n        LineItem(\"apple\", 10, 1.5),\n        LineItem(\"watermelon\", 5, 5.0)]\n</code></pre> <pre><code>Order(joe, cart, fidelity_promo)                ## &lt; Order total = 42.00; DUE = 42.00 &gt;\nOrder(ann, cart, fidelity_promo)                ## &lt; Order total = 42.00; DUE = 39.90 &gt;\n</code></pre></p> <p>Another Example</p> <pre><code>banana_cart = [LineItem(\"banana\", 30, 0.5),\n               LineItem(\"apple\", 10, 1.5)]\nOrder(joe, banana_cart, bulk_item_promo)        ## &lt; Order total = 30.00; DUE = 28.50 &gt;\n</code></pre> <p>Yet another Example</p> <pre><code>long_order = [LineItem(str(item_id), 1, 1.0) for item_id in range(10)]\n\nOrder(joe, long_order, large_order_promo)       ## &lt; Order total = 10.00; DUE = 9.30 &gt;\nOrder(joe, cart, large_order_promo)             ## &lt; Order total = 42.00; DUE = 42.00 &gt;\n</code></pre> <ol> <li>STRATEGY objects often make good FLYWEIGHTS</li> <li>A FLYWIGHT is a shared object that cane be use din multiple contexts simulatenously.</li> <li>Sharing is encouraged to reduce the creation of a new concrete strategy object when the  same strategy is applied over and over again in different contexts (i.e. with every new <code>Order</code> instance)</li> <li>If the strategies have no internal state (often the case);  then use plain old functions else adapt to use class version.</li> <li>A function is more lightweight than an user-defined <code>class</code></li> <li>A plain functions is also a_shared_ object that can be used in multiple contexts simulateneously.</li> </ol>"},{"location":"python/design_patterns/#choosing-the-best-strategy","title":"Choosing the best Strategy","text":"<p>Given the same customers and carts from above examples; we now add additional tests.</p> <pre><code>Order(joe, long_order, best_promo)          ## &lt; Order total = 10.00; DUE = 9.30 &gt;      ## case-1\nOrder(joe, banana_cart, best_promo)         ## &lt; Order total = 30.00; DUE = 28.50 &gt;     ## case-2\nOrder(ann, cart, best_promo)                ## &lt; Order total = 42.00; DUE = 39.90 &gt;     ## case-3\n</code></pre> <ul> <li>case-1: <code>best_promo</code> selected the <code>large_order_promo</code> for customer <code>joe</code></li> <li>case-2: <code>best_promo</code> selected the <code>bulk_item_promo</code> for customer <code>joe</code> (for ordering lots of bananas)</li> <li>case-3: <code>best_promo</code> selected the <code>fidelity_promo</code> for <code>ann</code>'s loyalty.</li> </ul> <p>Below is the implementation of <code>best_promo</code></p> <pre><code>all_promos = [fidelity_promo, bulk_item_promo, large_order_promo]\n\ndef best_promo(order):\n    \"\"\"Selects the best discount avaailable. Only one discount applicable\"\"\"\n    best_discount = max(promo(order) for promo in all_promos)\n    return best_discount\n</code></pre>"},{"location":"python/design_patterns/#finding-strategies-in-a-module","title":"Finding Strategies in a module","text":"<pre><code>## Method-1\n## using globals()\nall_promos = [globals()[name] for name in globals()\n              if name.endswith(\"_promo\")\n              and name != \"best_promo\"]\n\ndef best_promo(order):\n    best_discount = max(promo(order) for order in all_promos)\n    return best_discount\n</code></pre> <p>But a more flexible way to handle this is using inbuilt <code>inspect</code> module  and storing all the promos functions in a file <code>promotions.py</code>. This works regardless of the names given to promos.</p> <pre><code>## Method-2\n## using modules to store the promos separately\nall_promos = [func for name, func in inspect.getmembers(promotions, inspect.isfunction)]\n\ndef best_promo(order):\n    best_discount = max(promo(order) for promo in all_promos)\n    return best_discount\n</code></pre> <p>Both the methods have pros &amp; cons. Choose as you see fit.</p> <p>We could add more stringent tests to filter the functions,  by inspecting their arguments for instance.</p> <p>A more elegant solution would be use a decorator. (we will study this in later blogs)</p>"},{"location":"python/design_patterns/#references","title":"References","text":"<ol> <li> <p>https://github.com/gennad/Design-Patterns-in-Python \u21a9</p> </li> </ol>"},{"location":"python/hashing/","title":"Hashing in Python","text":""},{"location":"python/hashing/#reduce","title":"Reduce","text":""},{"location":"python/hashing/#map-reduce","title":"Map &amp; Reduce","text":""},{"location":"python/hashing/#zip","title":"ZIP","text":""},{"location":"python/protocols/","title":"Protocols","text":"<p>In the context of Object Oriented Programming,  a protocol is an informal interface that is defined only in the documentation, not in code.</p>"},{"location":"python/protocols/#sequence-protocol","title":"Sequence Protocol","text":"<p>For eg., the sequence protocol in Python entails just the <code>__len__()</code> and <code>__getitem__()</code> methods. Any class <code>Spam</code> that uses those methods can be used as a sequence. Whether <code>Spam</code> is a subclass of this or that is irrelevant;  all that matters is that it provides the necessary methods</p> <pre><code>import collections\nCard = collections.namedtuple(\"Card\", [\"rank\", \"suit\"])\n\nclass FrenchDeck:\n    ranks = [str(i) for i in range(2, 11)] + str(\"JQKA\")\n    suits = \"spades, diamonds, clubs, hearts\".split(\", \")\n\n    def __init__(self):\n        self._cards = [Card(rank, suit) for rank in ranks\n                                        for suit in suits]\n\n    def __len__(self):\n        ## necessary for usage as a SEQUENCE\n        return len(self._cards)\n\n    def __getitem__(self, idx):\n        ## necessary for usage as a SEQUENCE\n        return self._cards[idx]\n</code></pre> <p>Because protocols are informal and un-enforced, you can get away with just  implementing the part of the protocol that is necessary for your usage.</p> <p>For example, to support only iteration we just need to implement the <code>__getitem__()</code> method; we don't need to implement <code>__len__()</code>.</p>"},{"location":"python/pythonic_object/","title":"A Pythonic Object","text":"<p>Lets start by introducing a <code>Vector</code> class <pre><code>from array import array\nimport math\n\nclass Vector2D:\n    typecode = \"d\"\n    def __init__(self, x, y):\n        self.x = float(x)\n        self.y = float(y)\n\n    def __iter__(self):\n        return (i for i in (self.x, self.y))\n\n    def __repr__(self):\n        class_name = type(self).__name__\n        return \"{}({!r}, {!r})\".format(class_name, *self)\n\n    def __str__(self):\n        return str(tuple(self))\n\n    def __bytes__(self):\n        return (bytes([ord(self.typecode)]) + bytes(array(self.typecode, self)))\n\n    def __eq__(self, other):\n        return tuple(self) == tuple(other)\n\n    def __abs__(self):\n        return math.hypot(self.x, self.y)\n\n    def __bool__(self):\n        return bool(abs(self))\n</code></pre></p>"},{"location":"python/pythonic_object/#an-alternative-constructor","title":"An alternative constructor","text":"<p>Since in above example, we export a <code>Vector2D</code> as bytes;  we also need a method to imports a <code>vector2D</code> from binary sequence.</p> <p>Looking at the standard library for inspiration, we see that <code>array.array</code> has a class method <code>frombytes()</code>. Lets adopt this nomenclature in our <code>Vector2D</code> class.</p> <pre><code>## adding a import method from a binary sequence\n## inside Vector2D class definition\n\n@classmethod\ndef frombytes(cls, octets):\n    typecode = chr(octets[0])\n    memv = memoryview(octets[1:]).cast(typecode)\n    return cls(*memv)\n</code></pre>"},{"location":"python/pythonic_object/#classmethod-vs-staticmethod","title":"@classmethod v/s @staticmethod","text":"<p><code>@classmethod</code>:</p> <ol> <li>It is used to define a method that operates on class; not on instances.</li> <li>It receives the class itself as the 1<sup>st</sup> argument (eg. <code>cls</code> in above code)  instead of an instance (eg. <code>self</code>)</li> <li>Most commonly used as alternate constructors (eg. <code>frombytes</code> in above code) Note: (in the above <code>frombyytes()</code>) how the last line in <code>frombytes()</code> uses <code>cls</code> argument by invoking  it to build a new instance (<code>cls(*memv)</code>)</li> <li>By convention the 1<sup>st</sup> argument of the <code>@classmethod</code> should be named <code>cls</code>  (but Python does not care about the name though)</li> </ol> <p><code>@staticmethod</code></p> <p>A static method is just like a plain old function that happens to live inside the body of a class instead of being defind outside the class. It does not have access to internal state variables of the class or the instance. It is kept inside the class definition to provide easy access to related functions/method, so the user have access to all necessary method for a class within itself instead of finding it elsewhere.</p> <p>An example: <pre><code>class Demo:\n    @classmethod\n    def klassmeth(*args):\n        return *args\n\n    @staticmethod\n    def statmeth(*args):\n        return *args\n</code></pre> <pre><code>## no matter how it is invoked, Demo.klassmethod always receives Demo class as its 1st argument\nDemo.klassmeth()            ## (&lt;class \"__main__.Demo\"&gt;,)\nDemo.klassmeth(\"spam\")      ## (&lt;class \"__main__.Demo\"&gt;, \"spam\")\n\nDemo.statmeth()             ## ()\nDemo.statmeth(\"spam\")       ## (\"spam\")\n</code></pre></p>"},{"location":"python/pythonic_object/#formatted-displays","title":"Formatted displays","text":"<ol> <li><code>int</code> type supports <code>b</code> (for base=2 integers) and <code>x</code> (for base=16 integers).</li> <li><code>float</code> type implements <code>f</code> (for fixed-points) and <code>%</code> (for a percentage display).</li> </ol> <p><pre><code>format(42, \"b\")         ## \"101010\"\nformat(42, \"x\")         ## \"2a\"\n\nformat(2/3, 'f')        ## 0.666667\nformat(2/3, \".3f\")      ## 0.667\n\nformat(2/3, \"%\")        ## 66.666667%\nformat(2/3, \".2%\")      ## 66.67%\n</code></pre>  The Format Specifier Mini Language is extensible because each class gets to interpret  the <code>format_spec</code> argument as it likes. </p> <p><pre><code>from datetime import datetime\nnow = datetime.now()\nformat(now, \"%H:%M:%S\")             ## \"18:49:05\"\nprint(\"Its now {:%I:%M %p}\")        ## \"Its now 06:49 PM\"\n</code></pre> If a class has no <code>__format__()</code> method, the method inherited from <code>object</code> return <code>str(my_object)</code>.</p>"},{"location":"python/cookbook_dabeaz/ch01/","title":"1. Data Structures & Algorithms","text":""},{"location":"python/cookbook_dabeaz/ch01/#11-unpacking-a-sequence-into-separate-variables","title":"1.1: Unpacking a sequence into separate variables","text":"Problem <p>You have a <code>N-element</code> tuple or sequence that you would like to unpack into a collection of <code>N</code> variables.</p> Solution <ol> <li>Any sequence or iterable can be unpacked into variables using a simple assignment operation.</li> <li>The only requirement is that the the number of variables and structure of the sequence must match.</li> </ol> <pre><code>##----------------------------------------------------------\np = (4,5)   ## create a tuple\nx, y = p    ## unpack the tuple into variables 'x' &amp; 'y'. \n            ## x=4; y=5\n##----------------------------------------------------------\ndata = [\"ACME\", 50, 91.1, (2021,10,07)]\nname, shares, price, date = data    ## unpack the list\n                                    ## name=\"ACME\"; shares=50\n                                    ## price=91.1; date=(2021,10,07)\n\n## another way to unpack the nested iterable or container\n## name=\"ACME\"; shares=50; price=91.1; \n## year = 2021; month=10; day=07\nname, shares, price, (year, month, day) = data\n</code></pre> <p>If there is a mismatch in the number of elements; you will get an ERROR. <pre><code>p = (4, 5)\nx, y, z = p\n</code></pre> <pre><code>Traceback (most recent call last):\n    File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n    ValueError: need more than 2 values to unpack\n</code></pre></p> Discussion <p>Unpacking actually works with any object that happens to be iterable (tuples, lists, dicts, str, files, iteratirs, generators...)</p>"},{"location":"python/cookbook_dabeaz/ch01/#12-unpacking-elements-from-iterables-of-arbitrary-length","title":"1.2: Unpacking elements from iterables of arbitrary length","text":"Problem <p>You need to unpack <code>N</code> elements from an iterable; but the iterable may be longer than <code>N</code> elements (causing a <code>too many values to unpack</code> exception)</p> Solution"},{"location":"python/cookbook_dabeaz/ch01/#13-keeping-last-n-items","title":"1.3 Keeping last N items","text":"Problem <p>You want to keep a limited history of last few items seen during iteration.</p> Solution <ol> <li>Keeping a limited history is a perfect use for <code>collections.deque</code></li> </ol> <pre><code>\"\"\"\nPerforms a simple text match on a sequence of lines\nand yields the matching line &amp; previous N lines of context when found\n\"\"\"\n\nfrom collections import deque\n\ndef search(lines, pattern, history=5):\n    previous_lines = deque(maxlen=history)\n    for line in lines:\n        if pattern in line:\n            yield line, previous_lines\n        previous_lines.append(line)\n\n##-----------------------------------------------------------##\n## Example use ona file\nif __name__ == \"__main__\":\n    with open(\"somefile.txt\", \"r\") as f:\n        for line, prev_lines in search(f, \"my_pattern\"):\n            for x in lines:\n                print(x, end=\"\")\n            print(line, end=\"\")\n            print(\"--\"*10)\n</code></pre>"},{"location":"python/cookbook_dabeaz/ch01/#15-priority-queue","title":"1.5: Priority Queue","text":"Problem <p>You want to implement a <code>queue</code> that sorts items by a given priority  &amp; always returns the item with highest priority on each <code>pop</code> operation.</p> Solution <p>The following class uses <code>heapq</code> module to implement a simple priority queue</p> <pre><code>import heapq\n\nclass PriorityQueue:\n    def __init__(self):\n        self._queue = []\n        self._idx = 0\n\n    def __repr__(self):\n        return self._queue\n\n    def push(self, item, priority):\n\n        ## because heappop returns the 1st item in the queue \n        ## (which has the smallest priority);\n        ## So, we need to invert the priority as (-priority)\n        ## Thus, we get the item with highest priority on pop\n\n        heapq.heappush(self._queue, (-priority, self._idx, item))\n        self._idx += 1\n\n    def pop(self):\n        \"\"\"\n        Returns item with HIGHEST priority in the priority queue\n        \"\"\"\n        result = heapq.heappop(self._queue)     ## a tuple of type (-priority, idx, item)\n        return result[-1]                       ## \"item\" from above line\n</code></pre> <p>Here is an example of how we might use the above <code>PriorityQueue</code> class</p> <pre><code>class Item:\n    def __init__(self, name):\n        self.name = name\n\n    def __repr__(self):\n        return f\"Item({self.name})\"\n\n##-----------------------------------------------------------##\n\nq = PriorityQueue()                 ## Create a priority-queue object\nq.push(Item(\"foo\"), priority=1)     ## push an Item(\"foo\") with priority=1\nq.push(Item(\"bar\"), priority=5)     ## push an Item(\"bar\") with priority=5\nq.push(Item(\"spam\"), priority=4)    ## push an Item(\"spam\") with priority=4\nq.push(Item(\"grok\"), priority=1)    ## push an Item(\"grok\") with priority=1\n\nq.pop()                             ## Item(\"bar\")\nq.pop()                             ## Item(\"spam\")\nq.pop()                             ## Item(\"foo\")\nq.pop()                             ## Item(\"grok\")\n</code></pre> <p>In the above example note that the items with same priority (<code>Item(\"foo\")</code> and <code>Item(\"grok\")</code>) are returned in the same order as they were inserted into the queue.</p> Discussion <p>The core of this recipe concerns with the use of <code>heapq</code> module. The methods <code>heapq.heappush()</code> and <code>heapq.heappop()</code> insert and remove items from <code>self._queue</code> in such a way that the first tem in the list has the smallest priority.</p> <p>The <code>heappop()</code> method always returns the smallest item; so that is the key idea to make  our <code>PriorityQueue</code> pop correct items.</p> <p>In this recipe the queue consists of tuple <code>(-priority, idx, item)</code>.  The <code>priority</code> value is negated to get the queue to sort items from  highest priority to lowest priority.  This is opposite from normal heap ordering; which sorts items from smallest to highest value.</p> <p>The role of the <code>idx</code> value is to properly order items with the same priority level. By keeping a constantly increasing index, the items will be sorted according to the  order in which they were inserted.  However, the <code>idx</code> also serves an important role in making the comparision work for items with the smae priority level. To elaborate on this, the instances of <code>Item</code> can't be ordered.</p> <p><pre><code>## Item class objects cannot be compared; \n## because we have not implemented __eq__, __le__ etc...methods to support it\n\na = Item(\"foo\")\nb = Item(\"bar\")\na &lt; b                   ## ERROR. \n                        ## because \"Item\" object cannot be compared.\n</code></pre> <pre><code>Traceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nTypeError: unorderable types: Item() &lt; Item()\n</code></pre></p> <p>But, if you make (priority, item) tuple and compare them, then it works fine as long as the priorities are different.</p> <p><pre><code>## tuples (priority, item) with different priorities.\n## Comparision works fine\n\na = (1, Item(\"foo\"))\nb = (2, Item(\"bar\"))\na &lt; b                   ## Returns \"True\"\n                        ## because priority: 1 &lt; 2\n</code></pre> So, to handle the case of same priority value; the <code>idx</code> is used.  The <code>idx</code> is always increasing and unique; so there is no chance of collision as shown below</p> <pre><code>## Tuples (priority, idx, item) with same priorities but unique idx.\n## Comparision works fine\n\na = (1, 7, Item(\"foo\"))\nb = (1, 8, Item(\"bar\"))\na &lt; b                   ## Returns \"True\"\n                        ## because idx: 7 &lt; 8 \n</code></pre>"},{"location":"python/cookbook_dabeaz/ch01/#16-collectionsdefaultdict","title":"1.6: collections.defaultdict","text":"Mapping Keys to multiple values in Dictionary <p>You want to make a dictionary that maps keys to more than one value (so called <code>multdict</code>)</p> Solution <p>A dictionary is a <code>mapping</code> where each <code>key</code> is mapped to a single value. If you want to map keys to multiple values; you need to store multiple values in another container (like <code>list</code> or <code>set</code> or other <code>container types</code>)</p> <p><pre><code>d = {\"a\": [1,2,3],\n     \"b\": [4,5],\n    }\n\ne = {\"a\": {1,2,3},\n     \"b\": {4,5},\n    }\n</code></pre> 1. Use a <code>list</code> if you want to preserve the insertion-order of items. 2. Use a <code>set</code> if you want to eliminate duplicates.</p> <p>To easily construct such dictionaries, you can use <code>collections.defaultdict</code>. A feature of <code>defaultdict</code> is that it automatically initializes the first value so  you can simply focus on adding items.</p> <p><pre><code>from collections import defaultdict\n\nd = defaultdict(list)               ## use \"list\" as a constructor to initialize values map for new keys\nd[\"a\"].append(1)                    ## d = {\"a\":[1]}\nd[\"a\"].append(2)                    ## d = {\"a\":[1,2]}\nd[\"b\"].append(5)                    ## d = {\"a\":[1,2], \"b\":[5]}\nd[\"b\"].append(5)                    ## d = {\"a\":[1,2], \"b\":[5,5]}       duplication of 5 is allowed in list\n</code></pre> <pre><code>from collections import defaultdict\n\nd = defaultdict(set)                ## use \"set\" as a constructor to initialize values map for new keys\nd[\"a\"].add(1)                       ## d = {\"a\": {1}}\nd[\"a\"].add(2)                       ## d = {\"a\": {1,2}}\nd[\"b\"].add(5)                       ## d = {\"a\": {1,2}, \"b\": {5}}\nd[\"b\"].add(5)                       ## d = {\"a\": {1,2}, \"b\": {5}}       duplicates NOT allowed in \"set\"\n</code></pre></p> <p>NOTE: One caution with <code>defaultdict</code> is that it will automatically create dictionary entries for keys accessed later on (even if they aren't found in the dictionary) i.e. instead of throwing <code>KeyError</code> for keys that are not found, it adds that key to the dictinary and maps its value to the empty constructor (<code>list</code> or <code>set</code> etc. as above).</p> <p>If you want to avoid this behavior, its better to use usual <code>dict()</code> with <code>setdefault()</code>. This process is a bit messy and hard-to-read as shown below;  but provides user-control to handle edge cases.</p> <pre><code>d = {}                              ## a regular dictionary\nd.setdefault(\"a\", []).append(1)     ## d = {\"a\": [1]}\nd.setdefault(\"a\", []).append(2)     ## d = {\"a\": [1,2]}\nd.setdefault(\"b\", []).append(5)     ## d = {\"a\": [1,2], \"b\": [5]}\n</code></pre> Discussion <p>In principle, constructing a multivariate dictionary is simple. However, initialization of the first value can be messy if you try to do it yourself. Below two code-snippets show this tradeoff.</p> <pre><code>d = {}\nfor key, value in pairs:\n    if key not in d:\n        d[key] = []\n    d[key].append(value)\n\n##--------------- v/s -----------------##\n\nfrom collections import defaultdict\nd = defaultdict()\nfor key, value in items:\n    d[key].append(value)\n</code></pre>"},{"location":"python/cookbook_dabeaz/ch01/#19-finding-commonalities-in-two-dictionaries","title":"1.9: Finding commonalities in two dictionaries","text":"Problem <p>You have two dictionaries and you want to find what they have in common (like same <code>keys</code>, same <code>values</code> etc.)</p> Solution <p>Consider two dictionaries</p> <p><pre><code>a = {\"x\":1, \"y\":2, \"z\":3}\nb = {\"w\":10, \"x\":11, \"y\":2}\n</code></pre> To find out what the two dictionaries have in common, simply perform the common set operations using the <code>keys()</code> and <code>values()</code> methods.</p> <p>For example: <pre><code>## finding keys in common\na.keys() &amp; b.keys()         ##{\"x\", \"y\"}\n</code></pre></p> <pre><code>## finding keys in a, that are not in b\na.keys() - b.keys()         ## {\"z\"}\n</code></pre> <pre><code>## finding (key, value) pair in common\na.items() &amp; b.items()       ## {(\"y\", 2)}\n</code></pre> <p>These kinds of operations can also be used to alter or filter dictionary contents.</p> <p>For, example, suppose you want to make a new dictionary with selected keys removed. Below is a sample code using a dictionary comprehension <pre><code>## make a new dictionary with certain keys removed\nc = {key:a[key] for key in a.keys()-{\"z\", \"w\"}}     ## {\"x\":1, \"y\":2}\n</code></pre></p> Discussion <p>A dictionary is a mapping between a set of keys and values</p> <p>The <code>.keys()</code> method on a dict returns keys-view object of the dict. This keys-view object support common set operations like join, intersection, difference etc. These set operations are supported because it is guranteed that the keys of a dict are uniquely hashable.</p> <p>The <code>.items()</code>  method on a dict returns items-view object consisting  of <code>(key, value)</code> pairs. This items-view object also supports common set operations as above.</p> <p>The <code>.values()</code> method on the dict returns a values-view object consisting of values of the dict. BUT, this values-view object DOES-NOT SUPPORT common set operations  because the values of a dict are not guranteed to be unique. Although, if necessary we can always convert this values-view object into a set and  then perform required set-operations as usual.</p>"},{"location":"python/cookbook_dabeaz/ch01/#110-removing-duplicates-from-a-sequence-maintaining-order","title":"1.10: Removing duplicates from a Sequence (maintaining order)","text":"Problem <p>You want to remove duplicates from the sequence while maintaining order</p> Solution <p>If the values in a sequence are hashable, then we solve this problem by using <code>set</code> and <code>generator</code></p> <pre><code>def dedupe(items):\n    seen = set()\n    for x in items:\n        if x not in seen:\n            yield x\n            seen.add(x)\n\n##----------------------------------##\n\na = [1, 5, 2, 1, 9, 1, 5, 10]\nlist(dedupe(a))                     ## [1, 5, 2, 9, 10]\n\n## NOTE: above the order of the items in the output list is maintained according to the input \"a\"\n</code></pre> <p>But this above solution is valid only if the items in the sequence are hashable.</p> <p>For sequences, which have unhashable items (like <code>dict</code>), the below solution works:</p> <pre><code>def dedupe_unhashable(items, key=None):\n    seen = set()\n    for item in items:\n        val = item if key is None else key(item)\n        if val not in seen:\n            yield item\n            seen.add(item)\n</code></pre> <p>Here, the purpose of the <code>key</code> argument is to specify a <code>function</code> that converts sequence items into hashable type for the purpose of duplicate detection. For example:</p> <pre><code>a = [{\"x\":1, \"y\":2},\n     {\"x\":1, \"y\":3},\n     {\"X\":1, \"y\":2},\n     {\"x\":2, \"y\":4}\n    ]\n</code></pre> <pre><code>list(dedupe_unhashable(a, key=lambda d: (d[\"x\"], d[\"y\"])))\n</code></pre> <pre><code>[{\"x\":1, \"y\":2},\n {\"x\":1, \"y\":3},\n {\"x\":2, \"y\":4}]\n</code></pre> <pre><code>list(dedupe_unhashable(a, key=lambda d: (d[\"x\"])))\n</code></pre> <pre><code>[{\"x\":1, \"y\":2},\n {\"x\":2, \"y\":4}]\n</code></pre> <p>This solution will work for any kind of data structure and a <code>key</code> function that returns a hashable value</p>"},{"location":"python/cookbook_dabeaz/ch01/#112-frequency-of-items-in-a-sequence","title":"1.12: Frequency of items in a Sequence","text":"Problem <p>You have a sequence of hashable items asn you would want ot count the frequency of each item in the sequence.</p> Solution <p>Use <code>collections.Counter</code></p> <p>Suppose we are give a list of strings <code>words</code> as shown below and we want to find  the frequency of each word in the list</p> <pre><code>words = [\n   'look', 'into', 'my', 'eyes', 'look', 'into', 'my', 'eyes',\n   'the', 'eyes', 'the', 'eyes', 'the', 'eyes', 'not', 'around', 'the',\n   'eyes', \"don't\", 'look', 'around', 'the', 'eyes', 'look', 'into',\n   'my', 'eyes', \"you're\", 'under'\n]\n</code></pre> <pre><code>from collections import Counter\nword_counts = Counter(words)\n\ntop_three = word_counts.most_common(3)      ## returns the 3 most common items\nprint(top_three)                            ## [('eyes', 8), ('the', 5), ('look', 4)]\n</code></pre> <p>The <code>Counter</code> class has a method <code>most_common()</code> which returns the topn \"n\" most common items in the sequence.</p> Discussion <p> As input, the <code>Counter</code> object can be fed any sequence of hashable items. The <code>Counter</code> object is basically just a dictionary that holds the unique items  in the input sequence as its <code>keys</code> and their resepective counts as its <code>values</code>.</p> <p> If you want to increment the count manually, simply use addition op.</p> <pre><code>morewords = ['why','are','you','not','looking','in','my','eyes']\nfor word in morewords:\n    word_counts[word] += 1\n</code></pre> <p> Alternative, you can also use <code>update</code> method on the <code>Counter</code> object to  update the counter object.</p> <pre><code>word_counts.update(morewords)   ## does the same work as the above addiotion op.\n</code></pre> <p> The <code>Counter</code> object also supports common mathematical  operations like sum, difference etc.</p> <pre><code>from collections import Counter\nwords = [\"hello\", \"world\", \"hello\", \"world\", \"earth\"]\nmorewords = [\"hello\", \"vinay\", \"earth\"]\n\na = Counter(words)          ## {\"hello\":2, \"world\":2, \"earth\":1}\nb = Counter(morewords)      ## {\"hello\":1, \"vinay\":1, \"earth\":1}\n\n## adding the two counter objects together\na + b                                       ## {\"hello\":3, \"world\":2, \"earth\":2, \"vinay\":1}\n\n## subtracting the two counter objects\na - b                                       ## {\"hello\":1, \"world\":2, \"vinay\":1}\n</code></pre>"},{"location":"python/cookbook_dabeaz/ch01/#113-sorting-a-list-of-dicts-by-a-common-key","title":"1.13: Sorting a List of dicts by a common key","text":"Problem <p>You have a list of dictionaries and you would like to sort the items according to one or more of the dict values</p> Solution <p>Sorting this type of structure is easy using <code>operator</code> module's <code>itemgetter()</code> function. Let's say you've queried a database table to get a listing of the members on your website, and you receive the following data structure in return.</p> <p><pre><code>rows = [\n    {'fname': 'Brian', 'lname': 'Jones', 'uid': 1003},\n    {'fname': 'David', 'lname': 'Beazley', 'uid': 1002},\n    {'fname': 'John', 'lname': 'Cleese', 'uid': 1001},\n    {'fname': 'Big', 'lname': 'Jones', 'uid': 1004}\n]\n</code></pre> It is fairly easy to output these rows ordered by any of the fields common to all of the rows in the dict. For example:</p> <p><pre><code>from operator import itemgetter\n\nrows_by_fname = sorted(rows, key=itemgetter(\"fname\"))\nrows_by_uid = sorted(rows, key=itemgetter(\"uid\"))\n</code></pre> <pre><code>print(rows_by_fname)\n</code></pre> <pre><code>[{'fname': 'Big', 'uid': 1004, 'lname': 'Jones'},\n {'fname': 'Brian', 'uid': 1003, 'lname': 'Jones'},\n {'fname': 'David', 'uid': 1002, 'lname': 'Beazley'},\n {'fname': 'John', 'uid': 1001, 'lname': 'Cleese'}]\n</code></pre> <pre><code>print(rows_by_uid)\n</code></pre> <pre><code>[{'fname': 'John', 'uid': 1001, 'lname': 'Cleese'},\n {'fname': 'David', 'uid': 1002, 'lname': 'Beazley'},\n {'fname': 'Brian', 'uid': 1003, 'lname': 'Jones'},\n {'fname': 'Big', 'uid': 1004, 'lname': 'Jones'}]\n</code></pre></p> <p> The <code>itemgetter()</code> function also accepts multiple keys as arguments</p> <p><pre><code>rows_by_lfname = sorted(rows, key=itemgetter(\"lname\", \"fname\"))\nprint(rows_by_lfname)\n</code></pre> <pre><code>[{'fname': 'David', 'uid': 1002, 'lname': 'Beazley'},\n {'fname': 'John', 'uid': 1001, 'lname': 'Cleese'},\n {'fname': 'Big', 'uid': 1004, 'lname': 'Jones'},\n {'fname': 'Brian', 'uid': 1003, 'lname': 'Jones'}]\n</code></pre></p> Discussion <p>In this example, <code>rows</code> is passed to the built-in <code>sorted()</code> function which accepts a keyword  argument <code>key</code>. The <code>key</code> argument is expected to be a callable that accepts a single item from <code>rows</code> as input and returns a single value that is used as a basis for sorting. The <code>itemgetter()</code> is just such a callable.</p> <p>The <code>operator.itemgetter()</code> function takes as argument the lookup indices  which is used to extract the desired values from the records in the <code>rows</code>. It can be a dictionary key name, a numeric list element, or any value that can be fed to the  object's <code>__getitem__()</code> method.</p> <p> If you give multiple indices to <code>itemgetter()</code>, the callable it produces will return  a tuple with all the indexed elements into it; which is then passed to the <code>sorted()</code> function. This is important if you want to simulatenously sort on multiple fields  (as shown in above example <code>rows_by_lfname</code>, ie.e sorting by <code>lname</code> first then by <code>fname</code>).</p> <p>The functionality of <code>itemgetter()</code> is sometimes replaced by <code>lambda</code> expressions, </p> <pre><code>rows_by_fanme = sorted(rows, key=lambda r: r[\"fname\"])\nrows_by_lfname = sorted(rows, key=lambda r: (r[\"lname\"], r[\"fname\"]))\n</code></pre> <p> But <code>itemgetter()</code> is faster than <code>lambda</code> expressions.</p> <p> Note that this technique can also be used for other functions that does their operations which require <code>key</code> callable function such as <code>min()</code>, <code>max()</code> etc.</p> <pre><code>from operator import itemgetter\nmin(rows, key=itemgetter(\"uid\"))    ## {'fname': 'John', 'lname': 'Cleese', 'uid': 1001}\nmax(rows, key=itemgetter(\"uid\"))    ## {'fname': 'Big', 'lname': 'Jones', 'uid': 1004}\n</code></pre>"},{"location":"python/cookbook_dabeaz/ch01/#114-sorting-objects-wo-native-comparision-support","title":"1.14: Sorting objects w/o native comparision support","text":"Problem <p>You want to sort objects of the same class; but they don't natively support comparision operation.</p> Solution <p>The built-in <code>sorted()</code> function takes a <code>key</code> argument that can be passed a callable that will return a value in the object that <code>sorted()</code> will use to compare the objects.</p> <p>For example, if you have sequence of <code>User</code> instances and you want to sort the items in the sequence using their <code>used_id</code> attribute; you would supply a callable that will take a <code>User</code> instance  as input and return the <code>user_id</code> of that input which can then be used by <code>sorted()</code> function as a key to sort the instances in the sequence. This is illustrated in the below code.</p> <pre><code>class User:\n    def __init__(self, user_id):\n    self.user_id = user_id\n\n    def __repr__(self):\n        return f\"User({self.user_id})\"\n\n## ---------------------------------------------- ##\n\nusers = [User(23), User(1), User(9)]\n\nsorted(users, key=lambda u: u.user_id)  ## [User(1), User(9), User(23)]\n</code></pre> <p> Instead of using <code>lambda</code>, you can use a faster <code>operator.attrgetter()</code> function</p> <pre><code>from operator import attrgetter\nsorted(users, key=attrgetter(\"user_id\"))    ## [user(1), User(9), User(23)]\n</code></pre> Discussion <p>The choice of whether to use <code>lambda</code> or <code>operator.attrgetter()</code> is personal, but <code>operator.attrgetter()</code> is a bit faster and supports multiple attribute indexing as shown below. </p> <p>Suppose the <code>User</code> class has two attributes <code>user_id</code> and <code>user_name</code> and we want to sort sequence of <code>User</code> instances based on both these attributes; then we can use <code>operator.attrgetter()</code> very easily.</p> <pre><code>from operator import attrgetter\n\n## assume that \"User\" class has two attributes \"user_id\" and \"user_name\"\nsorted(users, key=attrgetter(\"user_id\", \"user_name\"))\n</code></pre> <p> This behavior of <code>attrgetter()</code> is analogous to <code>itemgetter()</code> function (used for dictionaries)</p> <p> Similar to <code>itemgetter()</code>, we can use <code>attrgetter()</code> for other  comparision operations that require a <code>key</code> attribute such as <code>min()</code>, <code>max()</code> etc.</p> <pre><code>min(users, key=attrgetter(\"user_id\"))\nmax(users, key=attrgetter(\"user_id\"))\n</code></pre>"},{"location":"python/cookbook_dabeaz/ch02/","title":"2. Strings & Texts","text":"<p>Almost every useful program involve some kind of text processing whether it is data or generating output.  Here we discuss about challenges involving text manipulation <code>searching</code>, <code>substitution</code>, <code>lexing</code>, <code>parsing</code> etc.</p> <p> Many of these problems can be easily solved with built-in methods. While more complicated operations might require the use of regular expressions.</p>"},{"location":"python/cookbook_dabeaz/ch02/#21-splitting-string-on-any-of-multiple-delimeters","title":"2.1: Splitting string on any of multiple delimeters","text":"Problem <p>You need to split the string into fields but the delimiters (and the space around them) aren't consistent throughtout the string.</p> Solution <p>The <code>str.split()</code> method of string objects is really meant for very simple use cases and doesn't allow for multiple delimiters. </p> <p> In case you need a bit more flexibility use <code>re.split()</code> instead as shown below.</p> <p><pre><code>import re\n\nline = \"vinay kumar, ram; lakshmi,           devi         anil-kumar-ram\"\n\nre.split(r\"[;,-\\s]\\s*\", line)   \n## [\"vinay\", \"kumar\", \"ram\", \"lakshmi\", \"devi\", \"anil\", \"kumar\", \"ram\"]\n</code></pre> Here in the above <code>re.split(r\"[;,-\\s]\\s*\", line)</code> the delimiter is either a  semi-colon (<code>;</code>), a comma (<code>,</code>), a hyphen (<code>-</code>), a single space (<code></code>) or any of these followed by any number of spaces.</p> Discussion <p>The <code>re.split()</code> is useful because you can specify multiple patterns for the delimiter. In the above solution using <code>re.split()</code>, the separator/delimiter is either  a semi-colon (<code>;</code>), a comma (<code>,</code>), a hyphen (<code>-</code>), a single space (<code></code>) or any of these followed by any number of spaces. Whenever that pattern is found, the entire match becomes a delimiter between whatever fileds lie on either side of the match.</p> <p> The result is list of fileds just like <code>str.split()</code></p> <p> When using <code>re.split()</code>, you need to be a bit careful if the regular expression involves a capture group enclosed in parenthesis. If capture groups are used, then the  matched text (i.e. the delimiter/separator) is also included in the result as shown below.</p> <pre><code>import re\n\nline = \"vinay kumar, ram; lakshmi,           devi         anil-kumar-ram\"\n\nfields = re.split(r\"(;|,|-|\\s)\\s*\", line)\n\nprint(fields)\n## [\"vinay\", \" \", \"kumar\", \",\", \"ram\", \";\", \"laksmi\", \",\", \"devi\", \" \", \"anil\", \"-\", \"kumar\", \"-\", \"ram\"]\n</code></pre> <p>Getting the split characters / delimiters / separators might be useful in certain contexts. For example, you may want ot use the split characters later on to reform the output string.</p> <pre><code>values = fields[::2]                ## [\"vinay\", \"kumar\", \"ram\", \"lakshmi\", \"devi\", \"anil\", \"kumar\", \"ram\"]\ndelimiters = fields[1::2] + [\"\"]    ## [\" \", \",\", \";\", \"-\", \" \", \"-\", \"-\", \"\"]\n\n## reform the line using the same delimiters\n\"\".join(v+d for v, d in zip(values, delimiters))    ## \"vinay kumar,ram;lakshmi,devi anil-kumar-ram\"\n</code></pre> <p> If you don't want the separator/delimiter to be in the result list but still need to use the parenthesis <code>()</code> to group parts of the regular expression pattern; then use a non-capture group specified by <code>(?:...)</code> as shown below.</p> <pre><code>re.split(r\"(?:;|,|-|\\s)\\s*\", line)  ## [\"vinay\", \"kumar\", \"ram\", \"lakshmi\", \"devi\", \"anil\", \"kumar\", \"ram\"]\n</code></pre>"},{"location":"python/cookbook_dabeaz/ch04/","title":"4. Iterators & Generators","text":"<p>Iterators are one of Python's most powerful features.  At a high level, you might view iteration simply as a way to process items in a sequence, but there is much more than this such as creating your own iterator object, applying useful iterator patterns in <code>itertools</code> module, make generator functions etc.</p>"},{"location":"python/cookbook_dabeaz/ch04/#41-manually-consuming-an-iterator","title":"4.1: Manually consuming an iterator","text":"problem <p>You wan tto consume items in an iterable but for whatever reasons you can't or don't want to use a <code>for</code> loop</p> Solution <p>To manually consume the iterable, use the <code>next()</code> function and write code to catch the <code>StopIteration</code> exception as shown below.</p> <pre><code>with open(\"etc/passwd\") as f:\n    try:\n        while True:\n            line = next(f)\n            print(line, end=\"\")\n    except StopIteration:\n        pass\n</code></pre> <p> Normally <code>StopIteration</code> exception is used to signal the end of the sequence.</p> <p> But, if you are using <code>next</code>, you can instruct <code>next()</code> to return a terminating value like <code>None</code> as shown below.</p> <pre><code>with open(\"etc/passwd\") as f:\n    try:\n        while True:\n            line = next(f, None)\n            if line is None:\n                break\n            print(line, end=\"\")\n</code></pre> Discussion <p>In most cases the <code>for</code> is used to consume the iterable. However every now and then a problem statement calls for precise control over the underlying mechanism. The below example explains what happens underneath  the iteration protocol.</p> <pre><code>&gt;&gt;&gt; items= [1, 2, 3]        ## the iterable\n&gt;&gt;&gt; \n&gt;&gt;&gt; ## Get the iterator\n&gt;&gt;&gt; it = iter(items)        ## invokes \"items.__iter__()\"\n&gt;&gt;&gt; next(it)                ## invokes \"items.__next__()\"\n1\n&gt;&gt;&gt; next(it)\n2\n&gt;&gt;&gt; next(it)\n3\n&gt;&gt;&gt; next(it)    ## this triggers \"StopIteration\" exception b'coz the iterator \"it\" is empty now\nTraceback (most recent call last):\n    file \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nStopIteration\n</code></pre>"},{"location":"python/cookbook_dabeaz/ch04/#42-delegating-iteration","title":"4.2: Delegating Iteration","text":"Problem <p>You have built a custom container object that internallt holds a <code>list</code> or <code>tuple</code>  or other iterables. You want the iteration to work with this custom container.</p> Solution <p>Typically all you need to do is to define an internal <code>iter()</code> method that delegates iteration to internally held container as shown below.</p> <p><pre><code>class Node:\n    def __init__(self, value):\n        self._value = value\n        self._children = []\n\n    def __repr__(self):\n        return f\"Node{self._value!r}\"\n\n    def add_child(self, node):\n        self._children.append(node)\n\n    def __iter__(self):\n        return iter(self._children)     ## forwars the iteration request to the \"self._children\" \n\n## ---------------------------------------------- ##\n\nif __naame__ == \"__main__\":\n    root = Node(0)\n    child1 = Node(1)\n    child2 = Node(2)\n    root.add_child(child1)\n    root.add_child(child2)\n\n    for ch in root:\n        print(ch)\n</code></pre> <pre><code>&gt;&gt;&gt; Node(1)\n    Node(2)\n</code></pre>  In the above example, the <code>__iter__()</code> method simply forwards the iteration request to the internally held <code>self._children</code> attribute.</p> Discussion <p>Python's iterator protocol requires the <code>__iter__()</code> method to return a special  iterator object that implements a <code>__next__()</code> method which actually carries out  the actual iteration.</p> <p>In the above example, we are delegating the implemeation of <code>__next__()</code> method  by returning a list (<code>iter(self._children)</code>) which internally implements <code>__iter__()</code>.</p> <p> Calling the <code>iter(x)</code> function internally calls <code>x.__iter__()</code>. Note that this behaviour is similar to <code>len(seq)</code> and <code>seq.__len__()</code></p>"},{"location":"python/cookbook_dabeaz/ch04/#43-creating-new-iteration-patterns-with-generators","title":"4.3: Creating new iteration patterns with GENERATORS","text":""},{"location":"python/cookbook_dabeaz/ch07/","title":"7. Functions","text":"<p>Writing functions using <code>def</code> is a cornerstone of all programs. In this tutorial we will look into some of the advance usage of functions viz.</p> <ol> <li>closures</li> <li>callback-functions</li> <li>control-flow</li> <li>keyword-only arguments</li> <li>default-arguments</li> <li>annotations</li> <li>any-number-of-arguments (<code>*args</code> &amp; <code>**kwargs</code>) etc.</li> </ol>"},{"location":"python/cookbook_dabeaz/ch07/#71-function-with-any-number-of-arguments-args-kwargs","title":"7.1: Function with any number of arguments (<code>*args, **kwargs</code>)","text":"Problem <p>You want to write a function that accepts any number of arguments.</p> Solution <p> For a function that accepts any number of positional arguments use a <code>*</code> argument.</p> <p> For a function that accepts any number of keyword arguments use a <code>**</code> argument.</p> <p>To write any function that accepts any numbe rof positional arguments, use a <code>*</code> argument as shown below. In this example, <code>rest</code> is a <code>tuple</code> of all extra positional arguments passed. In above code, it is treated as a <code>sequence</code> in further calculations inside the function.</p> <pre><code>def avg(first, *rest):\n    return (first + sum(rest)) / (1 + len(rest))\n\n## ------------------------------------------------- ##\n\navg(1, 2, 3)        ## return=2.0; first=1, rest=(2, 3)\navg(1, 2, 3, 4)     ## return=2.5; first=1, rest=(2, 3, 4)\n</code></pre> <p>To accept any number of keyword arguments use a <code>**</code> argument as shown below.  In the below code, <code>attrs</code> is a dictionary that holds the passed keyword arguments (if any)</p> <p><pre><code>import html\n\ndef make_element(name, value, **attrs):\n    keyvals = [f\"{key}='{val}'\" for key, val in atts.items]\n    attr_str = \"\".join(keyvals)\n    element = f\"&lt;{name}{attr_str}&gt;{html.escape(value)}&lt;/{value}&gt;\"\n    return element\n</code></pre> <pre><code>## Creates: \"&lt;item size='large' quantity=6&gt;Albatross&lt;/item&gt;\"\nmake_element(\"item\", \"Albatross\", size=\"large\", quantity=6)\n</code></pre> <pre><code>## Creates: \"&lt;p&gt;&amp;lt;spam&amp;gt;&lt;/p&gt;\"\nmake_element(p, \"&lt;spam&gt;\")\n</code></pre></p> <p> If you want a function that accepts both any-number-of-positional-arguments aswell as any-number-of-keyword-arguments use both <code>*</code> &amp; <code>**</code> arguments as shown below</p> <pre><code>def anyargs(*args, **kwargs):\n    print(args)     ## a tuple\n    print(kwargs)   ## a dictionary\n</code></pre> Discussion <p> A <code>*</code> argument can only appear as the last positional argument in a function and a <code>**</code> argumnet can only appear as the last argumnet in a function.</p> <p> A subtle aspect of function definition is that arguments can still appear  after a <code>*</code> argument by they are treated as keyword argument only (as discussed in further lessons).</p> <pre><code>## valid function definitions\ndef a(x, *args, y): pass\ndef b(x, *args, y, **kwargs): pass\n\n\n## WRONG function defintions\n## here argument \"z\" appears after `**kwargs`; hence its a WRONG definition\n\ndef c(x, *args, **kwargs, z): pass\n</code></pre>"},{"location":"python/cookbook_dabeaz/ch07/#72-keyword-only-arguments","title":"7.2: Keyword-only arguments","text":"Problem <p>You want to write a function that accepts only keyword arguments.</p> Solution <p>This feature is easy to implement if you place the keyword argument after the <code>*</code> argument or a single unnamed <code>*</code> as shown below.</p> <pre><code>def recv(maxsize, *, block):\n    \"\"\"Receives a message\"\"\"\n    pass\n\n##-------------------------------------##\n\nrecv(1024, True)            ## Type Error\nrecv(2024, block=True)      ## OK\n</code></pre> <p>This technique can also be used to specify keyword arguments for functions that accept varying numbe rof positional arguments as shown below.</p> <pre><code>def minimum(*values, clip=None):\n    m = min(values)\n    if clip is not None:\n        m = clip if clip &gt; m else m\n    return m\n\n## ----------------------------------------- ##\n\nminimum(1, 2, 5, -5, 10)            ## -5\nminimum(1, 2, 5, -5, 10, clip=0)    ## 0\n</code></pre> Discussion <p> Keyword-only arguments are a good way to enforce greater code-clarity when specifying optional function arguments. For example consider the following code:</p> <p><pre><code>msg = recv(1024, False)\n</code></pre> If someone is not familiar with <code>recv</code> (written in above section), they might not know  what does <code>False</code> mean in this scenario. On the other hand, its much clearer to the user  if someone writes the call as below:</p> <pre><code>msg = recv(1024, block=False)\n</code></pre> <p> The use of keyword-only arguments is often very useful for tricks involving <code>**kwargs</code>, since they show up nicely when the user asks for <code>help()</code></p> <pre><code>&gt;&gt;&gt; help(recv)\nHelp on function recv in module __main__:\nrecv(maxsize, *, block):\n    \"\"\"Receives a message\"\"\"\n</code></pre> <p> Keyword-only arguments also have utility in advance usage like argument injection  using <code>*args</code> &amp; <code>**kwargs</code>.</p>"},{"location":"python/cookbook_dabeaz/ch07/#73-function-argument-annotations","title":"7.3: Function argument annotations","text":"Problem <p>You've written a function but you want add some additional information about the arguments so that users can know more easily how a function is supposed to be used.</p> Solution <p>Function argument annotations can be very useful in givin gprogrammers hint about how a function is supposed to be used. Cosider the following annotated function.</p> <pre><code>## normal function definition\ndef add(x, y):\n    return x + y\n\n## Annotated function definition -- MUCH BETTER for users\ndef add(x:int, y:int) -&gt; int:\n    return x + y\n</code></pre> <p> The Python Interpreter does not add any semantic meaning to the attached annotations. Neither are they type-cheked nor does Puython behave any differently than before. However, they might give useful hunts about types that would be useful for users or third-party libraries.</p> <p> Any type of object can be uased as an annotation (eg. numbers, string, instances, types, etc.).</p> Discussion <p>Function annotations are merely stored in function's <code>__annotations__</code> attribute.</p> <pre><code>&gt;&gt;&gt; add.__annotations__\n{\"y\": &lt;class \"int\"&gt;,\n \"return\": &lt;class \"int\"&gt;,\n \"x\": &lt;class \"int\"&gt;,\n}\n</code></pre> <p>Although, annotations have many potential use-cases, their primary utility is probably just documentation. Because Python does not have type decalrations, it can be difficult to read a source code without much documentation and hind about the types of objects; function annotations are one way to resolve this problem.</p> <p> More advance usage of function annotations is to implement multiple dispatch (i.e. overloaded functions)</p>"},{"location":"python/cookbook_dabeaz/ch07/#77-capturing-variables-in-anonymous-functions","title":"7.7: Capturing variables in Anonymous functions","text":"Problem <p>You have defined an anonymous function using <code>lambda</code> and but you also need to capture the value of certain variable at the tim eof definition.</p> Solution <p>Consider the behaviour of the following code a python console:</p> <pre><code>&gt;&gt;&gt; x = 10\n&gt;&gt;&gt; a = lambda y: x + y\n&gt;&gt;&gt;\n&gt;&gt;&gt; x = 20\n&gt;&gt;&gt; b = lambda y: x + y\n</code></pre> <p>Now ask yourself a question. What is the value of <code>a(10)</code> and <code>b(10)</code> now? If you think that the values will be <code>20</code> and <code>30</code>; you would be wrong. The python console gives the follwoing results.</p> <pre><code>&gt;&gt;&gt; a(10)\n30\n&gt;&gt;&gt;\n&gt;&gt;&gt; b(10)\n30\n</code></pre> <p> The problem here is that <code>x</code> used here (in the <code>lambda</code> expression) is a free variable; that gets bound at runtime, not at definition time. The value <code>x</code> in the <code>lambda</code> expression is whatever the value of <code>x</code> variable happens to be at the time of execution of the statement <code>a(10), b(10)</code>. For example see below code:</p> <pre><code>&gt;&gt;&gt; x=15\n&gt;&gt;&gt; a(10)\n25\n&gt;&gt;&gt;\n&gt;&gt;&gt; x=3\n&gt;&gt;&gt; a(10)\n13\n</code></pre> <p> If you want an anonymous <code>lambda</code> function to capture a value at the point of definition include that value as a default value in the <code>lambda</code> expression definition as follows.</p> <pre><code>&gt;&gt;&gt; x = 10\n&gt;&gt;&gt; a = lambda y, x=x: x + y\n&gt;&gt;&gt;\n&gt;&gt;&gt; x = 20\n&gt;&gt;&gt; b = lambda y, x=x: x + y\n&gt;&gt;&gt;\n&gt;&gt;&gt; a(10)\n20                          ## Note the difference here from above code (earlier this was 30)\n&gt;&gt;&gt;\n&gt;&gt;&gt; b(10)\n30                          ## Note that \"b\" captured the correct value of \"x\"\n</code></pre> Discussion <p>The problem addressed here comes up very often in code that tries to be a just a bit more clever. For example, creating a list of <code>lambda</code> expressions using a <code>list comprehension</code> or a loop of some kind and expecting the <code>lambda</code> expressions to remember the iteration variable at the time of defintion. For example:</p> <pre><code>&gt;&gt;&gt; funcs = [lambda x: x+n for n in range(5)]\n&gt;&gt;&gt; for f in funcs:\n...     print(f(0))\n...\n4\n4\n4\n4\n4\n&gt;&gt;&gt;\n</code></pre> <p>Notice, how all <code>lambda</code> expressions take the same final value of <code>n</code> from the <code>range(5)</code> expression.</p> <p> But, if we specifically captuer the value of <code>n</code> using default arguments,  then we get the behaviour we want</p> <pre><code>&gt;&gt;&gt; funcs = [lambda x, n=n: x + n for n in range(5)]\n&gt;&gt;&gt; for f in funcs:\n...     print(f(0))\n...\n0\n1\n2\n3\n4\n&gt;&gt;&gt;\n</code></pre>"},{"location":"python/cookbook_dabeaz/ch07/#78-functoolspartial","title":"7.8: functools.partial()","text":"Making an N-argumnet callable work asa callable with fewwer argumnets <p>You have a callable that you want to make it work with other Python code. possbly as a callabel or handler, but it takes too many argumnets and causes an exception when called.</p> Solution <p>If you need to reduce the number of arguments to a function, you should use <code>functools.partial()</code> The <code>partial()</code> function allows you to assign fixed values to one or more of the arguments  thus reduce the number of required argumnets in subsequent calls of the reduced function. as shown below.</p> <p><pre><code>def spam(a, b, c, d):\n    print(a, b, c, d)\n</code></pre> Now consider <code>functools.partial()</code> to fix certain arguments.</p> <p><pre><code>&gt;&gt;&gt; from functools import partial\n&gt;&gt;&gt; \n&gt;&gt;&gt; s1 = partial(spam, 1)               ## a=1\n&gt;&gt;&gt; s1(2,3,4)                           ## note that it takes only 3 arguments\n1 2 3 4\n</code></pre> <pre><code>&gt;&gt;&gt; s2 = partial(spam, d=42)            ## d=42\n&gt;&gt;&gt; s2(1,2,3)\n1 2 3 42\n&gt;&gt;&gt; s2(5, 99, 32)\n5 99 32 42\n</code></pre> <pre><code>&gt;&gt;&gt; s3 = partial(spam, 1, 2, d=42)      ## a=1, b=2, d=42\n&gt;&gt;&gt; s3(99)\n1 2 99 42\n&gt;&gt;&gt;\n</code></pre></p> Discussion <p>...</p>"},{"location":"python/cookbook_dabeaz/ch07/#79-replacing-single-method-classes-with-functions","title":"7.9: Replacing single method classes with functions","text":"Problem <p>You have a class that only defines a single method except <code>__init__()</code> method. However, to simplify your code, you would much rather have a simple function.</p> Solution <p> In many cases, single method classes can be turned into functions using <code>closures</code>.</p> <p>Consider the following example, which allws users to fetch <code>URLs</code> using a kind of template-scheme.</p> <pre><code>from urllib.request import urlopen\n\nclass UrlTemplate:\n    def __init__(self, template):\n        self.template = template\n    def open(self, **kwargs):\n        return urlopen(self.template.format_map(kwargs))\n\n## -------------------------------------------------------------------------------- ##\n\n## Usage of above class\n## Example Use: Download stock data from yahoo\n\nyahoo = UrlTemplate(\"http://finance.yahoo.com/d/quotes.csv?s={names}&amp;f={fields}\")\nfor line in yahoo.open(names=\"APPL, FB, TSLA\", fields=\"sl1c1v\"):\n    print(line.decode(\"utf-8\"))\n</code></pre> <p>This <code>URLTemplate</code> class be very easily replaced with a much simpler function using closures. as shown below.</p> <pre><code>## Using closures\nfrom urllib.request import urlopen\n\ndef url_template(template):\n    def opener(**kwargs):\n        return urlopen(template.format_map(kwargs))\n    return opener\n\n## ------------------------------------------------------------------------------- ##\n\n## Example Usage of the above CLOSURE \"url_template\"\n\nyahoo = url_template(\"http://finance.yahoo.com/d/quotes.csv?s={names}&amp;f={fields}\")\nfor line in yahoo(names=\"APPL, FB, TSLA\", fileds=\"sl1c1v\"):\n    print(line.decode(\"utf-8\"))\n</code></pre>"},{"location":"python/cookbook_dabeaz/ch07/#710-carrying-extra-state-with-callback-functions","title":"7.10: Carrying extra state with callback functions","text":"Problem <p>You are writing code that relies on the use of callback functions  (e.g. event handlers, completion callback, etc.); but you want to have the  callback function carry extra state to be used inside the callback function itself.</p> Solution <p>Let's first talk about callback functions which are used in asynchronous processing via the following example:</p> <pre><code>def apply_async(func, args, *, callback):\n    ## compute the result\n    result = func(*args)\n\n    ## invoke the callback\n    callback(result)\n</code></pre> <p>In reality, such code might do all sorts of advanced processing involving threads, processes, &amp; timers. But here we are just focusing on the invocation of the callback. Below, we show how to use the above code:</p> <pre><code>def print_result(result):\n    print(f\"Got: {result}\")\n\ndef add(x, y):\n    return x + y\n</code></pre> <pre><code>&gt;&gt;&gt; apply_async(add, (2, 3), callback=print_result)\nGot: 5\n&gt;&gt;&gt; apply_result(add, ('Hello\", \"World'), callback=print_result)\nGot: \"HelloWorld\"\n</code></pre> <p>As you can notice above, the callbakc function <code>print_result()</code> only accepts a single argument i.e <code>result</code>. No other information is provided. This lack of information can be an issue when we want our callback function to interact with other contextual variables or other parts of the code. Below we provide some probable solutions to handle this issue:</p> <p> Some ways to carry extra information with callbacks:</p> <ol> <li>Use bound methods instead of regular sinple functions.</li> <li>Use closures</li> <li>Use coroutines.</li> <li>Using <code>functools.partial()</code></li> </ol> <p> Using BOUND METHODS: One way to carry extra information in a callback is to use bound methods  instead of simple function. For example, this below class keeps an internal sequence number  that is incremented everytime a result is received.</p> <p><pre><code>class ResultHandler:\n    def __init__(self):\n        self.sequence = 0\n    def handler(self, result):\n        self.sequence += 1\n        print(f\"[{self.sequence}] Got: {result}\")\n</code></pre> To use this class <code>ResultHandler</code>, we need to create its instance and use its  <code>handler()</code> method as a callback.</p> <p><pre><code>&gt;&gt;&gt; rh = ResultHandler()                 ## creating an instance of the class\n&gt;&gt;&gt; apply_async(add, (2, 3), callback=rh.handler)\n[1] Got: 5\n&gt;&gt;&gt; apply_async(add, (\"Hello\", \"World\"), callback=rh.handler)\n[2] Got: \"HelloWorld\"\n</code></pre> Notice in the above output, how the instance of <code>ResultHandler</code> class <code>rh</code> uses its internal state variable <code>self.sequence</code> across two different calls of the callback function <code>rh.handler</code>.</p> <p> Using CLOSURES: As an alternative we can also use closures to embed additional information in the callback function.</p> <pre><code>def make_handler():\n    sequence = 0\n    def handler(result):\n        nonlocal sequence\n        sequence += 1\n        print(f\"[{sequence}] Got: {result}\")\n    return handler\n</code></pre> <pre><code>&gt;&gt;&gt; handler = make_handler()                    ## our callback function to be used\n&gt;&gt;&gt; apply_async(add, (2,3), callback=handler)\n[1] Got: 5\n&gt;&gt;&gt; apply_async(add, (\"Hello\", \"World\"), callback=handler)\n[2] Got: \"HelloWorld\"                                       ## the callback remembers the count sequence\n</code></pre> <p> Using COROUTINES: Sometimes we can also use a coroutine to accomplish what we achieved using a bound method or a closure.</p> <p><pre><code>## Using COROUTINES to add context to a callback function\n\ndef make_handler():\n    sequence = 0\n    while True:\n        result = yield\n        sequence += 1\n        print(f\"[{sequence}] Got: {result}\")\n</code></pre> We can use it as usual as before.</p> <pre><code>&gt;&gt;&gt; handler = make_handler()                            ## our callback function\n&gt;&gt;&gt; apply_async(add, (2, 3), callback=handler.send)     \n[1] Got: 5\n&gt;&gt;&gt; apply_async(add, (\"Hello\", \"WORLD!!\"), callback=handler.send)\n[2] Got: \"HelloWORLD!!\"\n</code></pre> <p> NOTICE the use of <code>send</code> to push value to the <code>yield</code> in the callback coroutine</p> <p> Using <code>functools.partial()</code>: As an alternative, we can also carry state into a callback using an extra argument and a partial function application.</p> <pre><code>class SeqeunceNo:\n    def __init__(self):\n        self.sequence = 0\n\ndef handler(result, seq):\n    seq.sequence += 1\n    print(f\"[{seq.sequence}] Got: {result}\")\n</code></pre> <pre><code>&gt;&gt;&gt; from functools import partial\n&gt;&gt;&gt; seq = SequenceNo()\n&gt;&gt;&gt; apply_async(add, (2,3), callback=partial(handler, seq=seq))\n[1] Got: 5\n&gt;&gt;&gt; apply_async(add, (\"HELLO_\", \"WORLD!!!\"), callback=partial(handler, seq=seq))\n[2] Got: \"HELLO_WORLD!!!\"\n</code></pre> <p> Because, <code>lambda</code> expressions can be very useful in creating partial functions. We can use <code>lambda</code> expressions (with added contextual argumnets) as callbacks as shown below.</p> <pre><code>&gt;&gt;&gt; seq = SequenceNo()\n&gt;&gt;&gt; apply_async(add, (2,3), callback=lambda r: handler(r, seq))\n[1] Got: 5\n</code></pre> Discussion <p> Software based on callback functions often runs the risk of turning into a huge tangled mess. </p> <p>Part of the reason is that callback function is often disconnected from the code that made the initial request leading up the callback execution\". Thus the execution environment between making the request and handling the result is effectively lost. </p> <p>If you want the callback function to continue with a procedure involving multiple steps,  you have to figure out how to save and restore the associated states.</p> <p>There are really two main promising ways to capture and restore context states:</p> <ol> <li>You can carry it around in a class instance. Attach it to bound method perhaps.</li> <li>Or you can carry it around in a closure (an inner function).</li> </ol> <p>Of the above two techniques, probably the closures approach is more natural and lightweight in that they are built from functions. They also automatically captures all the variables used.</p> <p> But, if using closures, we need to pay careful attention to mutable variables. In the above solution we use <code>nonlocal</code> declaration for variable <code>sequence</code> to specify that we were using the variable from withing the closure; otherwise we will get an <code>ERROR</code>.</p> <p>The use of <code>coroutines</code> is interesting and very closely resembles the usage of closures.</p> <p> In some sense, it is even cleaner as we don't have to worry about the <code>nonlocal</code> declarations.</p> <p> Only potential downsides of using coroutines is that its somewhat difficult to understand  conceptually and we have to remembr some nitty-gritty details of its usage  (for example, making sure to call <code>next()</code> before using the couroutine as a callback etc...)</p> <p> Nevertheless, coroutines are very useful in defining inlined callbacks.</p>"},{"location":"python/cookbook_dabeaz/ch08/","title":"8. Classes & Objects","text":"<p>The primary focus in this article is to provide common programming patterns related to class definitions. Some of the topics include:</p> <ol> <li>Making objects support common Python features.</li> <li>usage of special methods.</li> <li>Encapsulation techniques.</li> <li>Inheritence.</li> <li>Memory management.</li> <li>Useful Design Patterns.</li> </ol>"},{"location":"python/cookbook_dabeaz/ch08/#81-str-repr","title":"8.1: str() &amp; repr()","text":"Problem <p>You want to change the output produced by printing or viewing instances to something more sensible.</p> Solution <p>To change the string representation of the class instances define  <code>__str__()</code> &amp; <code>__repr__()</code> methods as shown below.</p> <pre><code>class Pair:\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n    def __str__(self):\n        return \"({0.x!s}, {0.y!s})\".format(self)\n    def __repr__(self):\n        return \"Pair({0.x!r}, {0.y!r})\".format(self)\n</code></pre> <p> The <code>__repr__()</code> method returns the code represntation of the instance, and is usually the code you would type to re-create the instance. The built-in <code>repr()</code> function returns this text and so does the interactive interpreter while inspecting values.</p> <p> The <code>__str__()</code> method converts the instance to a string and is the output returned by <code>print()</code> or <code>str()</code> methods.</p> <pre><code>&gt;&gt;&gt; p = Pair(2, 3)\n&gt;&gt;&gt; p\nPair(2, 3)              ## __repr__() output\n&gt;&gt;&gt; print(p)\n&gt;&gt;&gt; (2, 3)              ## __str__() output\n</code></pre> <p> The implementation of the above class also show how different string representations  may be used during formatting. Specifically, the special <code>!r</code> formatting code indicates that the output of <code>__repr__()</code> should be used instead of <code>__str__()</code>, the default. An example below shows this behavior:</p> <pre><code>&gt;&gt;&gt; p = Pair(3, 4)\n&gt;&gt;&gt; print(\"p is {0!r}\").format(p)\np is Pair(3, 4)\n&gt;&gt;&gt; print(\"p is {0!s}\").format(p)\np is (3, 4)\n</code></pre> Discussion <p> It is standard practice to for the output of <code>__repr__()</code>  to produce text such that <code>eval(repr(x)) == x</code></p> <p>If it is not prossible or desired then it is common to create a textual represntation enclosed in <code>&lt;</code> &amp; <code>&gt;</code> instead as shown in below snippet.</p> <pre><code>&gt;&gt;&gt; f = open(\"file.dat\")\n&gt;&gt;&gt; f\n&lt;_io.TextIOWrapper name='file.dat' mode='r' encoding='utf-8'&gt;\n</code></pre> <p> If not <code>__str__()</code> is provided then <code>__repr__()</code> is used as a fallback.</p>"},{"location":"python/cookbook_dabeaz/ch08/#82-customizing-string-formatting","title":"8.2: Customizing string formatting","text":"Problem <p>You want an object to support customized formatting through the <code>formaat()</code> function and string method.</p> Solution <p>To customize string formatting, define a custom <code>__format__()</code> method in the class definition. For example:</p> <pre><code>## The format codes\n##\n_formats = {\n    \"ymd\": \"{d.year}-{d.month}-{d.day}\",\n    \"mdy\": \"{d.month}/{d.day}/{d.year}\",\n    \"dmy\": \"{d.day}/{d.month}/{d.year}\"\n}\n\n## ----------------------------------------------- ##\n\nclass date:\n    def __init__(self, day, month, year):\n        self.day = day\n        self.month = month\n        self.year = year\n\n    def __format__(self, code):\n        if code == \"\":\n            code = \"ymd\"\n        fmt = _formats[code]\n        return fmt.format(d=self)\n</code></pre> <p>Instances of <code>Date</code> now supports formatting options such as the following:</p> <pre><code>&gt;&gt;&gt; d = Date(15, 11, 2021)\n&gt;&gt;&gt;\n&gt;&gt;&gt; format(d)\n'2021-11-15'\n&gt;&gt;&gt;\n&gt;&gt;&gt; format(d, \"mdy\")\n'11-15-2021'\n&gt;&gt;&gt;\n&gt;&gt;&gt; \"The date is {:ymd}\".format(d)\n'The date is 2021-11-15'\n&gt;&gt;&gt;\n&gt;&gt;&gt; \"The date is {:mdy}\".format(d)\n'The date is 11-15-2021'\n</code></pre> <p>The <code>__format__()</code> method provides HOOK into Python's string formatting functionality.</p> <p> Its important to know that the interpretation of the format codes (e.g. <code>_formats</code> in above code) is entirely upto the class defintion and developer.</p>"},{"location":"python/cookbook_dabeaz/ch08/#86-creating-managed-attributes","title":"8.6: Creating managed attributes","text":"Problem <p>You want to add extra processing (e.e. type checking, validation) to the getting and setting of the instance attributes</p> Solution <p> A simple way to customize access to an attribute is to define it as <code>property</code>.</p> <p>For example, in below code, we define a <code>property</code> that adds simple type checking for its <code>getter</code> and <code>setter</code> methods.</p> <pre><code>class Person:\n    def __init__(self, first_name):\n        self.first_name = first_name        ## NOTE: I'm defining \"self.first_name\" not \"self._first_name\"\n\n    ## getter method\n    @property\n    def first_name(self):\n        return self._first_name\n\n    ## setter method\n    @first_name.setter\n    def first_name(self, value):\n        if not isinstance(value, str):\n            raise TypeError(\"Expected a string\")\n        self._first_name = value\n\n    ## deleter method\n    @first_name.deleter\n    def first_name(self):\n        raise AttributeError(\"Can't delete attribute\")\n</code></pre>"},{"location":"python/cookbook_dabeaz/ch08/#89-descriptor-class","title":"8.9: Descriptor Class","text":"Creating a new kind of class or instance attribute <p>You want to create a new kind of class with instance atrribute type with some extra functionality such as <code>type checking</code></p> Solution <p>If you want to create an entirely new kind of instance attribute, define its functionality in the form  of a descriptor class. For example:</p> <pre><code>## Descriptor attribute for an integer type-checked attribute\nclass Integer:\n    def __init__(self, name):\n        self.name = name\n\n    def __get__(self, instance, cls):\n        if instance is None:\n            return self\n        else:\n            return instance.__dict__[self.name]\n\n    def __set__(self, instance, val):\n        if not isinstance(val, int):\n            raise TypeError(\"Expected to be an int\")\n        instance.__dict__[self.name] = val\n\n    def __delete__(self, instance):\n        del instance.__dict__[self.name]\n</code></pre> <p> A Descriptor is a class that implements three core attribute access operations (get, set, delete) via the special methods <code>__get__(), __set__(), __delete__()</code>. These methods  work by receiving an instance as input. The underlying dictionary of the instance is then manipulated as appropriate.</p> <p> To use a descriptor, instances of the descriptor are placed into the class defintion as class variables as shown below.</p> <p><pre><code>## Using a Descriptor in a class defintion\n\nclass Point:\n    x = Integer(\"x\")\n    y = Integer(\"y\")\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n</code></pre> When we do this, all access to the descriptor attributes (<code>x, y</code>) are captured by  <code>__get__(), __set__(), __delete__()</code> methods of the descriptor class. For example:</p> <pre><code>&gt;&gt;&gt; p = Point(2, 3)\n&gt;&gt;&gt;\n&gt;&gt;&gt; p.x                     ## calls \"Point.x.__get__(p, Point)\"\n2\n&gt;&gt;&gt; p.y = 5                 ## calls \"Point.y.__set__(p, Point)\"\n&gt;&gt;&gt; p.x = 2.5               ## calls \"Point.x.__set__(p, Point)\"\n...\nTypeError: Expected an int\n</code></pre> <p> As input, each method of the descriptor receives the instance being manipulated. To carry out the requested operation, the underlying instance dictionary  (underlying <code>__dict__()</code> attribute) is manipulated. The <code>self.name</code> attribute of the  descriptor holds the dictionary key being used to store the actual data in the instance dictionary.</p> Discussion <p>Descriptors provide the underlying logic for most of the Python's class features  such as <code>@classmethod</code>, <code>@staticmethod</code>, <code>@property</code> and even <code>__slots__</code> attribute.</p> <p>By defining descriptors, you can capture the core instance oprtations (get, set, delete) at a very low level and completely customoze what they do.  It is one of the most important tool used by writers of most advance librarues &amp; frameeeeworks</p> <p> One confusion with the descriptors is that they can only be defined at the class level and not at the per-instance basis. The following descriptor usage will not work.</p> <pre><code>## this descriptor USAGE will not work\n\nclass Point:\n    def __init__(self, x, y):\n        self.x = Integer(\"x\")            ## NO! Must be a class variable\n        self.y = Integer(\"y\")            ## NO! Must be a class variable\n        self.x = x\n        self.y = y\n</code></pre>"},{"location":"python/cookbook_dabeaz/ch08/#813-implementing-a-data-model-or-type-system","title":"8.13: Implementing a Data Model or Type System","text":"Problem <p>You want to define various kinds of Data Structures, but want to enforce constraints on the values that are allowed to be assigned to certain constraints.</p> Solution <p>...</p>"},{"location":"speech/lecture-3/","title":"Lecture-3","text":"<p>Most of you may now that anbout 90% of the technology availabel is idle; in fact even more. Probably more than 95% of computing power is idle.</p> <p>In last class we dicussed two important ideas:</p> <ol> <li>Role of speech technology</li> <li><code>Signal -- to -- Symbol</code></li> </ol>"},{"location":"speech/lecture-3/#scope-of-speech-technology-for-this-course","title":"Scope of Speech Technology for this course","text":"<ol> <li>\u274c Language Translation</li> <li>\u274c Coding</li> <li>\u274c Transmission (Tx)</li> <li>\u274c Communication Technology in Speech (here it deals with coding &amp; Tx)</li> <li>\u2705 Signal to Symbol Transformation</li> <li>\u2705 Language Knowledge</li> <li>\u2705 Suprasegmental Knowledge (aka <code>prosody</code>)</li> </ol> <pre><code>@startyaml\nversion:3\n@endyaml\n</code></pre>"},{"location":"til/042123/","title":"Numpy v/s Pandas","text":""},{"location":"til/042123/#numpy-vs-pandas","title":"<code>numpy</code> v/s <code>pandas</code>","text":"<p>One subtle aspect that lot of engineers miss is the <code>access control</code> design of <code>numpy</code> and <code>pandas</code>.</p> <ol> <li><code>numpy</code> uses row-major access control.</li> <li><code>pandas</code> uses column-major access control.</li> </ol> <p>Genrally its seen that engineers complain about slow-nature of <code>numpy</code> compared to <code>pandas</code>. One of the prime culprits of this issue is the lack of knowledge about the access control used in these libraries. </p> <p>By default, <code>numpy</code> uses row-major but it allows the user to specify the access control while creating the <code>ndarray</code>. Below, are all senarios of accessing the row/col using pandas or numpy is shown.</p>"},{"location":"til/042123/#itertating-numpy-ndarray-using-column-major","title":"Itertating numpy <code>ndarray</code> using column-major","text":"<p><pre><code>import numpy as np\ndf_np = df.to_numpy()\nn_rows, n_cols = df_np.shape\n\nstart = time.time()\nfor j in range(n_cols):\n    for item in df_np[:, j]:\n        pass\n\nprint(time.time() - start, \"seconds\")\n</code></pre> <pre><code>&gt;&gt;&gt; 0.0058 seconds\n</code></pre></p>"},{"location":"til/042123/#itertating-numpy-ndarray-using-row-major-default","title":"Itertating numpy <code>ndarray</code> using row-major (default)","text":"<p><pre><code>import numpy as np\ndf_np = df.to_numpy()\nn_rows, n_cols = df_np.shape\n\nstart = time.time()\n\nfor i in range(n_rows):\n    for item in df_np[i]:\n        pass\n\nprint(time.time()-start, \"seconds\")\n</code></pre> <pre><code>&gt;&gt;&gt; 0.0195 seconds\n</code></pre></p>"},{"location":"til/about/","title":"About","text":""},{"location":"til/about/#stay-tuned","title":"<code>Stay tuned</code>.","text":""},{"location":"transformers/Untitled/","title":"Untitled","text":"<pre><code>from \u0935\u093f\u092d\u093e\u0917 import \u0935\u093f\u092d\u093e\u0917\n</code></pre> <pre><code>\u0935\u093f\u092d\u093e\u0917(12, 3)\n</code></pre> <pre>\n<code>-----------------------------------\n\u092d\u093e\u091c\u094d\u092f - (\u092d\u093e\u091c\u0915 x \u092d\u093e\u0917) = \u0936\u0947\u0937 [?] \u092d\u093e\u091c\u0915\n-----------------------------------\n12 - (3 x 1) = 9 &gt; 3\n12 - (3 x 2) = 6 &gt; 3\n12 - (3 x 3) = 3 &gt; 3\n12 - (3 x 4) = 0 &lt; 3 .\u0938\u092e\u093e\u092a\u094d\u0924\n-----------------------------------\n</code>\n</pre> <pre>\n<code>{'\u092d\u093e\u0917\u092b\u0932': 4, '\u0936\u0947\u0937\u092b\u0932': 0}</code>\n</pre> <pre><code>\u0935\u093f\u092d\u093e\u0917(1312, 3321)\n</code></pre> <pre>\n<code>-----------------------------------\n\u092d\u093e\u091c\u094d\u092f - (\u092d\u093e\u091c\u0915 x \u092d\u093e\u0917) = \u0936\u0947\u0937 [?] \u092d\u093e\u091c\u0915\n-----------------------------------\n</code>\n</pre> <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-3-8912aca34830&gt; in &lt;module&gt;\n----&gt; 1 \u0935\u093f\u092d\u093e\u0917(1312, 3321)\n\n~/Desktop/flashAI/shiksha/\u0935\u093f\u092d\u093e\u0917.py in \u0935\u093f\u092d\u093e\u0917(\u092d\u093e\u091c\u094d\u092f, \u092d\u093e\u091c\u0915)\n     14     if \u092d\u093e\u091c\u094d\u092f &lt; \u092d\u093e\u091c\u0915:\n     15         # print\n---&gt; 16         raise ValueError(f\"\u092d\u093e\u091c\u094d\u092f &lt; \u092d\u093e\u091c\u0915 [\u0917\u093c\u0932\u0924 \u0938\u0902\u0916\u094d\u092f\u093e\u090f\u0901 \u0926\u0940 \u0917\u092f\u0940\u0902. \u0915\u0943\u092a\u092f\u093e \u0938\u0939\u0940 \u0938\u0902\u0916\u094d\u092f\u093e \u0905\u0902\u0915\u093f\u0924 \u0915\u0930\u0947\u0902.]\")\n     17 \n     18     while True:\n\nValueError: \u092d\u093e\u091c\u094d\u092f &lt; \u092d\u093e\u091c\u0915 [\u0917\u093c\u0932\u0924 \u0938\u0902\u0916\u094d\u092f\u093e\u090f\u0901 \u0926\u0940 \u0917\u092f\u0940\u0902. \u0915\u0943\u092a\u092f\u093e \u0938\u0939\u0940 \u0938\u0902\u0916\u094d\u092f\u093e \u0905\u0902\u0915\u093f\u0924 \u0915\u0930\u0947\u0902.]</pre> <pre><code>\u0935\u093f\u092d\u093e\u0917(13212, 3321)\n</code></pre> <pre>\n<code>-----------------------------------\n\u092d\u093e\u091c\u094d\u092f - (\u092d\u093e\u091c\u0915 x \u092d\u093e\u0917) = \u0936\u0947\u0937 [?] \u092d\u093e\u091c\u0915\n-----------------------------------\n13212 - (3321 x 1) = 9891 &gt; 3321\n13212 - (3321 x 2) = 6570 &gt; 3321\n13212 - (3321 x 3) = 3249 &lt; 3321 .\u0938\u092e\u093e\u092a\u094d\u0924\n-----------------------------------\n</code>\n</pre> <pre>\n<code>{'\u092d\u093e\u0917\u092b\u0932': 3, '\u0936\u0947\u0937\u092b\u0932': 3249}</code>\n</pre> <pre><code>\n</code></pre>"},{"location":"transformers/about/","title":"About","text":""},{"location":"transformers/about/#stay-tuned","title":"<code>Stay tuned</code>.","text":""},{"location":"transformers/about/#references","title":"References:","text":"<ol> <li> <p>Transformers explained: https://e2eml.school/transformers.html \u21a9</p> </li> <li> <p>Positional Embeddings: https://kazemnejad.com/blog/transformer_architecture_positional_encoding/ \u21a9</p> </li> <li> <p>Positional Embeddings: https://timodenk.com/blog/linear-relationships-in-the-transformers-positional-encoding/ \u21a9</p> </li> <li> <p>POS Summation v/s Concat: https://github.com/tensorflow/tensor2tensor/issues/1591 \u21a9</p> </li> </ol>"},{"location":"transformers/detr/","title":"DETR","text":""},{"location":"transformers/detr/#arxiv-paper","title":"ArXiv paper","text":""},{"location":"transformers/detr/#review-presentation-slides","title":"Review Presentation Slides","text":""},{"location":"transformers/layout_lm/","title":"Layout LM","text":"<p>This article explains the structire and functionality of <code>LayoutLM</code> and its variants.</p>"},{"location":"transformers/layout_lm/#introduction","title":"Introduction","text":""},{"location":"work/about/","title":"About","text":""},{"location":"work/about/#stay-tuned","title":"<code>Stay tuned</code>.","text":""},{"location":"work/aros/","title":"Aros","text":"<pre><code>#####################################################################\n## \u092a\u094d\u0930\u094b\u091c\u0947\u0915\u094d\u091f-\u0936\u093f\u0915\u094d\u0937\u093e\n#####################################################################\n\ndef \u0935\u093f\u092d\u093e\u0917(\u092d\u093e\u091c\u094d\u092f, \u092d\u093e\u091c\u0915):\n    \u092d\u093e\u0917\u092b\u0932 = 0\n    \u092d\u093e\u0917 = 1\n    \u0936\u0947\u0937\u092b\u0932 = 0\n\n    print(f\"-----------------------------------\")\n    print(f\"\u092d\u093e\u091c\u094d\u092f - (\u092d\u093e\u091c\u0915 x \u092d\u093e\u0917) = \u0936\u0947\u0937 [?] \u092d\u093e\u091c\u0915\")\n    print(f\"-----------------------------------\")\n\n    if \u092d\u093e\u091c\u094d\u092f &lt; \u092d\u093e\u091c\u0915:\n        # print\n        raise ValueError(f\"\u092d\u093e\u091c\u094d\u092f &lt; \u092d\u093e\u091c\u0915 [\u0917\u093c\u0932\u0924 \u0938\u0902\u0916\u094d\u092f\u093e\u090f\u0901 \u0926\u0940 \u0917\u092f\u0940\u0902. \u0915\u0943\u092a\u092f\u093e \u0938\u0939\u0940 \u0938\u0902\u0916\u094d\u092f\u093e \u0905\u0902\u0915\u093f\u0924 \u0915\u0930\u0947\u0902.]\")\n\n    while True:\n        \u0936\u0947\u0937 = \u092d\u093e\u091c\u094d\u092f - (\u092d\u093e\u091c\u0915 * \u092d\u093e\u0917)\n        if \u0936\u0947\u0937 &gt;= \u092d\u093e\u091c\u0915:\n            print(f\"{\u092d\u093e\u091c\u094d\u092f} - ({\u092d\u093e\u091c\u0915} x {\u092d\u093e\u0917}) = {\u0936\u0947\u0937} &gt; {\u092d\u093e\u091c\u0915}\")\n            \u092d\u093e\u0917 = \u092d\u093e\u0917 + 1\n        else:\n            print(f\"{\u092d\u093e\u091c\u094d\u092f} - ({\u092d\u093e\u091c\u0915} x {\u092d\u093e\u0917}) = {\u0936\u0947\u0937} &lt; {\u092d\u093e\u091c\u0915} .\u0938\u092e\u093e\u092a\u094d\u0924\")\n            \u092d\u093e\u0917\u092b\u0932 = \u092d\u093e\u0917\n            \u0936\u0947\u0937\u092b\u0932 = \u0936\u0947\u0937\n            print(f\"-----------------------------------\")\n            return {\"\u092d\u093e\u0917\u092b\u0932\": \u092d\u093e\u0917\u092b\u0932, \"\u0936\u0947\u0937\u092b\u0932\": \u0936\u0947\u0937\u092b\u0932}\n\n#####################################################################\n</code></pre> <p> lorem ipsum</p> <p></p> Image caption <ul> <li> Lorem ipsum dolor sit amet, consectetur adipiscing elit</li> <li> Vestibulum convallis sit amet nisi a tincidunt<ul> <li> In hac habitasse platea dictumst</li> <li> In scelerisque nibh non dolor mollis congue sed et metus</li> <li> Praesent sed risus massa</li> </ul> </li> <li> Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque</li> </ul> \\[ \\operatorname{ker} f=\\{g\\in G:f(g)=e_{H}\\}{\\mbox{.}} \\] <p>The homomorphism \\(f\\) is injective if and only if its kernel is only the  singleton set \\(e_G\\), because otherwise \\(\\exists a,b\\in G\\) with \\(a\\neq b\\) such  that \\(f(a)=f(b)\\).</p> <p>The HTML specification is maintained by the W3C.</p>"},{"location":"work/cognizant/","title":"Cognizant","text":"<pre><code>#####################################################################\n## \u092a\u094d\u0930\u094b\u091c\u0947\u0915\u094d\u091f-\u0936\u093f\u0915\u094d\u0937\u093e\n#####################################################################\n\ndef \u0935\u093f\u092d\u093e\u0917(\u092d\u093e\u091c\u094d\u092f, \u092d\u093e\u091c\u0915):\n    \u092d\u093e\u0917\u092b\u0932 = 0\n    \u092d\u093e\u0917 = 1\n    \u0936\u0947\u0937\u092b\u0932 = 0\n\n    print(f\"-----------------------------------\")\n    print(f\"\u092d\u093e\u091c\u094d\u092f - (\u092d\u093e\u091c\u0915 x \u092d\u093e\u0917) = \u0936\u0947\u0937 [?] \u092d\u093e\u091c\u0915\")\n    print(f\"-----------------------------------\")\n\n    if \u092d\u093e\u091c\u094d\u092f &lt; \u092d\u093e\u091c\u0915:\n        # print\n        raise ValueError(f\"\u092d\u093e\u091c\u094d\u092f &lt; \u092d\u093e\u091c\u0915 [\u0917\u093c\u0932\u0924 \u0938\u0902\u0916\u094d\u092f\u093e\u090f\u0901 \u0926\u0940 \u0917\u092f\u0940\u0902. \u0915\u0943\u092a\u092f\u093e \u0938\u0939\u0940 \u0938\u0902\u0916\u094d\u092f\u093e \u0905\u0902\u0915\u093f\u0924 \u0915\u0930\u0947\u0902.]\")\n\n    while True:\n        \u0936\u0947\u0937 = \u092d\u093e\u091c\u094d\u092f - (\u092d\u093e\u091c\u0915 * \u092d\u093e\u0917)\n        if \u0936\u0947\u0937 &gt;= \u092d\u093e\u091c\u0915:\n            print(f\"{\u092d\u093e\u091c\u094d\u092f} - ({\u092d\u093e\u091c\u0915} x {\u092d\u093e\u0917}) = {\u0936\u0947\u0937} &gt; {\u092d\u093e\u091c\u0915}\")\n            \u092d\u093e\u0917 = \u092d\u093e\u0917 + 1\n        else:\n            print(f\"{\u092d\u093e\u091c\u094d\u092f} - ({\u092d\u093e\u091c\u0915} x {\u092d\u093e\u0917}) = {\u0936\u0947\u0937} &lt; {\u092d\u093e\u091c\u0915} .\u0938\u092e\u093e\u092a\u094d\u0924\")\n            \u092d\u093e\u0917\u092b\u0932 = \u092d\u093e\u0917\n            \u0936\u0947\u0937\u092b\u0932 = \u0936\u0947\u0937\n            print(f\"-----------------------------------\")\n            return {\"\u092d\u093e\u0917\u092b\u0932\": \u092d\u093e\u0917\u092b\u0932, \"\u0936\u0947\u0937\u092b\u0932\": \u0936\u0947\u0937\u092b\u0932}\n\n#####################################################################\n</code></pre> <p> lorem ipsum</p> <p></p> Image caption <ul> <li> Lorem ipsum dolor sit amet, consectetur adipiscing elit</li> <li> Vestibulum convallis sit amet nisi a tincidunt<ul> <li> In hac habitasse platea dictumst</li> <li> In scelerisque nibh non dolor mollis congue sed et metus</li> <li> Praesent sed risus massa</li> </ul> </li> <li> Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque</li> </ul> \\[ \\operatorname{ker} f=\\{g\\in G:f(g)=e_{H}\\}{\\mbox{.}} \\] <p>The homomorphism \\(f\\) is injective if and only if its kernel is only the  singleton set \\(e_G\\), because otherwise \\(\\exists a,b\\in G\\) with \\(a\\neq b\\) such  that \\(f(a)=f(b)\\).</p> <p>The HTML specification is maintained by the W3C.</p>"},{"location":"work/flashai/","title":"Flashai","text":"<pre><code>#####################################################################\n## \u092a\u094d\u0930\u094b\u091c\u0947\u0915\u094d\u091f-\u0936\u093f\u0915\u094d\u0937\u093e\n#####################################################################\n\ndef \u0935\u093f\u092d\u093e\u0917(\u092d\u093e\u091c\u094d\u092f, \u092d\u093e\u091c\u0915):\n    \u092d\u093e\u0917\u092b\u0932 = 0\n    \u092d\u093e\u0917 = 1\n    \u0936\u0947\u0937\u092b\u0932 = 0\n\n    print(f\"-----------------------------------\")\n    print(f\"\u092d\u093e\u091c\u094d\u092f - (\u092d\u093e\u091c\u0915 x \u092d\u093e\u0917) = \u0936\u0947\u0937 [?] \u092d\u093e\u091c\u0915\")\n    print(f\"-----------------------------------\")\n\n    if \u092d\u093e\u091c\u094d\u092f &lt; \u092d\u093e\u091c\u0915:\n        # print\n        raise ValueError(f\"\u092d\u093e\u091c\u094d\u092f &lt; \u092d\u093e\u091c\u0915 [\u0917\u093c\u0932\u0924 \u0938\u0902\u0916\u094d\u092f\u093e\u090f\u0901 \u0926\u0940 \u0917\u092f\u0940\u0902. \u0915\u0943\u092a\u092f\u093e \u0938\u0939\u0940 \u0938\u0902\u0916\u094d\u092f\u093e \u0905\u0902\u0915\u093f\u0924 \u0915\u0930\u0947\u0902.]\")\n\n    while True:\n        \u0936\u0947\u0937 = \u092d\u093e\u091c\u094d\u092f - (\u092d\u093e\u091c\u0915 * \u092d\u093e\u0917)\n        if \u0936\u0947\u0937 &gt;= \u092d\u093e\u091c\u0915:\n            print(f\"{\u092d\u093e\u091c\u094d\u092f} - ({\u092d\u093e\u091c\u0915} x {\u092d\u093e\u0917}) = {\u0936\u0947\u0937} &gt; {\u092d\u093e\u091c\u0915}\")\n            \u092d\u093e\u0917 = \u092d\u093e\u0917 + 1\n        else:\n            print(f\"{\u092d\u093e\u091c\u094d\u092f} - ({\u092d\u093e\u091c\u0915} x {\u092d\u093e\u0917}) = {\u0936\u0947\u0937} &lt; {\u092d\u093e\u091c\u0915} .\u0938\u092e\u093e\u092a\u094d\u0924\")\n            \u092d\u093e\u0917\u092b\u0932 = \u092d\u093e\u0917\n            \u0936\u0947\u0937\u092b\u0932 = \u0936\u0947\u0937\n            print(f\"-----------------------------------\")\n            return {\"\u092d\u093e\u0917\u092b\u0932\": \u092d\u093e\u0917\u092b\u0932, \"\u0936\u0947\u0937\u092b\u0932\": \u0936\u0947\u0937\u092b\u0932}\n\n#####################################################################\n</code></pre> <p> lorem ipsum</p> <p></p> Image caption <ul> <li> Lorem ipsum dolor sit amet, consectetur adipiscing elit</li> <li> Vestibulum convallis sit amet nisi a tincidunt<ul> <li> In hac habitasse platea dictumst</li> <li> In scelerisque nibh non dolor mollis congue sed et metus</li> <li> Praesent sed risus massa</li> </ul> </li> <li> Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque</li> </ul> \\[ \\operatorname{ker} f=\\{g\\in G:f(g)=e_{H}\\}{\\mbox{.}} \\] <p>The homomorphism \\(f\\) is injective if and only if its kernel is only the  singleton set \\(e_G\\), because otherwise \\(\\exists a,b\\in G\\) with \\(a\\neq b\\) such  that \\(f(a)=f(b)\\).</p> <p>The HTML specification is maintained by the W3C.</p>"},{"location":"work/ivmcl/","title":"Ivmcl","text":"<pre><code>#####################################################################\n## \u092a\u094d\u0930\u094b\u091c\u0947\u0915\u094d\u091f-\u0936\u093f\u0915\u094d\u0937\u093e\n#####################################################################\n\ndef \u0935\u093f\u092d\u093e\u0917(\u092d\u093e\u091c\u094d\u092f, \u092d\u093e\u091c\u0915):\n    \u092d\u093e\u0917\u092b\u0932 = 0\n    \u092d\u093e\u0917 = 1\n    \u0936\u0947\u0937\u092b\u0932 = 0\n\n    print(f\"-----------------------------------\")\n    print(f\"\u092d\u093e\u091c\u094d\u092f - (\u092d\u093e\u091c\u0915 x \u092d\u093e\u0917) = \u0936\u0947\u0937 [?] \u092d\u093e\u091c\u0915\")\n    print(f\"-----------------------------------\")\n\n    if \u092d\u093e\u091c\u094d\u092f &lt; \u092d\u093e\u091c\u0915:\n        # print\n        raise ValueError(f\"\u092d\u093e\u091c\u094d\u092f &lt; \u092d\u093e\u091c\u0915 [\u0917\u093c\u0932\u0924 \u0938\u0902\u0916\u094d\u092f\u093e\u090f\u0901 \u0926\u0940 \u0917\u092f\u0940\u0902. \u0915\u0943\u092a\u092f\u093e \u0938\u0939\u0940 \u0938\u0902\u0916\u094d\u092f\u093e \u0905\u0902\u0915\u093f\u0924 \u0915\u0930\u0947\u0902.]\")\n\n    while True:\n        \u0936\u0947\u0937 = \u092d\u093e\u091c\u094d\u092f - (\u092d\u093e\u091c\u0915 * \u092d\u093e\u0917)\n        if \u0936\u0947\u0937 &gt;= \u092d\u093e\u091c\u0915:\n            print(f\"{\u092d\u093e\u091c\u094d\u092f} - ({\u092d\u093e\u091c\u0915} x {\u092d\u093e\u0917}) = {\u0936\u0947\u0937} &gt; {\u092d\u093e\u091c\u0915}\")\n            \u092d\u093e\u0917 = \u092d\u093e\u0917 + 1\n        else:\n            print(f\"{\u092d\u093e\u091c\u094d\u092f} - ({\u092d\u093e\u091c\u0915} x {\u092d\u093e\u0917}) = {\u0936\u0947\u0937} &lt; {\u092d\u093e\u091c\u0915} .\u0938\u092e\u093e\u092a\u094d\u0924\")\n            \u092d\u093e\u0917\u092b\u0932 = \u092d\u093e\u0917\n            \u0936\u0947\u0937\u092b\u0932 = \u0936\u0947\u0937\n            print(f\"-----------------------------------\")\n            return {\"\u092d\u093e\u0917\u092b\u0932\": \u092d\u093e\u0917\u092b\u0932, \"\u0936\u0947\u0937\u092b\u0932\": \u0936\u0947\u0937\u092b\u0932}\n\n#####################################################################\n</code></pre> <p> lorem ipsum</p> <p></p> Image caption <ul> <li> Lorem ipsum dolor sit amet, consectetur adipiscing elit</li> <li> Vestibulum convallis sit amet nisi a tincidunt<ul> <li> In hac habitasse platea dictumst</li> <li> In scelerisque nibh non dolor mollis congue sed et metus</li> <li> Praesent sed risus massa</li> </ul> </li> <li> Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque</li> </ul> \\[ \\operatorname{ker} f=\\{g\\in G:f(g)=e_{H}\\}{\\mbox{.}} \\] <p>The homomorphism \\(f\\) is injective if and only if its kernel is only the  singleton set \\(e_G\\), because otherwise \\(\\exists a,b\\in G\\) with \\(a\\neq b\\) such  that \\(f(a)=f(b)\\).</p> <p>The HTML specification is maintained by the W3C.</p>"},{"location":"work/meeami/","title":"Meeami","text":"<pre><code>#####################################################################\n## \u092a\u094d\u0930\u094b\u091c\u0947\u0915\u094d\u091f-\u0936\u093f\u0915\u094d\u0937\u093e\n#####################################################################\n\ndef \u0935\u093f\u092d\u093e\u0917(\u092d\u093e\u091c\u094d\u092f, \u092d\u093e\u091c\u0915):\n    \u092d\u093e\u0917\u092b\u0932 = 0\n    \u092d\u093e\u0917 = 1\n    \u0936\u0947\u0937\u092b\u0932 = 0\n\n    print(f\"-----------------------------------\")\n    print(f\"\u092d\u093e\u091c\u094d\u092f - (\u092d\u093e\u091c\u0915 x \u092d\u093e\u0917) = \u0936\u0947\u0937 [?] \u092d\u093e\u091c\u0915\")\n    print(f\"-----------------------------------\")\n\n    if \u092d\u093e\u091c\u094d\u092f &lt; \u092d\u093e\u091c\u0915:\n        # print\n        raise ValueError(f\"\u092d\u093e\u091c\u094d\u092f &lt; \u092d\u093e\u091c\u0915 [\u0917\u093c\u0932\u0924 \u0938\u0902\u0916\u094d\u092f\u093e\u090f\u0901 \u0926\u0940 \u0917\u092f\u0940\u0902. \u0915\u0943\u092a\u092f\u093e \u0938\u0939\u0940 \u0938\u0902\u0916\u094d\u092f\u093e \u0905\u0902\u0915\u093f\u0924 \u0915\u0930\u0947\u0902.]\")\n\n    while True:\n        \u0936\u0947\u0937 = \u092d\u093e\u091c\u094d\u092f - (\u092d\u093e\u091c\u0915 * \u092d\u093e\u0917)\n        if \u0936\u0947\u0937 &gt;= \u092d\u093e\u091c\u0915:\n            print(f\"{\u092d\u093e\u091c\u094d\u092f} - ({\u092d\u093e\u091c\u0915} x {\u092d\u093e\u0917}) = {\u0936\u0947\u0937} &gt; {\u092d\u093e\u091c\u0915}\")\n            \u092d\u093e\u0917 = \u092d\u093e\u0917 + 1\n        else:\n            print(f\"{\u092d\u093e\u091c\u094d\u092f} - ({\u092d\u093e\u091c\u0915} x {\u092d\u093e\u0917}) = {\u0936\u0947\u0937} &lt; {\u092d\u093e\u091c\u0915} .\u0938\u092e\u093e\u092a\u094d\u0924\")\n            \u092d\u093e\u0917\u092b\u0932 = \u092d\u093e\u0917\n            \u0936\u0947\u0937\u092b\u0932 = \u0936\u0947\u0937\n            print(f\"-----------------------------------\")\n            return {\"\u092d\u093e\u0917\u092b\u0932\": \u092d\u093e\u0917\u092b\u0932, \"\u0936\u0947\u0937\u092b\u0932\": \u0936\u0947\u0937\u092b\u0932}\n\n#####################################################################\n</code></pre> <p> lorem ipsum</p> <p></p> Image caption <ul> <li> Lorem ipsum dolor sit amet, consectetur adipiscing elit</li> <li> Vestibulum convallis sit amet nisi a tincidunt<ul> <li> In hac habitasse platea dictumst</li> <li> In scelerisque nibh non dolor mollis congue sed et metus</li> <li> Praesent sed risus massa</li> </ul> </li> <li> Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque</li> </ul> \\[ \\operatorname{ker} f=\\{g\\in G:f(g)=e_{H}\\}{\\mbox{.}} \\] <p>The homomorphism \\(f\\) is injective if and only if its kernel is only the  singleton set \\(e_G\\), because otherwise \\(\\exists a,b\\in G\\) with \\(a\\neq b\\) such  that \\(f(a)=f(b)\\).</p> <p>The HTML specification is maintained by the W3C.</p>"}]}