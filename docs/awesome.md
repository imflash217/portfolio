
[^1]: ðŸŸ¡ PositionalEncoding: 
            https://e2eml.school/transformers.html
[^2]: ðŸŸ¡ PositionalEncoding: 
            https://github.com/tensorflow/tensor2tensor/issues/1591
[^3]: ðŸŸ¢ PositionalEncoding: 
            https://kazemnejad.com/blog/transformer_architecture_positional_encoding/
[^4]: ðŸŸ¡ BatchNorm: 
            https://colab.research.google.com/drive/1Sw2GXJmylz9DvtPaBfJIeutoXNISelZy?usp=sharing
[^5]: ðŸŸ¢ BatchNorm: https://e2eml.school/batch_normalization.html
[^6]: ðŸŸ¡ How BatchNorm Helps? https://arxiv.org/pdf/1805.11604.pdf
[^7]: ðŸŸ¡ Understanding BatchNorm: 
            https://proceedings.neurips.cc/paper/2018/file/36072923bfc3cf47745d704feb489480-Paper.pdf
[^8]: ðŸŸ¡ Batch-ReNorm: https://arxiv.org/pdf/1702.03275.pdf
[^9]: ðŸŸ¡ StreamingNorm: https://arxiv.org/pdf/1610.06160.pdf
[^10]: ðŸŸ¡ Online Norm: 
            https://proceedings.neurips.cc/paper/2019/file/cb3ce9b06932da6faaa7fc70d5b5d2f4-Paper.pdf
[^11]: ðŸŸ¢ How optimization works: 
            https://e2eml.school/how_optimization_works_1.html
[^12]: ðŸŸ¢ How optimization works:
            https://e2eml.school/how_optimization_works_2.html
[^13]: ðŸŸ¢ Optimizing a linear model:
            https://e2eml.school/how_optimization_works_3.html
[^14]: ðŸŸ¢ Optimizing complex models:
            https://e2eml.school/how_optimization_works_4.html

``` title=".browserslistrc"
--8<--â€‹ "https://github.com/imflash217/cybertron/blob/main/cybertron/utils.py"
```

